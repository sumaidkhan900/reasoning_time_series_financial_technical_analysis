{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luTHdw5Sl9lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# **Verbal Technical Analysis: A Production-Grade Implementation**\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2511.08616-b31b1b.svg)](https://arxiv.org/abs/2511.08616)\n",
        "[![Conference](https://img.shields.io/badge/Conference-ICAIF%202025-9cf)](https://ai-finance.org/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Quantitative%20Finance-00529B)](https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-StockNet-003299)](https://github.com/yumoxu/stocknet-dataset)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Reinforcement%20Learning-orange)](https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Time--Series%20Forecasting-red)](https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n",
        "[![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Transformers-yellow)](https://huggingface.co/transformers)\n",
        "[![PEFT](https://img.shields.io/badge/PEFT-LoRA-green)](https://github.com/huggingface/peft)\n",
        "[![CVXPY](https://img.shields.io/badge/CVXPY-F4B841-blue)](https://www.cvxpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Reasoning on Time-Series for Financial Technical Analysis\"** by:\n",
        "\n",
        "*   Kelvin J.L. Koa\n",
        "*   Jan Chen\n",
        "*   Yunshan Ma\n",
        "*   Huanhuan Zheng\n",
        "*   Tat-Seng Chua\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and cleansing to multi-stage model training, baseline comparison, and final evaluation of both forecasting accuracy and portfolio utility.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_vta_pipeline`](#key-callable-run_vta_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the Verbal Technical Analysis (VTA) framework presented in Koa et al. (2025). The core of this repository is the iPython Notebook `reasoning_time_series_financial_technical_analysis_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed as a robust and scalable system for generating dual-output stock forecasts that are both numerically accurate and accompanied by a human-readable analytical narrative.\n",
        "\n",
        "The paper's central contribution is a novel, multi-stage training methodology that teaches a Large Language Model (LLM) to perform financial technical analysis and fuses its reasoning with a dedicated time-series forecasting model. This codebase operationalizes the paper's experimental design, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Execute a multi-stage data preparation pipeline to cleanse, window, and annotate time-series data with a full suite of technical indicators.\n",
        "-   Train a reasoning LLM (`Ï€Î¸`) using a three-stage process: cold-start Reinforcement Learning (RL), Rejection Sampling with Supervised Fine-Tuning (SFT), and final performance-tuning RL.\n",
        "-   Train a bespoke, dual-branch time-series backbone (`Ï†`) using a cross-modal alignment objective.\n",
        "-   Train a conditional fusion model (`Ïˆ`) that learns to guide the backbone's forecast using attributes derived from the LLM's reasoning.\n",
        "-   Run a complete, end-to-end inference pipeline to generate the final dual output (forecast + narrative).\n",
        "-   Train and evaluate strong baseline models (DLinear, TSMixer) under identical conditions for fair comparison.\n",
        "-   Perform a comprehensive evaluation of all models on both statistical error metrics (MSE, MAE) and a realistic portfolio backtest using Markowitz optimization.\n",
        "-   Conduct a full suite of sensitivity analyses to test the robustness of the results to key hyperparameter choices.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in principles from deep learning, reinforcement learning, and modern portfolio theory.\n",
        "\n",
        "**1. Time-GRPO for Reasoning:**\n",
        "The reasoning LLM is trained using a novel objective called Time-Series Group Relative Policy Optimization (Time-GRPO), a variant of PPO. The policy `Ï€Î¸` is optimized to maximize a reward signal derived from the accuracy of its generated forecast. The core reward is the inverse Mean Squared Error:\n",
        "$$\n",
        "r_{\\mathrm{MSE}}(\\theta) = \\frac{1}{\\lambda \\cdot \\lVert \\hat{y}_\\theta - y \\rVert^2 + \\epsilon}\n",
        "$$\n",
        "The policy is updated using a clipped surrogate objective with a KL penalty to maintain language fluency, based on the group-relative advantage \\( A_i = (r_i - \\mathrm{mean}(\\{r_j\\})) / (\\mathrm{std}(\\{r_j\\}) + \\epsilon) \\).\n",
        "\n",
        "**2. Cross-Modal Alignment:**\n",
        "The time-series backbone `Ï†` is trained to align the numerical time-series domain with a latent language space. This is achieved via a cross-attention mechanism where the time-series embeddings `X_time` act as the query and a set of \"language prototypes\" `D` (derived from a base LLM's vocabulary via PCA) act as the key and value.\n",
        "$$\n",
        "X_{\\text{text}} = \\mathrm{Softmax}\\left( \\frac{(X_{\\text{time}} W_Q) (D W_K)^T}{\\sqrt{C}} \\right) (D W_V)\n",
        "$$\n",
        "The model is trained with a dual-loss objective that encourages consistency between the temporal and aligned-textual branches.\n",
        "\n",
        "**3. Classifier-Free Guidance for Fusion:**\n",
        "The final forecast is a blend of the unconditional prediction from the backbone `Å·_Ï†(X)` and a conditional prediction `Å·_Ïˆ(X, c)` that is guided by attributes `c` from the LLM's reasoning. The fusion model `Ïˆ` is trained with random dropping of the conditioning vector `c`. At inference, two forward passes are performed (one with `c`, one without) and the results are blended:\n",
        "$$\n",
        "\\hat{y} = \\hat{y}_\\phi(X) + s \\cdot \\big( \\hat{y}_\\psi(X, c) - \\hat{y}_\\phi(X) \\big)\n",
        "$$\n",
        "where `s` is the guidance scale.\n",
        "\n",
        "**4. Markowitz Portfolio Optimization:**\n",
        "To assess financial utility, a daily-rebalanced portfolio is constructed by solving the Markowitz mean-variance optimization problem. The model's multi-step price forecasts are used to derive the expected returns `Î¼` and the covariance matrix `Î£` of the assets. The optimizer finds the weights `w` that maximize the risk-adjusted return:\n",
        "$$\n",
        "\\max_{w} \\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w \\quad \\text{subject to} \\quad w \\succeq 0, \\; \\mathbf{1}^\\top w = 1\n",
        "$$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`reasoning_time_series_financial_technical_analysis_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 15 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All hyperparameters and settings are managed in an external `config.yaml` file.\n",
        "-   **High-Performance Feature Engineering:** Includes a complete, from-scratch implementation of 10 technical indicators using vectorized `numpy` and JIT-compiled `numba` for C-level speed.\n",
        "-   **Resumable Pipeline:** The master orchestrator implements atomic artifact management, allowing the long-running pipeline to be stopped and resumed without re-running expensive completed stages.\n",
        "-   **Production-Grade Training:** Implements best practices for RL and deep learning, including a robust PPO-style loop, SFT with the `transformers` Trainer, validation-based checkpointing, and gradient clipping.\n",
        "-   **Rigorous Financial Backtesting:** Implements a daily rebalancing portfolio backtest with a professional-grade `cvxpy` optimizer and covariance matrix regularization.\n",
        "-   **Complete Replication and Robustness:** A single top-level function call can execute the entire study, including a comprehensive suite of sensitivity analyses.\n",
        "-   **Full Provenance:** The pipeline generates a unique run directory for each experiment, containing a detailed log file, a copy of the exact configuration used, and all generated artifacts for full reproducibility.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Preparation (Tasks 1-5):** Ingests and validates all raw inputs, cleanses the market data, creates memory-efficient sliding windows, computes a full suite of technical indicators, and assembles the final prompts.\n",
        "2.  **Reasoning Model Training (Tasks 6-8):** Executes the three-stage RL and SFT pipeline to train the reasoning model `Ï€Î¸`.\n",
        "3.  **Backbone Model Training (Task 9):** Trains the dual-branch forecasting backbone `Ï†` with the cross-modal alignment objective.\n",
        "4.  **Fusion Model Training (Task 10):** Trains the conditional fusion model `Ïˆ` using classifier-free guidance.\n",
        "5.  **Inference (Task 11):** Runs the complete, three-model pipeline to generate the final dual outputs (forecast + narrative).\n",
        "6.  **Baseline Evaluation (Task 12):** Trains and evaluates DLinear and TSMixer under identical conditions.\n",
        "7.  **Final Evaluation (Task 13):** Computes all final error metrics and portfolio performance metrics for all models and generates comparison tables.\n",
        "8.  **Robustness Analysis (Task 15):** Systematically re-runs the pipeline with varied hyperparameters to test for sensitivity.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `reasoning_time_series_financial_technical_analysis_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 15 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_vta_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_vta_pipeline`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   A CUDA-enabled GPU is highly recommended for all model training stages.\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `torch`, `transformers`, `peft`, `numba`, `scikit-learn`, `cvxpy`, `exchange_calendars`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis.git\n",
        "    cd reasoning_time_series_financial_technical_analysis\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a primary `market_data_df` with a specific schema, which is rigorously validated. A synthetic data generator is included in the notebook for a self-contained demonstration.\n",
        "\n",
        "-   **`market_data_df`**: A `pandas.DataFrame` with a `MultiIndex` of `['date', 'ticker']`.\n",
        "    -   **Index:**\n",
        "        -   `date`: `datetime64[ns]`\n",
        "        -   `ticker`: `object` (string)\n",
        "    -   **Columns:**\n",
        "        -   `Open`, `High`, `Low`, `Close`, `Adj Close`: `float64`\n",
        "        -   `Volume`: `int64` or `float64`\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `reasoning_time_series_financial_technical_analysis_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `main` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Define paths and parameters.\n",
        "    CONFIG_PATH = \"./config.yaml\"\n",
        "    DATA_PATH = \"./synthetic_market_data.csv\"\n",
        "    \n",
        "    # 2. Load configuration from the YAML file.\n",
        "    with open(CONFIG_PATH, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # 3. Define necessary data mappings.\n",
        "    TICKER_TO_MARKET_MAP = {'SYNTH_A': 'US', 'SYNTH_B': 'US', ...}\n",
        "    TICKER_TO_DATASET_MAP = {'SYNTH_A': 'StockNet', 'SYNTH_B': 'StockNet', ...}\n",
        "    \n",
        "    # 4. Execute the entire replication study in dry-run mode for a quick test.\n",
        "    #    Set dry_run=False for a full run.\n",
        "    final_results = main(\n",
        "        market_data_path=DATA_PATH,\n",
        "        config=config,\n",
        "        ticker_to_market_map=TICKER_TO_MARKET_MAP,\n",
        "        ticker_to_dataset_map=TICKER_TO_DATASET_MAP,\n",
        "        dataset_name=\"StockNet\",\n",
        "        base_run_id=\"vta_replication\",\n",
        "        run_sensitivity=False,\n",
        "        dry_run=True\n",
        "    )\n",
        "    \n",
        "    # 5. Inspect final results.\n",
        "    print(\"--- PIPELINE EXECUTION SUCCEEDED ---\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline generates a structured `results/` directory. Each call to the master orchestrator creates a unique run directory:\n",
        "-   **`results/<run_id>/`**: Contains all artifacts for a specific run.\n",
        "    -   `artifacts/`: Pickled Python objects for each major task's output (e.g., `task_6_outputs.pkl`).\n",
        "    -   `models/`: Saved model checkpoints for each training stage (e.g., `stage3_final_lora/`).\n",
        "    -   `config.yaml`: An exact copy of the configuration used for this run.\n",
        "    -   `pipeline.log`: A detailed log file for the run.\n",
        "-   **`results/<base_run_id>_sensitivity_analysis_summary.csv`**: If sensitivity analysis is run, this master table summarizes the results.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "reasoning_time_series_financial_technical_analysis/\n",
        "â”‚\n",
        "â”œâ”€â”€ reasoning_time_series_financial_technical_analysis_draft.ipynb\n",
        "â”œâ”€â”€ config.yaml\n",
        "â”œâ”€â”€ requirements.txt\n",
        "â”‚\n",
        "â”œâ”€â”€ results/\n",
        "â”‚   â”œâ”€â”€ vta_replication_baseline_dry_run/\n",
        "â”‚   â”‚   â”œâ”€â”€ artifacts/\n",
        "â”‚   â”‚   â”œâ”€â”€ models/\n",
        "â”‚   â”‚   â”œâ”€â”€ config.yaml\n",
        "â”‚   â”‚   â””â”€â”€ pipeline.log\n",
        "â”‚   â””â”€â”€ ...\n",
        "â”‚\n",
        "â”œâ”€â”€ LICENSE\n",
        "â””â”€â”€ README.md\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify all study parameters, including model identifiers, learning rates, architectural details, and technical indicator settings, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "An ablation study was proposed but not implemented. A key extension would be to implement this analysis to quantify the contribution of each component of the VTA framework. This would involve:\n",
        "-   Creating a meta-orchestrator similar to the sensitivity analysis.\n",
        "-   Programmatically creating modified configurations for each ablation scenario (e.g., setting `guidance_scale = 0`, using a simplified `c` vector).\n",
        "-   Running the pipeline for each ablation and comparing the final performance metrics against the full VTA model.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{koa2025reasoning,\n",
        "  title={Reasoning on Time-Series for Financial Technical Analysis},\n",
        "  author={Koa, Kelvin J.L. and Chen, Jan and Ma, Yunshan and Zheng, Huanhuan and Chua, Tat-Seng},\n",
        "  journal={arXiv preprint arXiv:2511.08616},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Production-Grade Implementation of \"Reasoning on Time-Series for Financial Technical Analysis\".\n",
        "GitHub repository: https://github.com/chirindaopensource/reasoning_time_series_financial_technical_analysis\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Kelvin J.L. Koa, Jan Chen, Yunshan Ma, Huanhuan Zheng, and Tat-Seng Chua** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **PyTorch, Hugging Face (Transformers, PEFT), Pandas, NumPy, Numba, CVXPY, and Scikit-learn**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `reasoning_time_series_financial_technical_analysis_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "xXLQyDcy_WPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Reasoning on Time-Series for Financial Technical Analysis*\"\n",
        "\n",
        "Authors: Kelvin J.L. Koa, Jan Chen, Yunshan Ma, Huanhuan Zheng, Tat-Seng Chua\n",
        "\n",
        "E-Journal Submission Date: 6 November 2025\n",
        "\n",
        "Conference Affiliation: International Conference on AI in Finance (ICAIF) 2025 - Best Paper\n",
        "\n",
        "Link: https://arxiv.org/abs/2511.08616\n",
        "\n",
        "Abstract:\n",
        "\n",
        "While Large Language Models have been used to produce interpretable stock forecasts, they mainly focus on analyzing textual reports but not historical price data, also known as Technical Analysis. This task is challenging as it switches between domains: the stock price inputs and outputs lie in the time-series domain, while the reasoning step should be in natural language. In this work, we introduce Verbal Technical Analysis (VTA), a novel framework that combine verbal and latent reasoning to produce stock time-series forecasts that are both accurate and interpretable. To reason over time-series, we convert stock price data into textual annotations and optimize the reasoning trace using an inverse Mean Squared Error (MSE) reward objective. To produce time-series outputs from textual reasoning, we condition the outputs of a time-series backbone model on the reasoning-based attributes. Experiments on stock datasets across U.S., Chinese, and European markets show that VTA achieves state-of-the-art forecasting accuracy, while the reasoning traces also perform well on evaluation by industry experts."
      ],
      "metadata": {
        "id": "24cPxCbvmDsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Professor's Summary: Koa et al., \"Reasoning on Time-Series for Financial Technical Analysis\"**\n",
        "\n",
        "#### **The Core Problem and Research Gap**\n",
        "\n",
        "The authors begin by correctly identifying a significant gap in the application of Large Language Models (LLMs) to finance. While LLMs like BloombergGPT have proven adept at processing textual dataâ€”such as news, sentiment, and financial reportsâ€”they have largely neglected the domain of **Technical Analysis**.\n",
        "\n",
        "Technical Analysis is the art and science of forecasting future price movements based on historical price and volume data. This is a fundamentally different task from text processing. It involves a challenging **domain switch**:\n",
        "\n",
        "1.  **Input:** Numerical time-series data (stock prices, volume, etc.).\n",
        "2.  **Process:** Human-like reasoning based on established technical indicators (e.g., Moving Averages, RSI, Bollinger Bands). This reasoning is expressed in natural language.\n",
        "3.  **Output:** A numerical time-series forecast.\n",
        "\n",
        "The paper posits that existing models fail here. Traditional time-series models are often \"black boxes\" that lack interpretability, while direct application of LLMs to numerical forecasting is suboptimal and loses the model's core language capabilities.\n",
        "\n",
        "#### **The Proposed Solution â€” Verbal Technical Analysis (VTA)**\n",
        "\n",
        "To address this, the authors introduce a novel framework called **Verbal Technical Analysis (VTA)**. The core idea is to create a hybrid, dual-process system that mimics a human analyst working with a quantitative model. It elegantly separates the task into two components:\n",
        "\n",
        "1.  **Verbal Reasoning (\"The Analyst\"):** An LLM is trained to analyze the time-series data and produce an interpretable, natural language reasoning trace that explains its forecast.\n",
        "2.  **Latent Thinking (\"The Quant\"):** A powerful, LLM-based time-series forecasting model generates the actual numerical predictions.\n",
        "\n",
        "The key innovation is not just having these two components, but making them work synergistically. The verbal reasoning from the \"analyst\" is used to guide and improve the numerical output of the \"quant,\" making the final forecast both more accurate and fully explainable.\n",
        "\n",
        "#### **The Methodological Breakdown**\n",
        "\n",
        "The VTA framework is implemented through a sophisticated three-part methodology.\n",
        "\n",
        "**Part A: Teaching the LLM to Reason (Verbal Reasoning)**\n",
        "\n",
        "This is perhaps the most novel part of the paper. How do you teach an LLM to perform technical analysis without a massive dataset of human analysts' thoughts?\n",
        "\n",
        "1.  **Input Annotation:** The raw time-series data (price, volume) is first converted into a textual format the LLM can understand. This involves calculating key statistics (min, max, mean) and a suite of standard **technical indicators** (SMA, EMA, Momentum, etc.). This annotated text serves as the prompt for the LLM.\n",
        "2.  **Reinforcement Learning for Optimization:** The LLM is then fine-tuned using a Reinforcement Learning (RL) objective called **Time-GRPO**. The crucial element here is the **reward function**. Instead of rewarding for stylistic correctness, the model is rewarded based on the **accuracy of the forecast it implies**. Specifically, the reward is the *inverse Mean Squared Error (MSE)*. This brilliantly incentivizes the LLM to generate reasoning that leads to quantitatively accurate predictions.\n",
        "3.  **Multi-Stage Training:** This RL process is conducted in stages (cold-start, rejection sampling, and final optimization) to ensure the model learns effectively and produces high-quality reasoning.\n",
        "\n",
        "**Part B: The Backbone Forecasting Model (Latent Thinking)**\n",
        "\n",
        "For the numerical forecasting, the authors employ a state-of-the-art LLM-based time-series model (specifically, one similar to CALF). This model works by aligning the embedding space of the time-series data with the LLM's internal word embedding space. This allows the powerful transformer architecture, originally designed for language, to effectively capture complex temporal patterns in the numerical data. On its own, this model is powerful but not interpretable.\n",
        "\n",
        "**Part C: Fusing Reasoning and Forecasting (Joint Conditional Training)**\n",
        "\n",
        "This is the final, critical step where the two components are integrated.\n",
        "\n",
        "1.  **Attribute Extraction:** The system takes the verbal reasoning trace generated by the LLM in Part A and extracts key descriptive attributes (e.g., predicted max value, min value, mean value).\n",
        "2.  **Conditional Forecasting:** The numerical forecasting model from Part B is then conditioned on these attributes. In essence, the model is asked to produce a forecast *given that the reasoning trace is true*.\n",
        "3.  **Classifier-Free Guidance:** Drawing inspiration from image generation models, the final forecast is a weighted average of the *conditional* forecast (guided by the reasoning) and an *unconditional* forecast (the model's raw prediction). A \"guidance scale\" parameter controls how much influence the verbal reasoning has on the final output. This joint training preserves both the raw accuracy of the forecasting model and the interpretability from the reasoning model.\n",
        "\n",
        "#### **Experimental Validation and Results**\n",
        "\n",
        "The authors conduct a rigorous and comprehensive evaluation across multiple dimensions.\n",
        "\n",
        "1.  **Forecasting Accuracy:** On several stock market datasets (StockNet, Dow Jones, China A50, etc.), VTA achieves **state-of-the-art performance**, outperforming 14 other models, including traditional time-series transformers and other time-series LLMs. It consistently yields lower MSE and MAE.\n",
        "2.  **Reasoning Quality:** This is a crucial test of their \"interpretability\" claim. They surveyed 25 financial industry experts (from firms like JPMorgan and UBS), who blindly rated the reasoning traces from VTA against those from other powerful LLMs. VTA's outputs were rated significantly higher across all criteria, especially in **Depth, Accuracy, and Relevance**â€”the metrics most tied to genuine technical expertise.\n",
        "3.  **Practical Utility (Portfolio Optimization):** To demonstrate real-world value, the forecasts from VTA were used to construct **Markowitz-optimized portfolios**. The resulting portfolio achieved the **highest Sharpe Ratio** among all tested models, indicating superior risk-adjusted returns. This is a strong validation from a practical, econometric perspective.\n",
        "\n",
        "#### **Conclusion and Professor's Critique**\n",
        "\n",
        "**Conclusion:** The paper successfully demonstrates that a hybrid framework combining verbal reasoning and latent time-series forecasting can achieve superior results in both accuracy and interpretability for financial technical analysis. The Time-GRPO objective with an inverse MSE reward is a clever mechanism for training quantitative reasoning without direct supervision.\n",
        "\n",
        "**Critique and Outlook:**\n",
        "\n",
        "*   **Strengths:** The dual-process \"Analyst + Quant\" paradigm is conceptually elegant and powerful. The methodology is technically sophisticated, particularly the use of a forecast-accuracy-based reward signal in the RL loop and the classifier-free guidance for conditioning. The three-pronged evaluation (accuracy, expert review, portfolio performance) is exceptionally thorough.\n",
        "*   **Potential Limitations:** The model's reasoning is constrained by the initial set of technical indicators provided in the annotation step. It can reason about them, but it cannot invent new ones. Furthermore, the system's performance during unprecedented market events or \"black swans\" remains an open question, as its knowledge is derived from historical patterns.\n",
        "*   **Future Directions:** This work opens up exciting avenues. The framework could be extended to incorporate multi-modal data, such as news text and sentiment, into the reasoning process. It could also be applied to other domains where numerical data requires interpretable reasoning, such as medical chart analysis or economic forecasting.\n",
        "\n",
        "In summary, Koa et al. have produced a first-rate piece of research that represents a significant step towards creating true hybrid intelligence in quantitative financeâ€”systems that not only predict accurately but can also explain *why* they are making those predictions in a manner that a human expert can trust and verify."
      ],
      "metadata": {
        "id": "tfqlup0nwfkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules\n"
      ],
      "metadata": {
        "id": "cvftq-52fZVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Verbal Technical Analysis (VTA): A Framework for Self-Explaining Financial Forecasting\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  Verbal Technical Analysis (VTA) framework presented in \"Reasoning on Time-Series\n",
        "#  for Financial Technical Analysis\" by Koa et al. (2025). It delivers a novel,\n",
        "#  hybrid system that fuses the pattern recognition capabilities of deep learning\n",
        "#  time-series models with the reasoning and natural language generation abilities\n",
        "#  of Large Language Models (LLMs). The result is a dual-output model that produces\n",
        "#  both a state-of-the-art numerical forecast and a coherent, data-driven analytical\n",
        "#  narrative explaining the forecast's rationale.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  â€¢ A three-stage Reinforcement Learning (RL) pipeline (Time-GRPO) to train an\n",
        "#    LLM to perform financial technical analysis from numerical data.\n",
        "#  â€¢ A dual-branch, cross-modal transformer backbone for deep time-series feature extraction.\n",
        "#  â€¢ A Classifier-Free Guidance (CFG) mechanism to fuse the LLM's high-level\n",
        "#    reasoning with the backbone's numerical prediction.\n",
        "#  â€¢ A complete, end-to-end experimental harness for data processing, multi-stage\n",
        "#    model training, baseline comparison, and evaluation.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  â€¢ High-performance, vectorized, and JIT-compiled technical indicator calculations.\n",
        "#  â€¢ Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA).\n",
        "#  â€¢ A robust, resumable pipeline with atomic artifact management for long-running experiments.\n",
        "#  â€¢ A professional-grade backtesting engine using Markowitz portfolio optimization\n",
        "#    to assess the practical financial utility of model forecasts.\n",
        "#  â€¢ Comprehensive hyperparameter sensitivity and model ablation analysis framework.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Koa, K. J. L., Chen, J., Ma, Y., Zheng, H., & Chua, T.-S. (2025).\n",
        "#  Reasoning on Time-Series for Financial Technical Analysis.\n",
        "#  arXiv preprint arXiv:2511.08616. https://arxiv.org/abs/2511.08616\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# Fused Imports for the Complete VTA Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Standard Library Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "from collections.abc import Mapping\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import (Any, Dict, List, Literal, NamedTuple, Optional, Tuple,\n",
        "                    Union)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Third-Party Imports\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import cvxpy as cp\n",
        "import numba\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (AutoModel, AutoModelForCausalLM, AutoTokenizer,\n",
        "                          DataCollatorForLanguageModeling, GenerationConfig,\n",
        "                          PreTrainedModel, PreTrainedTokenizer, Trainer,\n",
        "                          TrainingArguments)\n",
        "\n",
        "# `exchange_calendars` is an optional dependency for data cleansing.\n",
        "try:\n",
        "    import exchange_calendars as xcals\n",
        "except ImportError:\n",
        "    xcals = None\n",
        "\n",
        "# `peft` is a core dependency for LoRA.\n",
        "try:\n",
        "    from peft import LoraConfig, PeftModel, get_peft_model\n",
        "except ImportError:\n",
        "    LoraConfig, PeftModel, get_peft_model = None, None,\n",
        ""
      ],
      "metadata": {
        "id": "q1rGGrosfdqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "55PjUXqOffXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Analysis of Final Orchestrator Callables**\n",
        "\n",
        "#### **Callable: `validate_inputs_and_config` (Task 1)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    1.  `market_data_df`: A raw `pandas.DataFrame` with a potential `MultiIndex` and OHLCV columns.\n",
        "    2.  `config`: The master `VTA_MASTER_CONFIG` dictionary containing all project hyperparameters and schemas.\n",
        "\n",
        "*   **Process:**\n",
        "    1.  The function sequentially invokes a series of specialized validation helpers.\n",
        "    2.  It first validates the structural integrity of `market_data_df`: presence and correctness of the `['date', 'ticker']` `MultiIndex`, column names, and data types.\n",
        "    3.  It then validates the numerical and chronological integrity: checking for non-positive/non-finite values and ensuring dates are monotonically increasing for each ticker.\n",
        "    4.  It recursively traverses the `config` dictionary to ensure no `\"REQUIRED_\"` placeholders remain, guaranteeing the experiment is fully specified.\n",
        "    5.  It performs cross-consistency checks on key parameters within the `config` (e.g., ensuring `T` and `T'` are consistent across different sections).\n",
        "    6.  It validates the `prompt_template` configuration, ensuring all necessary placeholders exist and the regex for parsing predictions is valid.\n",
        "    7.  If any validation check fails, it aggregates all errors and raises a single, comprehensive `ValueError`.\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   `None`. The function returns nothing upon success. Its purpose is to act as a gatekeeper, halting execution via an exception if inputs are invalid.\n",
        "\n",
        "*   **Data Transformation:** This function performs no data transformation. It is a pure validation and assertion function that inspects the inputs without modifying them.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable is the foundational **Pre-flight Check** of the entire experiment. It ensures that the raw data and the experimental configuration are perfectly aligned with the assumptions and requirements of all subsequent stages. It programmatically enforces the structural integrity required to instantiate the mathematical objects of the study, such as the time-series window \\(X\\).\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `cleanse_and_prepare_data` (Task 2)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    1.  `raw_market_data_df`: The structurally validated `DataFrame` from Task 1.\n",
        "    2.  `ticker_to_market_map`: A dictionary mapping tickers to their exchange market (e.g., `'AAPL': 'US'`).\n",
        "\n",
        "*   **Process:**\n",
        "    1.  The function executes a three-step cleansing pipeline in a strict sequence.\n",
        "    2.  **Invalid Data Removal:** It first creates boolean masks to identify and remove any rows containing non-finite, non-positive prices or invalid volume figures.\n",
        "    3.  **Consistency Enforcement:** It then applies another set of masks to remove rows with logically inconsistent OHLC data (e.g., `Low > High`).\n",
        "    4.  **Calendar Alignment:** Finally, it uses the `exchange_calendars` library to identify and remove any rows corresponding to non-trading days (weekends, market holidays) for each ticker's specific market.\n",
        "\n",
        "*   **Outputs:**\n",
        "    1.  `cleansed_df`: A new `pandas.DataFrame` containing only valid, consistent, chronologically sorted data on official trading days.\n",
        "    2.  `reports`: A dictionary of `DataFrames`, providing a detailed audit trail of how many rows were dropped at each stage and for what reason.\n",
        "\n",
        "*   **Data Transformation:** This function transforms the raw, potentially noisy `DataFrame` into a pristine, analysis-ready `DataFrame`. The transformation is subtractive; it filters out invalid rows, reducing the number of samples while increasing the quality and reliability of the dataset.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable implements the **Data Sanitization** phase. Its purpose is to ensure the time-series data is free from corruption and temporal inconsistencies. This is a critical prerequisite for the windowing process in Task 3, guaranteeing that every constructed window \\(X\\) consists of a clean, unbroken sequence of valid trading days.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `construct_windows_and_split_data` (Task 3)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    1.  `cleansed_df`: The sanitized `DataFrame` from Task 2.\n",
        "    2.  `config`: The master configuration dictionary.\n",
        "    3.  `dataset_name`: A string specifying the dataset to determine the splitting strategy.\n",
        "\n",
        "*   **Process:**\n",
        "    1.  **Windowing:** The function iterates through the `cleansed_df` on a per-ticker basis. Using `numpy.lib.stride_tricks.as_strided` for memory efficiency, it creates overlapping views of the data to construct the input windows `X` and target windows `y`.\n",
        "    2.  **Metadata Generation:** For each window, it generates a rich metadata record, including the `ticker`, `window_start_date`, `window_end_date`, and the actual series of dates within the window.\n",
        "    3.  **Splitting:** It then partitions the complete set of windows into `train`, `val`, and `test` sets based on the `window_end_date` and the specified `dataset_name` strategy (chronological percentage or fixed date cutoff).\n",
        "    4.  **Reporting:** Finally, it generates a summary report detailing the number of windows in each split for each ticker.\n",
        "\n",
        "*   **Outputs:**\n",
        "    1.  `data_splits`: A dictionary containing the `train`, `val`, and `test` data, where each is a sub-dictionary holding the `X` windows, `y` targets, and `metadata`.\n",
        "    2.  `summary_report`: A `DataFrame` summarizing the window counts.\n",
        "    3.  `excluded_tickers`: A list of tickers with insufficient data for training.\n",
        "\n",
        "*   **Data Transformation:** This is the primary data structuring step of the pipeline. It transforms the 2D `(time, features)` `DataFrame` into a 3D `(samples, time_steps, features)` `numpy` array format required by deep learning models.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable is the implementation of the **Supervised Learning Problem Formulation**. It directly instantiates the core mathematical objects of the study:\n",
        "    *   The historical input window: \\(X = \\{x_{t-T+1}, \\dots, x_t\\}\\) where \\(x_\\tau \\in \\mathbb{R}^6\\).\n",
        "    *   The future price trajectory target: \\(y = \\{p_{t+1}, \\dots, p_{t+T'}\\}\\).\n",
        "    It creates the complete dataset of `(X, y)` pairs that will be used in all subsequent modeling tasks.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `compute_technical_annotations` (Task 4)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    1.  `X_windows`: A 3D `numpy` array of input windows.\n",
        "    2.  `config`: The master configuration dictionary.\n",
        "\n",
        "*   **Process:**\n",
        "    1.  **Statistics Calculation:** It first computes basic descriptive statistics (mean, min, max) for each of the 6 features across the time dimension of every window in a vectorized manner.\n",
        "    2.  **Indicator Calculation:** It then invokes a series of specialized, high-performance helper functions to calculate the full suite of 10 financial technical indicators (SMA, EMA, RSI, ADX, etc.) for each window, as defined in Appendix A, Table 6. These helpers are vectorized with `numpy` and JIT-compiled with `numba` for maximum performance.\n",
        "    3.  **Serialization:** Finally, it takes the numerical results from the previous two steps and serializes them into formatted, human-readable strings (`statistics_blocks` and `indicators_blocks`), handling `NaN` values gracefully.\n",
        "\n",
        "*   **Outputs:**\n",
        "    1.  `annotations`: A dictionary containing two lists of strings: `statistics_blocks` and `indicators_blocks`, perfectly aligned with the input `X_windows`.\n",
        "\n",
        "*   **Data Transformation:** This function transforms the numerical `X_windows` array into a structured textual representation. It is a feature engineering step that converts raw price-volume data into the domain-specific language of technical analysis.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable is the direct implementation of the **Textual Annotation Mapping**, a core novelty of the VTA framework. It instantiates the function \\(f\\) from the equation:\n",
        "    \\[ X' = f(X) \\]\n",
        "    where \\(X\\) is the numerical time-series window and \\(X'\\) is its textual, annotated representation. This \\(X'\\) is the primary input for the reasoning LLM.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `assemble_and_validate_prompts` (Task 5)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    1.  `X_windows`: The 3D `numpy` array of input windows.\n",
        "    2.  `metadata`: The rich metadata `DataFrame` from Task 3.\n",
        "    3.  `annotations`: The dictionary of annotation strings from Task 4.\n",
        "    4.  `config`: The master configuration dictionary.\n",
        "\n",
        "*   **Process:**\n",
        "    1.  **Prompt Assembly:** The function iterates through each window. For each window, it populates the `prompt_template` string from the `config` with the corresponding data: ticker, dates from `metadata`; the formatted `ohlcv_table` (created from `X_windows` and `metadata`); and the `statistics_block` and `indicators_block` from `annotations`.\n",
        "    2.  **Tokenization & Validation:** It then tokenizes every generated prompt string using the specified LLM's tokenizer and verifies that the token count does not exceed the model's context limit.\n",
        "    3.  **Parser Instantiation:** It instantiates and returns an `LLMOutputParser` object, which is configured with the parsing rules from the `config`.\n",
        "\n",
        "*   **Outputs:**\n",
        "    1.  `prompts`: A list of fully-formed, validated prompt strings `q`.\n",
        "    2.  `parser`: An `LLMOutputParser` instance ready for use in the training loop.\n",
        "\n",
        "*   **Data Transformation:** This function performs the final transformation of all prepared data artifacts (`X`, `X'`, metadata) into the exact textual format `q` that the reasoning LLM will receive as input.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable implements the **Prompt Engineering** stage. It creates the final input `q` for the reasoning policy `Ï€Î¸(o|q)`. The quality and consistency of these prompts are critical for the success of the subsequent RL training stages.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callables: `train_reasoning_llm_stage1`, `train_reasoning_llm_stage2`, `train_reasoning_llm_stage3` (Tasks 6, 7, 8)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   Stage 1: Training prompts `q` and targets `y`.\n",
        "    *   Stage 2: Artifacts from Stage 1 (experiences DataFrame, model path).\n",
        "    *   Stage 3: Best model checkpoint from Stage 2, training prompts `q`, and targets `y`.\n",
        "\n",
        "*   **Process:** These three orchestrators collectively implement the paper's three-stage training pipeline for the reasoning model `Ï€Î¸`.\n",
        "    1.  **Stage 1 (Cold-Start RL):** Initializes a base LLM with LoRA adapters and trains it using the Time-GRPO objective function. It repeatedly samples groups of `G` responses, calculates a combined format and inverse-MSE reward, computes group-relative advantages, and updates the policy using the PPO-style clipped surrogate objective with a KL penalty. It saves all generated experiences.\n",
        "    2.  **Stage 2 (Rejection Sampling & SFT):** Filters the experiences from Stage 1, keeping only the top-performing samples (bottom 10% MSE) within each `(ticker, time_period)` bucket. It then performs standard Supervised Fine-Tuning (SFT) on this \"golden\" dataset to distill high-quality reasoning patterns into the model.\n",
        "    3.  **Stage 3 (Performance RL):** Initializes the model from the best SFT checkpoint and resumes RL training with the same Time-GRPO objective as Stage 1. This final phase fine-tunes the policy for maximum forecast accuracy.\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   Stage 1: A dictionary containing the trained LoRA model path and a `DataFrame` of all collected experiences.\n",
        "    *   Stage 2: The file path to the best SFT model checkpoint.\n",
        "    *   Stage 3: The file path to the final, best-performing reasoning model `Ï€Î¸`.\n",
        "\n",
        "*   **Data Transformation:** These functions transform a dataset of prompts into a fully trained, parameter-efficient reasoning model. The data transformation is the learning process itself, where the model's weights (specifically, the LoRA adapters) are optimized.\n",
        "\n",
        "*   **Role in Research Pipeline:** These callables are the implementation of the **Verbal Reasoner Training**. They are the heart of the paper's methodology for teaching an LLM to perform technical analysis. They directly implement the core optimization algorithms:\n",
        "    *   The inverse-MSE reward: \\( r_{\\mathrm{MSE}}(\\theta) = 1 / (\\lambda \\cdot \\lVert \\hat{y}_\\theta - y \\rVert^2 + \\epsilon) \\)\n",
        "    *   The group-relative advantage: \\( A_i = (r_i - \\mathrm{mean}(\\{r_j\\})) / (\\mathrm{std}(\\{r_j\\}) + \\epsilon) \\)\n",
        "    *   The Time-GRPO loss function (a variant of Equation 2): \\( \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CLIP}} + \\mathcal{L}_{\\text{VF}} + \\beta D_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}}) \\)\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `train_forecasting_backbone` (Task 9)**\n",
        "\n",
        "*   **Inputs:** The `data_splits` dictionary containing `X_train` and `X_val`.\n",
        "*   **Process:**\n",
        "    1.  Initializes the `ForecastingBackbone` model `Ï†`, which includes performing PCA on a base LLM's embeddings to create the language prototypes `D`.\n",
        "    2.  Runs a standard supervised training loop. In each step, it performs a forward pass to get outputs from both the temporal and textual branches.\n",
        "    3.  It computes a combined loss consisting of the Feature Regularization Loss and the Output Alignment Loss.\n",
        "    4.  It uses a validation set to monitor performance and saves the best model checkpoint.\n",
        "\n",
        "*   **Outputs:** The file path to the best trained `ForecastingBackbone` model `Ï†`.\n",
        "\n",
        "*   **Data Transformation:** This function transforms the training dataset into a trained `ForecastingBackbone` model.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable implements the training of the **Latent Thinker** (`Ï†`). It directly implements the novel training objective designed to align the time-series domain with the language domain:\n",
        "    *   Cross-Modal Alignment via cross-attention (Equation 4): \\( X_{\\text{text}} = \\text{Softmax}(\\frac{QK^T}{\\sqrt{C}})V \\)\n",
        "    *   Feature Regularization Loss (Equation 5): \\( \\mathcal{L}_{\\mathrm{feature}} = \\sum \\gamma^{(N-n)} \\mathrm{sim}(\\phi_{\\text{text}}(F_{\\text{text}}^n), \\phi_{\\text{time}}(F_{\\text{time}}^n)) \\)\n",
        "    *   Output Alignment Loss (Equation 6): \\( \\mathcal{L}_{\\mathrm{output}} = \\mathrm{sim}(\\hat{y}_{\\mathrm{time}}, \\hat{y}_{\\mathrm{text}}) \\)\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `train_conditional_forecaster` (Task 10)**\n",
        "\n",
        "*   **Inputs:** Data splits, prompts, and paths to the trained reasoning model `Ï€Î¸` and backbone model `Ï†`.\n",
        "*   **Process:**\n",
        "    1.  **Attribute Derivation:** It first uses the trained `Ï€Î¸` to generate forecasts for all training/validation samples and derives the conditioning attributes `c = [min, mean, max]` for each.\n",
        "    2.  **Model Initialization:** It initializes the `ConditionalFusionModel` `Ïˆ`, loading the pre-trained backbone `Ï†` into it.\n",
        "    3.  **Training:** It runs a supervised training loop. In each forward pass, it randomly \"drops\" the conditioning vector `c` with probability `p_uncond`, replacing it with a null token. The loss is the MSE between the model's output and the ground truth `y`.\n",
        "    4.  **Validation:** The validation loop is crucial. It performs the full, two-pass guided inference procedure to calculate the final forecast `Å·` and computes the validation loss on this guided output. It saves the best model based on this metric.\n",
        "\n",
        "*   **Outputs:** The file path to the best trained `ConditionalFusionModel` `Ïˆ`.\n",
        "\n",
        "*   **Data Transformation:** This function transforms the trained `Ï€Î¸` and `Ï†` models, along with the training data, into the final, trained fusion model `Ïˆ`.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable implements the **Joint Conditional Training** stage (Section 3.4). This is the final fusion step that teaches the system how to blend the latent and verbal reasoning pathways. It directly implements:\n",
        "    *   The classifier-free guidance training objective (Equation 7): \\( \\mathcal{L}_{\\mathrm{forecast}}(\\psi) = \\mathbb{E} \\big[ \\lVert \\hat{y}_\\psi(X, \\tilde{c}) - y \\rVert^2 \\big] \\), where `Ä` is randomly dropped.\n",
        "    *   The validation process uses the guided inference equation (Equation 9): \\( \\hat{y} = \\hat{y}_\\phi(X) + s \\cdot \\big( \\hat{y}_\\psi(X, c) - \\hat{y}_\\phi(X) \\big) \\)\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `run_end_to_end_inference` (Task 11)**\n",
        "\n",
        "*   **Inputs:** Test data, test prompts, and paths to the final trained `Ï€Î¸` and `Ïˆ` models.\n",
        "*   **Process:** For each sample in the test set:\n",
        "    1.  It generates the narrative `v_Î¸` and the initial forecast `Å·_Î¸` using the reasoning model `Ï€Î¸`.\n",
        "    2.  It derives the conditioning attributes `c` from `Å·_Î¸`.\n",
        "    3.  It performs the two-pass guided inference using the fusion model `Ïˆ` to get the final forecast `Å·`.\n",
        "    4.  It handles cases where `Ï€Î¸` fails to generate a valid output by using a fallback strategy (using the unconditional forecast).\n",
        "\n",
        "*   **Outputs:** A `pandas.DataFrame` containing the final dual output for each test sample: the guided numerical forecast `Å·` and the textual narrative `v_Î¸`.\n",
        "\n",
        "*   **Data Transformation:** This function transforms the test set inputs into the final predictions and explanations.\n",
        "\n",
        "*   **Role in Research Pipeline:** This is the **Inference Engine**. It executes the complete, trained VTA framework to produce the final results that are then evaluated in the subsequent tasks.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `train_and_evaluate_baselines` (Task 12)**\n",
        "\n",
        "*   **Inputs:** The `data_splits`.\n",
        "*   **Process:** It uses a generic, rigorous training harness to train and evaluate the DLinear and TSMixer models. For each baseline, it runs a full training loop with validation-based checkpointing and then evaluates the best checkpoint on the test set to get MSE and MAE.\n",
        "*   **Outputs:** A dictionary containing the final test metrics for each baseline model.\n",
        "\n",
        "*   **Role in Research Pipeline:** This callable implements the **Baseline Model Evaluation**. Its purpose is to provide a set of strong, standard benchmarks against which the performance of the VTA model can be fairly compared.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `evaluate_performance_and_utility` (Task 13)**\n",
        "\n",
        "*   **Inputs:** Inference results for all models (VTA and baselines), ground truth data, and the `cleansed_df`.\n",
        "*   **Process:**\n",
        "    1.  **Error Metrics:** It computes and aggregates the MSE and MAE for all models, creating a comparison table.\n",
        "    2.  **Portfolio Backtest:** For each model, it runs a daily rebalancing backtest. On each day, it uses the model's forecasts to solve a Markowitz mean-variance optimization problem to get portfolio weights. It then calculates the realized return using actual market data.\n",
        "    3.  **Performance Metrics:** From the resulting daily return series of the backtest, it calculates annualized return, volatility, Sharpe ratio, and maximum drawdown.\n",
        "\n",
        "*   **Outputs:** A dictionary containing two `DataFrames`: one for the error metrics and one for the portfolio performance metrics.\n",
        "\n",
        "*   **Role in Research Pipeline:** This is the final **Results Analysis** stage. It translates the raw model predictions into the final, interpretable tables (Table 2 and Table 5 from the paper) that quantify the statistical accuracy and financial utility of the VTA framework relative to the baselines.\n",
        "\n",
        "\n",
        "\n",
        "#### **Callable: `run_vta_pipeline` (Task 14)**\n",
        "\n",
        "*   **Inputs:** Raw data, master config, and data mappings.\n",
        "*   **Process:** This is the master orchestrator. It initializes the experimental environment (setting seeds, configuring logging, creating a run directory), then sequentially calls the data preparation sub-orchestrator (`_run_data_preparation_pipeline`) and the modeling sub-orchestrator (`_run_modeling_pipeline`). It manages the overall workflow, including the `dry_run` mode.\n",
        "*   **Outputs:** A comprehensive dictionary containing all artifacts from the entire run.\n",
        "\n",
        "*   **Role in Research Pipeline:** This is the **Experiment Execution Engine**. It provides the single, top-level entry point to run the entire research project from start to finish in a reproducible, auditable, and resumable manner.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Examples**\n",
        "\n",
        "Here is a complete, example of how to use the end-to-end pipeline, starting from data generation and configuration loading, through to the final execution call.\n",
        "\n",
        "### **Pre-Execution Discussion**\n",
        "\n",
        "The goal is to create a self-contained, executable example that demonstrates the use of the `main` orchestrator function. This requires three preparatory steps:\n",
        "\n",
        "1.  **Synthetic Data Generation:** We cannot run the pipeline without realistic input data. We will create a helper function, `_generate_synthetic_market_data`, that uses `pandas` and `numpy` to generate a `market_data_df` DataFrame that perfectly matches the required schema. It will simulate realistic price movements (a random walk with drift) and ensure the OHLCV constraints (`Low <= Open/Close <= High`) are respected. This data will be saved to a local CSV file, `synthetic_market_data.csv`, mimicking a real-world scenario where data is loaded from disk.\n",
        "\n",
        "2.  **Configuration Loading:** The `config.yaml` file is the master configuration. We will use the `PyYAML` library to load this file into a Python dictionary named `config`. This is the standard, robust way to manage configurations in a Python application.\n",
        "\n",
        "3.  **Input Parameter Definition:** We will explicitly define the other necessary inputs for the `main` function: the `ticker_to_market_map`, `ticker_to_dataset_map`, `dataset_name`, and a `base_run_id`. These will be defined directly in the script for clarity.\n",
        "\n",
        "4.  **Execution Call:** Finally, we will show the exact call to the `main` function, passing all the prepared inputs. The example will demonstrate how to run a `dry_run` for quick testing, which is a critical feature of the pipeline we designed.\n",
        "\n",
        "This step-by-step process ensures that the example is not just a code snippet, but a complete, runnable demonstration of the entire project's entry point:\n",
        "<br>\n",
        "\n",
        "### **Implementation: End-to-End Pipeline Execution Example**\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# Example Usage: Running the Complete VTA Pipeline\n",
        "# ==============================================================================\n",
        "# This script demonstrates how to use the top-level orchestrator (`main` function)\n",
        "# to run the entire VTA research pipeline from start to finish.\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Preamble: Import necessary libraries for this example script\n",
        "# ------------------------------------------------------------------------------\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# Assume all previously defined orchestrators and helpers (Tasks 1-15) are\n",
        "# available in the execution scope (e.g., in a single Jupyter notebook or imported).\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Generate High-Fidelity Synthetic Input Data\n",
        "# ------------------------------------------------------------------------------\n",
        "# In a real-world scenario, this data would be sourced from a financial data\n",
        "# provider. For this example, we generate a realistic, synthetic dataset that\n",
        "# adheres to the required schema.\n",
        "\n",
        "def _generate_synthetic_market_data(\n",
        "    tickers: List[str],\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    output_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates a realistic, synthetic OHLCV dataset and saves it to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        tickers: A list of ticker symbols to generate data for.\n",
        "        start_date: The start date for the data range in 'YYYY-MM-DD' format.\n",
        "        end_date: The end date for the data range.\n",
        "        output_path: The file path to save the generated CSV.\n",
        "    \"\"\"\n",
        "    print(f\"Generating synthetic market data for tickers: {tickers}...\")\n",
        "    \n",
        "    # Create a date range of business days.\n",
        "    dates = pd.bdate_range(start=start_date, end=end_date)\n",
        "    \n",
        "    all_ticker_data = []\n",
        "    \n",
        "    for ticker in tickers:\n",
        "        # Simulate a price series using a geometric random walk with a small drift.\n",
        "        num_days = len(dates)\n",
        "        returns = np.random.normal(loc=0.0005, scale=0.02, size=num_days)\n",
        "        # Start the price at a random value between 50 and 200.\n",
        "        initial_price = np.random.uniform(50, 200)\n",
        "        price_series = initial_price * (1 + returns).cumprod()\n",
        "        \n",
        "        # Create realistic OHLC values around the simulated price series.\n",
        "        # Ensure that Low <= Open/Close <= High.\n",
        "        open_prices = price_series * np.random.uniform(0.995, 1.005, size=num_days)\n",
        "        close_prices = price_series * np.random.uniform(0.995, 1.005, size=num_days)\n",
        "        \n",
        "        low_prices = np.minimum(open_prices, close_prices) * np.random.uniform(0.98, 1.0, size=num_days)\n",
        "        high_prices = np.maximum(open_prices, close_prices) * np.random.uniform(1.0, 1.02, size=num_days)\n",
        "        \n",
        "        # Assume Adj Close is the same as Close for simplicity in this synthetic data.\n",
        "        adj_close_prices = close_prices\n",
        "        \n",
        "        # Simulate trading volume.\n",
        "        volume = np.random.randint(1_000_000, 50_000_000, size=num_days)\n",
        "        \n",
        "        # Create a DataFrame for the current ticker.\n",
        "        ticker_df = pd.DataFrame({\n",
        "            'Open': open_prices,\n",
        "            'High': high_prices,\n",
        "            'Low': low_prices,\n",
        "            'Close': close_prices,\n",
        "            'Adj Close': adj_close_prices,\n",
        "            'Volume': volume\n",
        "        }, index=dates)\n",
        "        \n",
        "        ticker_df['ticker'] = ticker\n",
        "        all_ticker_data.append(ticker_df)\n",
        "        \n",
        "    # Concatenate data for all tickers and set the MultiIndex.\n",
        "    market_data_df = pd.concat(all_ticker_data)\n",
        "    market_data_df = market_data_df.reset_index().rename(columns={'index': 'date'})\n",
        "    market_data_df = market_data_df.set_index(['date', 'ticker'])\n",
        "    \n",
        "    # Save the final DataFrame to the specified path.\n",
        "    market_data_df.to_csv(output_path)\n",
        "    print(f\"Synthetic data saved to: {output_path}\")\n",
        "\n",
        "# --- Generate the data for our example run ---\n",
        "# Define the universe of tickers for our synthetic dataset.\n",
        "synthetic_tickers = ['SYNTH_A', 'SYNTH_B', 'SYNTH_C', 'SYNTH_D']\n",
        "# Define the path where the data will be saved.\n",
        "market_data_filepath = \"./synthetic_market_data.csv\"\n",
        "\n",
        "# Execute the data generation function.\n",
        "_generate_synthetic_market_data(\n",
        "    tickers=synthetic_tickers,\n",
        "    start_date='2020-01-01',\n",
        "    end_date='2024-12-31',\n",
        "    output_path=market_data_filepath\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Load the Master Configuration File\n",
        "# ------------------------------------------------------------------------------\n",
        "# The `config.yaml` file, as defined previously, contains all hyperparameters.\n",
        "# We load it into a Python dictionary.\n",
        "\n",
        "config_filepath = \"./config.yaml\"\n",
        "\n",
        "print(f\"\\nLoading master configuration from: {config_filepath}\")\n",
        "with open(config_filepath, 'r') as f:\n",
        "    # Use the safe_load function to prevent arbitrary code execution.\n",
        "    config = yaml.safe_load(f)\n",
        "print(\"Configuration loaded successfully.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Define Other Input Parameters for the Pipeline\n",
        "# ------------------------------------------------------------------------------\n",
        "# These are the data mappings required by the pipeline orchestrators. In a real\n",
        "# project, these might also be loaded from a file.\n",
        "\n",
        "# Map each ticker to its geographical market for calendar alignment.\n",
        "ticker_to_market_map = {\n",
        "    'SYNTH_A': 'US',\n",
        "    'SYNTH_B': 'US',\n",
        "    'SYNTH_C': 'US',\n",
        "    'SYNTH_D': 'US'\n",
        "}\n",
        "\n",
        "# Map each ticker to its source dataset for correct metric aggregation.\n",
        "# For this example, we'll assign them all to a mock \"StockNet\" dataset.\n",
        "ticker_to_dataset_map = {\n",
        "    'SYNTH_A': 'StockNet',\n",
        "    'SYNTH_B': 'StockNet',\n",
        "    'SYNTH_C': 'StockNet',\n",
        "    'SYNTH_D': 'StockNet'\n",
        "}\n",
        "\n",
        "# Define the primary dataset name to guide the data splitting strategy.\n",
        "# This must match a key in the `splits` section of the config.\n",
        "dataset_name = \"StockNet\"\n",
        "\n",
        "# Define a base identifier for this experimental run. All artifacts will be\n",
        "# grouped under this name.\n",
        "base_run_id = \"vta_official_run\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 4: Execute the End-to-End Pipeline\n",
        "# ------------------------------------------------------------------------------\n",
        "# This is the final step where we call the top-level `main` orchestrator.\n",
        "# We will run it in `dry_run=True` mode first to demonstrate a quick test of\n",
        "# the entire pipeline's integrity.\n",
        "\n",
        "print(f\"\\n{'='*80}\\nEXECUTING THE VTA PIPELINE\\n{'='*80}\")\n",
        "\n",
        "try:\n",
        "    # The main execution call.\n",
        "    # We set `dry_run=True` for a quick, end-to-end validation of the code.\n",
        "    # To run the full experiment, set `dry_run=False`.\n",
        "    # To skip the sensitivity analysis, set `run_sensitivity=False`.\n",
        "    final_artifacts = main(\n",
        "        market_data_path=market_data_filepath,\n",
        "        config=config,\n",
        "        ticker_to_market_map=ticker_to_market_map,\n",
        "        ticker_to_dataset_map=ticker_to_dataset_map,\n",
        "        dataset_name=dataset_name,\n",
        "        base_run_id=base_run_id,\n",
        "        run_sensitivity=False, # Set to False for a quicker example run\n",
        "        dry_run=True\n",
        "    )\n",
        "    \n",
        "    print(\"\\n--- PIPELINE EXECUTION SUCCEEDED ---\")\n",
        "    # You can now inspect the `final_artifacts` dictionary for all results.\n",
        "    # For example, to see the final evaluation tables:\n",
        "    # final_evals = final_artifacts['baseline_run_artifacts']['modeling_and_evaluation']['final_evaluation_results']\n",
        "    # print(\"\\nFinal Error Metrics:\")\n",
        "    # print(final_evals['error_metrics'])\n",
        "    # print(\"\\nFinal Portfolio Metrics:\")\n",
        "    # print(final_evals['portfolio_metrics'])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n--- PIPELINE EXECUTION FAILED ---\")\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    # In a production script, you would log the full traceback here.\n",
        "\n",
        "```\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "949cJJ8awyG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate Input Schemas and Parameter Completeness\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate Input Schemas and Parameter Completeness\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Validate market_data_df structure and integrity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_market_data_df_structure(\n",
        "    market_data_df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure, index, and column properties of the market_data_df.\n",
        "\n",
        "    Args:\n",
        "        market_data_df: The DataFrame containing market data.\n",
        "\n",
        "    Returns:\n",
        "        A list of error messages. An empty list indicates successful validation.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate validation error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Check 1: Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(market_data_df, pd.DataFrame):\n",
        "        # If not a DataFrame, no other checks can be performed. Return immediately.\n",
        "        return [\"'market_data_df' must be a pandas DataFrame.\"]\n",
        "\n",
        "    # Check 2: Validate the MultiIndex structure.\n",
        "    if not isinstance(market_data_df.index, pd.MultiIndex):\n",
        "        # The index must be a MultiIndex for subsequent checks.\n",
        "        errors.append(\"DataFrame index must be a pandas MultiIndex.\")\n",
        "    elif market_data_df.index.nlevels != 2:\n",
        "        # The MultiIndex must have exactly two levels.\n",
        "        errors.append(f\"MultiIndex must have 2 levels, but found {market_data_df.index.nlevels}.\")\n",
        "    elif list(market_data_df.index.names) != [\"date\", \"ticker\"]:\n",
        "        # The index levels must be named 'date' and 'ticker' in that specific order.\n",
        "        errors.append(f\"Index names must be ['date', 'ticker'], but found {market_data_df.index.names}.\")\n",
        "    else:\n",
        "        # Check 2a: Validate the dtypes of the index levels.\n",
        "        if not pd.api.types.is_datetime64_any_dtype(market_data_df.index.get_level_values('date')):\n",
        "            errors.append(\"Index level 'date' must have a datetime64 dtype.\")\n",
        "        if not pd.api.types.is_string_dtype(market_data_df.index.get_level_values('ticker')):\n",
        "            errors.append(\"Index level 'ticker' must have a string/object dtype.\")\n",
        "\n",
        "    # Check 3: Validate the uniqueness of the (date, ticker) index pairs.\n",
        "    if not market_data_df.index.is_unique:\n",
        "        # Duplicate index entries can corrupt time-series analysis.\n",
        "        errors.append(\"DataFrame has duplicate (date, ticker) index pairs.\")\n",
        "\n",
        "    # Check 4: Validate the required columns and their dtypes.\n",
        "    required_columns = {\n",
        "        \"Open\": \"float64\", \"High\": \"float64\", \"Low\": \"float64\",\n",
        "        \"Close\": \"float64\", \"Adj Close\": \"float64\", \"Volume\": [\"int64\", \"float64\"]\n",
        "    }\n",
        "    missing_columns = set(required_columns.keys()) - set(market_data_df.columns)\n",
        "    if missing_columns:\n",
        "        # All required columns must be present for the model to function.\n",
        "        errors.append(f\"Missing required columns: {sorted(list(missing_columns))}.\")\n",
        "\n",
        "    for col, expected_dtype in required_columns.items():\n",
        "        # Check dtypes for each required column.\n",
        "        if col in market_data_df.columns:\n",
        "            actual_dtype = str(market_data_df[col].dtype)\n",
        "            # Volume can be int64 or float64, others must be float64.\n",
        "            is_valid_dtype = (\n",
        "                actual_dtype in expected_dtype\n",
        "                if isinstance(expected_dtype, list)\n",
        "                else actual_dtype == expected_dtype\n",
        "            )\n",
        "            if not is_valid_dtype:\n",
        "                errors.append(f\"Column '{col}' has dtype '{actual_dtype}', but expected {expected_dtype}.\")\n",
        "\n",
        "    # Return the aggregated list of errors.\n",
        "    return errors\n",
        "\n",
        "def _validate_market_data_df_integrity(\n",
        "    market_data_df: pd.DataFrame,\n",
        "    min_history: int\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the numerical and chronological integrity of the market_data_df.\n",
        "\n",
        "    Args:\n",
        "        market_data_df: The DataFrame containing market data.\n",
        "        min_history: The minimum number of trading days required per ticker (T + T').\n",
        "\n",
        "    Returns:\n",
        "        A list of error messages. An empty list indicates successful validation.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate validation error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Check 5: Validate numerical integrity of price and volume data.\n",
        "    price_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]\n",
        "    for col in price_cols:\n",
        "        # Prices must be positive and finite.\n",
        "        if col in market_data_df.columns:\n",
        "            if not market_data_df[col].gt(0).all():\n",
        "                errors.append(f\"Column '{col}' contains non-positive values.\")\n",
        "            if not np.isfinite(market_data_df[col]).all():\n",
        "                errors.append(f\"Column '{col}' contains non-finite (NaN or Inf) values.\")\n",
        "\n",
        "    if \"Volume\" in market_data_df.columns:\n",
        "        # Volume must be non-negative and finite.\n",
        "        if not market_data_df[\"Volume\"].ge(0).all():\n",
        "            errors.append(\"Column 'Volume' contains negative values.\")\n",
        "        if not np.isfinite(market_data_df[\"Volume\"]).all():\n",
        "            errors.append(\"Column 'Volume' contains non-finite (NaN or Inf) values.\")\n",
        "\n",
        "    # Check 6: Validate chronological order and minimum history per ticker.\n",
        "    if 'ticker' in market_data_df.index.names:\n",
        "        # Group data by ticker to perform per-asset checks.\n",
        "        grouped_by_ticker = market_data_df.groupby(level='ticker', sort=False)\n",
        "\n",
        "        # Check 6a: Chronological order (monotonic increasing dates).\n",
        "        non_monotonic_tickers = [\n",
        "            ticker for ticker, group in grouped_by_ticker\n",
        "            if not group.index.get_level_values('date').is_monotonic_increasing\n",
        "        ]\n",
        "        if non_monotonic_tickers:\n",
        "            errors.append(f\"Date index is not strictly increasing for tickers: {non_monotonic_tickers[:5]}.\")\n",
        "\n",
        "        # Check 6b: Minimum history requirement.\n",
        "        ticker_counts = grouped_by_ticker.size()\n",
        "        insufficient_history_tickers = ticker_counts[ticker_counts < min_history].index.tolist()\n",
        "        if insufficient_history_tickers:\n",
        "            errors.append(\n",
        "                f\"Tickers with insufficient history (< {min_history} days): \"\n",
        "                f\"{insufficient_history_tickers[:5]}.\"\n",
        "            )\n",
        "\n",
        "    # Return the aggregated list of errors.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate VTA_MASTER_CONFIG for completeness and consistency\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _find_required_keys(\n",
        "    config: Dict[str, Any],\n",
        "    path: str = \"\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively traverses a dictionary to find placeholder values starting with \"REQUIRED_\".\n",
        "\n",
        "    Args:\n",
        "        config: The dictionary or sub-dictionary to scan.\n",
        "        path: The current path for tracking nested keys (used internally).\n",
        "\n",
        "    Returns:\n",
        "        A list of full key paths that still contain \"REQUIRED_\" placeholders.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store paths of keys with placeholder values.\n",
        "    unfilled_keys = []\n",
        "    # Iterate over each key-value pair in the current dictionary level.\n",
        "    for key, value in config.items():\n",
        "        # Construct the full path for the current key.\n",
        "        current_path = f\"{path}.{key}\" if path else key\n",
        "        # If the value is another dictionary, recurse into it.\n",
        "        if isinstance(value, Mapping):\n",
        "            unfilled_keys.extend(_find_required_keys(value, current_path))\n",
        "        # If the value is a string that starts with the placeholder prefix...\n",
        "        elif isinstance(value, str) and value.startswith(\"REQUIRED_\"):\n",
        "            # ...add its full path to the list of unfilled keys.\n",
        "            unfilled_keys.append(current_path)\n",
        "    # Return the list of all found placeholder paths.\n",
        "    return unfilled_keys\n",
        "\n",
        "def _validate_config_consistency(config: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs consistency checks across different sections of the configuration.\n",
        "\n",
        "    Args:\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of consistency error messages.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate consistency error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Attempt to access nested keys with error handling.\n",
        "    try:\n",
        "        # Consistency Check 1: Prediction horizons must match.\n",
        "        # The target specification horizon (T') must be identical to the task's prediction horizon.\n",
        "        target_horizon = config[\"raw_data_schemas\"][\"market_data_df\"][\"target_spec\"][\"horizon\"]\n",
        "        pred_horizon = config[\"data_and_task_spec\"][\"prediction_horizon\"]\n",
        "        if target_horizon != pred_horizon:\n",
        "            errors.append(\n",
        "                f\"Mismatch: raw_data_schemas.target_spec.horizon ({target_horizon}) != \"\n",
        "                f\"data_and_task_spec.prediction_horizon ({pred_horizon}).\"\n",
        "            )\n",
        "\n",
        "        # Consistency Check 2: Fixed window size and stride must be as specified.\n",
        "        # The paper defines a fixed input window size (T) of 10.\n",
        "        input_window = config[\"data_and_task_spec\"][\"input_window_size\"]\n",
        "        if input_window != 10:\n",
        "            errors.append(f\"data_and_task_spec.input_window_size must be 10, but found {input_window}.\")\n",
        "\n",
        "        # The paper implies a stride of 1 for dense windowing.\n",
        "        stride = config[\"data_and_task_spec\"][\"stride\"]\n",
        "        if stride != 1:\n",
        "            errors.append(f\"data_and_task_spec.stride must be 1, but found {stride}.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # If a required key for a consistency check is missing, report it.\n",
        "        errors.append(f\"Missing key for consistency check: {e}.\")\n",
        "\n",
        "    # Return the aggregated list of errors.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate prompt template and output format constraints\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_prompt_config(config: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the reasoning model's prompt template and format enforcement rules.\n",
        "\n",
        "    Args:\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of prompt-related validation error messages.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate validation error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Attempt to access nested keys with error handling.\n",
        "    try:\n",
        "        # Extract the prompt configuration section for easier access.\n",
        "        prompt_config = config[\"reasoning_model_config\"][\"prompt_template\"]\n",
        "        template_str = prompt_config[\"template_string\"]\n",
        "\n",
        "        # Check 1: Ensure required placeholders are present in the template string.\n",
        "        # The prompt must be able to display raw data, statistics, and indicators.\n",
        "        required_placeholders = [\"{ohlcv_table}\", \"{statistics_block}\", \"{indicators_block}\"]\n",
        "        for placeholder in required_placeholders:\n",
        "            if placeholder not in template_str:\n",
        "                errors.append(f\"Prompt template is missing required placeholder: {placeholder}.\")\n",
        "\n",
        "        # Check 2: Validate format enforcement rules.\n",
        "        # The model's output must be strictly structured for reliable parsing and reward calculation.\n",
        "        enforcement_rules = prompt_config[\"format_enforcement\"]\n",
        "        if not enforcement_rules.get(\"require_think_tags\"):\n",
        "            errors.append(\"format_enforcement.require_think_tags must be True.\")\n",
        "        if not enforcement_rules.get(\"require_prediction_line\"):\n",
        "            errors.append(\"format_enforcement.require_prediction_line must be True.\")\n",
        "\n",
        "        # Check 3: Validate the prediction parsing regex.\n",
        "        # The regex must be a valid pattern to be used for parsing.\n",
        "        prediction_pattern = enforcement_rules.get(\"prediction_pattern\")\n",
        "        if not prediction_pattern or not isinstance(prediction_pattern, str):\n",
        "            errors.append(\"format_enforcement.prediction_pattern must be a non-empty string.\")\n",
        "        else:\n",
        "            try:\n",
        "                # Attempt to compile the regex to check for syntax errors.\n",
        "                re.compile(prediction_pattern)\n",
        "            except re.error as e:\n",
        "                errors.append(f\"Invalid regex for prediction_pattern: {e}.\")\n",
        "\n",
        "        # Check 4: Validate stop sequences.\n",
        "        # Stop sequences help prevent the model from generating extraneous text.\n",
        "        stop_sequences = prompt_config.get(\"stop_sequences\")\n",
        "        if not isinstance(stop_sequences, list) or not all(isinstance(s, str) for s in stop_sequences):\n",
        "            errors.append(\"stop_sequences must be a list of strings.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # If a required key for prompt validation is missing, report it.\n",
        "        errors.append(f\"Missing key for prompt validation: {e}.\")\n",
        "\n",
        "    # Return the aggregated list of errors.\n",
        "    return errors\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_inputs_and_config(\n",
        "    market_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of input data and the master configuration dictionary.\n",
        "\n",
        "    This function serves as the main entry point for Task 1, sequentially calling\n",
        "    all validation helpers. It aggregates any errors found and raises a single,\n",
        "    comprehensive ValueError if any validation check fails.\n",
        "\n",
        "    Args:\n",
        "        market_data_df: The primary DataFrame containing historical market data,\n",
        "                        expected to have a MultiIndex of ('date', 'ticker').\n",
        "        config: The master configuration dictionary (VTA_MASTER_CONFIG) that\n",
        "                governs the entire pipeline.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation check fails, containing a detailed list\n",
        "                    of all identified issues.\n",
        "        TypeError: If inputs are of the wrong type.\n",
        "    \"\"\"\n",
        "    # Perform initial type checking on the main inputs.\n",
        "    if not isinstance(market_data_df, pd.DataFrame):\n",
        "        raise TypeError(\"'market_data_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"'config' must be a dictionary.\")\n",
        "\n",
        "    # Initialize a list to aggregate all error messages from all validation steps.\n",
        "    all_errors = []\n",
        "\n",
        "    # --- Step 1: Validate market_data_df ---\n",
        "    # First, validate the fundamental structure (index, columns, dtypes).\n",
        "    structural_errors = _validate_market_data_df_structure(market_data_df)\n",
        "    all_errors.extend(structural_errors)\n",
        "\n",
        "    # Only proceed to integrity checks if the structure is fundamentally sound.\n",
        "    if not structural_errors:\n",
        "        # Calculate the minimum history required based on the config.\n",
        "        try:\n",
        "            min_history = (\n",
        "                config[\"data_and_task_spec\"][\"input_window_size\"] +\n",
        "                config[\"data_and_task_spec\"][\"prediction_horizon\"]\n",
        "            )\n",
        "            # Validate numerical and chronological integrity.\n",
        "            integrity_errors = _validate_market_data_df_integrity(market_data_df, min_history)\n",
        "            all_errors.extend(integrity_errors)\n",
        "        except KeyError as e:\n",
        "            all_errors.append(f\"Cannot check data integrity due to missing config key: {e}\")\n",
        "\n",
        "    # --- Step 2: Validate VTA_MASTER_CONFIG ---\n",
        "    # Check for any \"REQUIRED_\" placeholders that have not been filled.\n",
        "    unfilled_keys = _find_required_keys(config)\n",
        "    if unfilled_keys:\n",
        "        all_errors.append(f\"Configuration has unfilled 'REQUIRED' keys: {unfilled_keys}\")\n",
        "\n",
        "    # Check for consistency between different parts of the configuration.\n",
        "    consistency_errors = _validate_config_consistency(config)\n",
        "    all_errors.extend(consistency_errors)\n",
        "\n",
        "    # --- Step 3: Validate prompt configuration ---\n",
        "    # Check the prompt template and format enforcement rules.\n",
        "    prompt_errors = _validate_prompt_config(config)\n",
        "    all_errors.extend(prompt_errors)\n",
        "\n",
        "    # --- Final Check ---\n",
        "    # If any errors were found during any validation step...\n",
        "    if all_errors:\n",
        "        # ...compile them into a single, formatted error message and raise a ValueError.\n",
        "        error_message = \"Input validation failed with the following issues:\\n\" + \\\n",
        "                        \"\\n\".join(f\"- {error}\" for error in all_errors)\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    # If no errors were found, the function completes silently, indicating success.\n",
        "    print(\"Task 1: All input schemas and parameters validated successfully.\")\n"
      ],
      "metadata": {
        "id": "2P852Lpffi3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Cleanse and Prepare Raw Market Data\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Cleanse and Prepare Raw Market Data\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Remove rows with missing or invalid data\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _remove_invalid_data(\n",
        "    market_data_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Removes rows with missing, non-finite, or invalid numerical data.\n",
        "\n",
        "    This function performs the first stage of cleansing by targeting data points\n",
        "    that are fundamentally unusable for quantitative analysis. This includes:\n",
        "    - Any row with null or non-finite values in price columns.\n",
        "    - Any row with non-positive prices.\n",
        "    - Any row with null, non-finite, or negative volume.\n",
        "\n",
        "    Args:\n",
        "        market_data_df: The raw DataFrame, assumed to have passed initial\n",
        "                        structural validation from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A new DataFrame with invalid rows removed.\n",
        "        - A summary DataFrame reporting the count of rows dropped per ticker\n",
        "          for each specific reason.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = market_data_df.copy()\n",
        "\n",
        "    # Initialize a DataFrame to log the reasons for row removal.\n",
        "    # This provides a detailed audit trail of the cleansing process.\n",
        "    report = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Define the columns to check for price and volume integrity.\n",
        "    price_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]\n",
        "\n",
        "    # --- Identify Invalid Data Points ---\n",
        "\n",
        "    # Condition 1: Check for null or non-finite values in price columns.\n",
        "    # np.isfinite checks for both NaN and infinity.\n",
        "    report['is_non_finite_price'] = ~df[price_cols].apply(np.isfinite).all(axis=1)\n",
        "\n",
        "    # Condition 2: Check for non-positive values in price columns.\n",
        "    # Prices must be strictly positive.\n",
        "    report['is_non_positive_price'] = df[price_cols].le(0).any(axis=1)\n",
        "\n",
        "    # Condition 3: Check for null, non-finite, or negative volume.\n",
        "    # Volume must be a non-negative, finite number.\n",
        "    report['is_non_finite_volume'] = ~df['Volume'].apply(np.isfinite)\n",
        "    report['is_negative_volume'] = df['Volume'] < 0\n",
        "\n",
        "    # Combine all invalid volume conditions into a single flag.\n",
        "    report['is_invalid_volume'] = report['is_non_finite_volume'] | report['is_negative_volume']\n",
        "\n",
        "    # --- Filter Data and Generate Report ---\n",
        "\n",
        "    # Create a final mask to identify any row that violates at least one condition.\n",
        "    rows_to_drop_mask = (\n",
        "        report['is_non_finite_price'] |\n",
        "        report['is_non_positive_price'] |\n",
        "        report['is_invalid_volume']\n",
        "    )\n",
        "\n",
        "    # Extract the tickers and reasons for the rows that will be dropped.\n",
        "    dropped_report_data = report.loc[rows_to_drop_mask]\n",
        "\n",
        "    # Generate a summary report by ticker and reason.\n",
        "    # This aggregates the boolean flags to count violations.\n",
        "    summary_report = (\n",
        "        dropped_report_data\n",
        "        .reset_index()\n",
        "        .groupby('ticker')[['is_non_finite_price', 'is_non_positive_price', 'is_invalid_volume']]\n",
        "        .sum()\n",
        "        .astype(int)\n",
        "    )\n",
        "    summary_report = summary_report[summary_report.sum(axis=1) > 0]\n",
        "\n",
        "    # Drop the identified invalid rows from the DataFrame.\n",
        "    # We use the inverted mask to keep only the valid rows.\n",
        "    cleaned_df = df[~rows_to_drop_mask].copy()\n",
        "\n",
        "    # Re-sort the index to ensure chronological order is maintained after deletions.\n",
        "    # This is critical for subsequent time-series operations.\n",
        "    cleaned_df.sort_index(inplace=True)\n",
        "\n",
        "    return cleaned_df, summary_report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Enforce intra-day price consistency\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _enforce_price_consistency(\n",
        "    market_data_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Removes rows where intra-day OHLC prices are logically inconsistent.\n",
        "\n",
        "    This function enforces the fundamental logic of an OHLC bar:\n",
        "    - The 'Low' must be the minimum price.\n",
        "    - The 'High' must be the maximum price.\n",
        "    Any row violating these rules represents corrupt data and is removed. The\n",
        "    default outlier policy is \"no winsorization\", as specified.\n",
        "\n",
        "    Args:\n",
        "        market_data_df: The DataFrame after initial invalid data removal.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A new DataFrame with inconsistent rows removed.\n",
        "        - A summary DataFrame reporting the count of rows dropped per ticker\n",
        "          due to consistency violations.\n",
        "    \"\"\"\n",
        "    # Create a copy to ensure the original DataFrame is not modified.\n",
        "    df = market_data_df.copy()\n",
        "\n",
        "    # --- Identify Inconsistent Rows ---\n",
        "\n",
        "    # Define the logical consistency constraints for an OHLC bar.\n",
        "    # Using vectorized operations for maximum performance.\n",
        "    inconsistency_mask = (\n",
        "        (df['Low'] > df['Open']) |\n",
        "        (df['Low'] > df['High']) |\n",
        "        (df['Low'] > df['Close']) |\n",
        "        (df['High'] < df['Open']) |\n",
        "        (df['High'] < df['Close'])\n",
        "    )\n",
        "\n",
        "    # --- Filter Data and Generate Report ---\n",
        "\n",
        "    # Generate a summary report of dropped rows before filtering.\n",
        "    # This captures which tickers had consistency issues.\n",
        "    dropped_tickers = df.loc[inconsistency_mask].index.get_level_values('ticker')\n",
        "    summary_report = (\n",
        "        dropped_tickers.value_counts()\n",
        "        .to_frame('consistency_violations')\n",
        "        .sort_index()\n",
        "    )\n",
        "\n",
        "    # Use the inverted mask to select only the rows that are consistent.\n",
        "    cleaned_df = df[~inconsistency_mask].copy()\n",
        "\n",
        "    # The index is already sorted from the previous step, so no re-sort is needed.\n",
        "\n",
        "    return cleaned_df, summary_report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Align trading calendars\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _align_to_trading_calendars(\n",
        "    market_data_df: pd.DataFrame,\n",
        "    ticker_to_market_map: Dict[str, str]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to include only rows on valid trading days.\n",
        "\n",
        "    This function uses the `exchange_calendars` library to ensure that every\n",
        "    data point corresponds to an actual trading day for its respective market,\n",
        "    removing any data from weekends or market holidays. It does not forward-fill,\n",
        "    as per the specified constraints.\n",
        "\n",
        "    Args:\n",
        "        market_data_df: The DataFrame after consistency checks.\n",
        "        ticker_to_market_map: A dictionary mapping each ticker to its market\n",
        "                                identifier (e.g., 'US', 'CN', 'EU').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A new DataFrame with non-trading days removed.\n",
        "        - A summary DataFrame reporting the count of rows dropped per ticker\n",
        "          due to calendar misalignment.\n",
        "\n",
        "    Raises:\n",
        "        ImportError: If `exchange_calendars` is not installed.\n",
        "        ValueError: If a ticker in the data is not in the map, or if a market\n",
        "                    in the map is unsupported.\n",
        "    \"\"\"\n",
        "    # --- Pre-computation and Validation ---\n",
        "    try:\n",
        "        # Dynamically import the required library.\n",
        "        import exchange_calendars as xcals\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install the 'exchange_calendars' library via 'pip install exchange_calendars'\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = market_data_df.copy()\n",
        "\n",
        "    # Map the high-level market identifiers from the config to specific\n",
        "    # exchange codes recognized by the `exchange_calendars` library.\n",
        "    market_to_exchange_code = {\n",
        "        'US': 'XNYS',  # New York Stock Exchange\n",
        "        'CN': 'SSE',   # Shanghai Stock Exchange\n",
        "        'EU': 'XFRA',  # Frankfurt Stock Exchange\n",
        "    }\n",
        "\n",
        "    # Validate that all markets in the map are supported.\n",
        "    unsupported_markets = set(ticker_to_market_map.values()) - set(market_to_exchange_code.keys())\n",
        "    if unsupported_markets:\n",
        "        raise ValueError(f\"Unsupported markets found in ticker_to_market_map: {unsupported_markets}\")\n",
        "\n",
        "    # --- Calendar Alignment ---\n",
        "\n",
        "    # Get the full date range of the dataset to pre-calculate all valid trading days.\n",
        "    min_date, max_date = df.index.get_level_values('date').min(), df.index.get_level_values('date').max()\n",
        "\n",
        "    # Pre-calculate valid trading days for each required calendar.\n",
        "    # Storing them in a set provides O(1) average time complexity for lookups.\n",
        "    valid_days_by_calendar = {\n",
        "        market: set(xcals.get_calendar(code).sessions_in_range(min_date, max_date))\n",
        "        for market, code in market_to_exchange_code.items()\n",
        "        if market in set(ticker_to_market_map.values())\n",
        "    }\n",
        "\n",
        "    # Create a boolean mask to identify rows that are on valid trading days.\n",
        "    # This is more efficient than filtering group by group.\n",
        "    is_valid_trading_day = pd.Series(False, index=df.index)\n",
        "\n",
        "    # Iterate through each ticker to apply its specific calendar rules.\n",
        "    for ticker, group in df.groupby(level='ticker', sort=False):\n",
        "        market = ticker_to_market_map.get(ticker)\n",
        "        if market is None:\n",
        "            raise ValueError(f\"Ticker '{ticker}' not found in ticker_to_market_map.\")\n",
        "\n",
        "        # Get the pre-calculated set of valid days for the ticker's market.\n",
        "        valid_days = valid_days_by_calendar[market]\n",
        "\n",
        "        # Get the dates for the current ticker group.\n",
        "        ticker_dates = group.index.get_level_values('date').normalize()\n",
        "\n",
        "        # Update the mask for the current ticker's indices.\n",
        "        # A date is valid if it exists in the pre-calculated set.\n",
        "        is_valid_trading_day.loc[group.index] = ticker_dates.isin(valid_days)\n",
        "\n",
        "    # --- Filter Data and Generate Report ---\n",
        "\n",
        "    # Generate a summary report of rows dropped due to calendar misalignment.\n",
        "    dropped_tickers = df.loc[~is_valid_trading_day].index.get_level_values('ticker')\n",
        "    summary_report = (\n",
        "        dropped_tickers.value_counts()\n",
        "        .to_frame('calendar_violations')\n",
        "        .sort_index()\n",
        "    )\n",
        "\n",
        "    # Apply the final mask to keep only the valid trading days.\n",
        "    cleaned_df = df[is_valid_trading_day].copy()\n",
        "\n",
        "    return cleaned_df, summary_report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_and_prepare_data(\n",
        "    raw_market_data_df: pd.DataFrame,\n",
        "    ticker_to_market_map: Dict[str, str]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end data cleansing and preparation pipeline.\n",
        "\n",
        "    This function executes the three core cleansing steps in sequence:\n",
        "    1. Removes rows with fundamentally invalid data (nulls, non-positives).\n",
        "    2. Enforces logical consistency of OHLC prices.\n",
        "    3. Aligns the data to official exchange trading calendars.\n",
        "\n",
        "    It returns a fully cleansed DataFrame ready for the next stage of processing\n",
        "    (windowing and feature engineering), along with a comprehensive report\n",
        "    detailing all modifications made to the data.\n",
        "\n",
        "    Args:\n",
        "        raw_market_data_df: The raw DataFrame of market data.\n",
        "        ticker_to_market_map: A dictionary mapping each ticker to its market\n",
        "                                identifier (e.g., 'US', 'CN', 'EU').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A new, fully cleansed pandas DataFrame.\n",
        "        - A dictionary of reports, where each key corresponds to a cleansing\n",
        "          step and the value is a DataFrame summarizing the changes made.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store reports from each cleansing step.\n",
        "    reports = {}\n",
        "\n",
        "    # --- Step 1: Remove rows with missing or invalid data ---\n",
        "    # This is the first and most critical cleansing step.\n",
        "    df_after_step1, report1 = _remove_invalid_data(raw_market_data_df)\n",
        "    reports['invalid_data_removal'] = report1\n",
        "    print(f\"Step 1: Removed {report1.sum().sum()} rows with invalid numerical data.\")\n",
        "\n",
        "    # --- Step 2: Enforce intra-day price consistency ---\n",
        "    # This step operates on the output of the previous step.\n",
        "    df_after_step2, report2 = _enforce_price_consistency(df_after_step1)\n",
        "    reports['price_consistency_enforcement'] = report2\n",
        "    print(f\"Step 2: Removed {report2.sum().sum()} rows with inconsistent OHLC prices.\")\n",
        "\n",
        "    # --- Step 3: Align trading calendars ---\n",
        "    # The final cleansing step ensures temporal integrity.\n",
        "    df_after_step3, report3 = _align_to_trading_calendars(df_after_step2, ticker_to_market_map)\n",
        "    reports['calendar_alignment'] = report3\n",
        "    print(f\"Step 3: Removed {report3.sum().sum()} rows corresponding to non-trading days.\")\n",
        "\n",
        "    # The final DataFrame is the result of all sequential cleansing operations.\n",
        "    final_cleaned_df = df_after_step3\n",
        "\n",
        "    # Calculate and print the total reduction in data size.\n",
        "    initial_rows = len(raw_market_data_df)\n",
        "    final_rows = len(final_cleaned_df)\n",
        "    reduction_pct = (1 - final_rows / initial_rows) * 100 if initial_rows > 0 else 0\n",
        "    print(f\"\\nData cleansing complete. Total rows reduced from {initial_rows} to {final_rows} ({reduction_pct:.2f}% reduction).\")\n",
        "\n",
        "    return final_cleaned_df, reports\n"
      ],
      "metadata": {
        "id": "CKEfYmAmiO8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Construct Sliding Windows and Supervised Targets\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Construct Sliding Windows and Supervised Targets\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Define historical windows X and future targets y\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_sliding_windows(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    input_window_size: int,\n",
        "    prediction_horizon: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Converts a time-series DataFrame into sliding windows and rich metadata.\n",
        "\n",
        "    This function implements the core data transformation from a 2D time-series\n",
        "    format to 3D windowed arrays (X) and 2D target arrays (y). This revised\n",
        "    version creates a comprehensive metadata DataFrame that includes start dates\n",
        "    and the full date series for each window, making downstream processes\n",
        "    robust and self-contained. It continues to use numpy's stride tricks for\n",
        "    maximum memory efficiency.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df: The fully cleansed market data DataFrame from Task 2.\n",
        "        input_window_size: The number of time steps in each input window (T).\n",
        "        prediction_horizon: The number of time steps to predict (T').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - X_windows: A 3D numpy array of shape (num_windows, T, num_features).\n",
        "        - y_targets: A 2D numpy array of shape (num_windows, T').\n",
        "        - metadata: A DataFrame with (ticker, window_start_date, window_end_date,\n",
        "          window_dates) for each window.\n",
        "    \"\"\"\n",
        "    # Define the exact feature set and order for the model input.\n",
        "    feature_cols = ['Open', 'High', 'Low', 'Volume', 'Close', 'Adj Close']\n",
        "    target_col = 'Adj Close'\n",
        "\n",
        "    # Lists to store the results from each ticker before final concatenation.\n",
        "    all_x, all_y, all_meta = [], [], []\n",
        "\n",
        "    # Group by ticker to process each time-series independently, preventing data leakage.\n",
        "    for ticker, group in cleansed_df.groupby(level='ticker', sort=False):\n",
        "        # Extract data as numpy arrays for performance.\n",
        "        features = group[feature_cols].values\n",
        "        targets = group[target_col].values\n",
        "        dates = group.index.get_level_values('date').values\n",
        "\n",
        "        # Define the total length required to form one complete (X, y) sample.\n",
        "        total_len = input_window_size + prediction_horizon\n",
        "\n",
        "        # Skip tickers with insufficient data.\n",
        "        if len(group) < total_len:\n",
        "            continue\n",
        "\n",
        "        # Determine the number of sliding windows that can be created.\n",
        "        num_windows = len(group) - total_len + 1\n",
        "\n",
        "        # --- Create X_windows using memory-efficient stride tricks ---\n",
        "        shape = (num_windows, input_window_size, features.shape[1])\n",
        "        strides = (features.strides[0], features.strides[0], features.strides[1])\n",
        "        x_ticker = np.lib.stride_tricks.as_strided(features, shape=shape, strides=strides)\n",
        "\n",
        "        # --- Create y_targets using memory-efficient stride tricks ---\n",
        "        target_shape = (num_windows, prediction_horizon)\n",
        "        target_strides = (targets.strides[0], targets.strides[0])\n",
        "        y_ticker = np.lib.stride_tricks.as_strided(targets[input_window_size:], shape=target_shape, strides=target_strides)\n",
        "\n",
        "        # --- Create Comprehensive Metadata ---\n",
        "        # Pre-calculate all metadata fields for the current ticker.\n",
        "        meta_list = []\n",
        "        for i in range(num_windows):\n",
        "            # The start index of the window in the original group's array.\n",
        "            start_idx = i\n",
        "            # The end index of the window.\n",
        "            end_idx = i + input_window_size\n",
        "            meta_list.append({\n",
        "                'ticker': ticker,\n",
        "                'window_start_date': dates[start_idx],\n",
        "                'window_end_date': dates[end_idx - 1],\n",
        "                # Store the actual series of dates for this window.\n",
        "                'window_dates': dates[start_idx:end_idx]\n",
        "            })\n",
        "        meta_ticker = pd.DataFrame(meta_list)\n",
        "\n",
        "        # Append the results for the current ticker to the master lists.\n",
        "        all_x.append(x_ticker)\n",
        "        all_y.append(y_ticker)\n",
        "        all_meta.append(meta_ticker)\n",
        "\n",
        "    # Handle the edge case where no valid windows could be created across all tickers.\n",
        "    if not all_x:\n",
        "        # Return empty, correctly-shaped artifacts.\n",
        "        empty_x = np.empty((0, input_window_size, len(feature_cols)), dtype=np.float64)\n",
        "        empty_y = np.empty((0, prediction_horizon), dtype=np.float64)\n",
        "        empty_meta = pd.DataFrame(columns=['ticker', 'window_start_date', 'window_end_date', 'window_dates'])\n",
        "        return empty_x, empty_y, empty_meta\n",
        "\n",
        "    # Concatenate results from all tickers into the final arrays and DataFrame.\n",
        "    X_windows = np.concatenate(all_x, axis=0)\n",
        "    y_targets = np.concatenate(all_y, axis=0)\n",
        "    metadata = pd.concat(all_meta, axis=0).reset_index(drop=True)\n",
        "\n",
        "    return X_windows, y_targets, metadata\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Split data chronologically into train/val/test\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _split_data_chronologically(\n",
        "    X_windows: np.ndarray,\n",
        "    y_targets: np.ndarray,\n",
        "    metadata: pd.DataFrame,\n",
        "    dataset_name: Literal[\"StockNet\", \"Indices_2024_2025\"],\n",
        ") -> Dict[str, Dict[str, Union[np.ndarray, pd.DataFrame]]]:\n",
        "    \"\"\"\n",
        "    Splits the windowed data into training, validation, and test sets chronologically.\n",
        "\n",
        "    This function ensures a strict temporal separation between sets, which is\n",
        "    paramount for valid time-series model evaluation. It supports different\n",
        "    splitting strategies based on the dataset name.\n",
        "\n",
        "    Args:\n",
        "        X_windows: The 3D numpy array of input windows.\n",
        "        y_targets: The 2D numpy array of target values.\n",
        "        metadata: The DataFrame containing metadata for each window.\n",
        "        dataset_name: The name of the dataset to determine the split strategy.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the data splits. For example:\n",
        "        {\n",
        "            'train': {'X': X_train, 'y': y_train, 'meta': meta_train},\n",
        "            'val':   {'X': X_val,   'y': y_val,   'meta': meta_val},\n",
        "            'test':  {'X': X_test,  'y': y_test,  'meta': meta_test}\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Ensure metadata dates are in datetime format for comparison.\n",
        "    metadata['window_end_date'] = pd.to_datetime(metadata['window_end_date'])\n",
        "\n",
        "    # Get the unique, sorted list of all window end dates in the dataset.\n",
        "    unique_dates = np.sort(metadata['window_end_date'].unique())\n",
        "\n",
        "    if dataset_name == \"StockNet\":\n",
        "        # Strategy for StockNet: 70% train, 15% validation, 15% test.\n",
        "        # Calculate the split indices based on quantiles of the unique dates.\n",
        "        n_dates = len(unique_dates)\n",
        "        train_end_idx = int(n_dates * 0.7)\n",
        "        val_end_idx = int(n_dates * 0.85)\n",
        "\n",
        "        # Determine the cutoff dates.\n",
        "        train_end_date = unique_dates[train_end_idx]\n",
        "        val_end_date = unique_dates[val_end_idx]\n",
        "\n",
        "        # Create boolean masks based on the cutoff dates.\n",
        "        train_mask = metadata['window_end_date'] <= train_end_date\n",
        "        val_mask = (metadata['window_end_date'] > train_end_date) & (metadata['window_end_date'] <= val_end_date)\n",
        "        test_mask = metadata['window_end_date'] > val_end_date\n",
        "\n",
        "    elif dataset_name == \"Indices_2024_2025\":\n",
        "        # Strategy for Indices: Test on 2024-01-01 to 2025-01-01.\n",
        "        # The rest is used for training. No validation set is defined by this rule.\n",
        "        test_start_date = pd.Timestamp('2024-01-01')\n",
        "        test_end_date = pd.Timestamp('2025-01-01')\n",
        "\n",
        "        # Create boolean masks.\n",
        "        test_mask = (metadata['window_end_date'] >= test_start_date) & (metadata['window_end_date'] < test_end_date)\n",
        "        train_mask = metadata['window_end_date'] < test_start_date\n",
        "        # Validation set is empty under this strategy.\n",
        "        val_mask = pd.Series(False, index=metadata.index)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset_name for splitting: {dataset_name}\")\n",
        "\n",
        "    # Apply the masks to create the data splits.\n",
        "    splits = {\n",
        "        'train': {\n",
        "            'X': X_windows[train_mask],\n",
        "            'y': y_targets[train_mask],\n",
        "            'meta': metadata[train_mask].reset_index(drop=True)\n",
        "        },\n",
        "        'val': {\n",
        "            'X': X_windows[val_mask],\n",
        "            'y': y_targets[val_mask],\n",
        "            'meta': metadata[val_mask].reset_index(drop=True)\n",
        "        },\n",
        "        'test': {\n",
        "            'X': X_windows[test_mask],\n",
        "            'y': y_targets[test_mask],\n",
        "            'meta': metadata[test_mask].reset_index(drop=True)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return splits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate and log window counts\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _log_window_counts(\n",
        "    data_splits: Dict[str, Dict[str, Union[np.ndarray, pd.DataFrame]]],\n",
        "    min_windows_per_ticker: int = 50\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Generates a summary report of window counts per ticker and per split.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The dictionary of data splits from the previous step.\n",
        "        min_windows_per_ticker: The minimum number of training windows a ticker\n",
        "                                must have to be considered for robust training.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A summary DataFrame with window counts.\n",
        "        - A list of tickers recommended for exclusion due to insufficient data.\n",
        "    \"\"\"\n",
        "    # List to store summary information from each split.\n",
        "    summary_list = []\n",
        "\n",
        "    # Iterate over each split (train, val, test) to generate counts.\n",
        "    for split_name, split_data in data_splits.items():\n",
        "        meta = split_data['meta']\n",
        "        if meta.empty:\n",
        "            continue\n",
        "\n",
        "        # Count windows per ticker within the current split.\n",
        "        counts = meta['ticker'].value_counts().reset_index()\n",
        "        counts.columns = ['ticker', 'window_count']\n",
        "        counts['split'] = split_name\n",
        "        summary_list.append(counts)\n",
        "\n",
        "    # Combine all summaries into a single DataFrame.\n",
        "    if not summary_list:\n",
        "        return pd.DataFrame(), []\n",
        "\n",
        "    summary_df = pd.concat(summary_list, ignore_index=True)\n",
        "    # Pivot for a more readable, wide-format report.\n",
        "    summary_pivot = summary_df.pivot_table(\n",
        "        index='ticker', columns='split', values='window_count', fill_value=0\n",
        "    )\n",
        "\n",
        "    # Identify tickers with insufficient training data.\n",
        "    if 'train' in summary_pivot.columns:\n",
        "        train_counts = summary_pivot['train']\n",
        "        tickers_to_exclude = train_counts[train_counts < min_windows_per_ticker].index.tolist()\n",
        "    else:\n",
        "        tickers_to_exclude = summary_pivot.index.tolist() # Exclude all if no train data\n",
        "\n",
        "    return summary_pivot, tickers_to_exclude\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_windows_and_split_data(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    dataset_name: Literal[\"StockNet\", \"Indices_2024_2025\"],\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of sliding windows and their chronological split.\n",
        "\n",
        "    This function executes the full workflow for Task 3:\n",
        "    1. Creates memory-efficient sliding windows (X, y) and metadata.\n",
        "    2. Splits the data into train, validation, and test sets chronologically.\n",
        "    3. Generates a detailed report of window counts for validation.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df: The fully cleansed market data from Task 2.\n",
        "        config: The master configuration dictionary.\n",
        "        dataset_name: The name of the dataset to guide the splitting strategy.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the complete results:\n",
        "        {\n",
        "            'data_splits': The train/val/test data splits.\n",
        "            'summary_report': A DataFrame with window counts.\n",
        "            'excluded_tickers': A list of tickers with insufficient data.\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Extract necessary parameters from the configuration.\n",
        "    try:\n",
        "        input_window_size = config['data_and_task_spec']['input_window_size']\n",
        "        prediction_horizon = config['data_and_task_spec']['prediction_horizon']\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Configuration is missing a required key for windowing: {e}\")\n",
        "\n",
        "    # --- Step 1: Create sliding windows ---\n",
        "    print(\"Step 1: Creating sliding windows from time-series data...\")\n",
        "    X_windows, y_targets, metadata = _create_sliding_windows(\n",
        "        cleansed_df, input_window_size, prediction_horizon\n",
        "    )\n",
        "    print(f\"Created {len(metadata)} total windows.\")\n",
        "    if len(metadata) == 0:\n",
        "        print(\"Warning: No valid windows could be created from the provided data.\")\n",
        "        return {\n",
        "            'data_splits': {},\n",
        "            'summary_report': pd.DataFrame(),\n",
        "            'excluded_tickers': []\n",
        "        }\n",
        "\n",
        "    # --- Step 2: Split data chronologically ---\n",
        "    print(f\"Step 2: Splitting data chronologically using '{dataset_name}' strategy...\")\n",
        "    data_splits = _split_data_chronologically(\n",
        "        X_windows, y_targets, metadata, dataset_name\n",
        "    )\n",
        "    print(f\"Train windows: {len(data_splits['train']['meta'])}\")\n",
        "    print(f\"Validation windows: {len(data_splits['val']['meta'])}\")\n",
        "    print(f\"Test windows: {len(data_splits['test']['meta'])}\")\n",
        "\n",
        "    # --- Step 3: Validate and log window counts ---\n",
        "    print(\"Step 3: Generating window count summary report...\")\n",
        "    summary_report, excluded_tickers = _log_window_counts(data_splits)\n",
        "    print(f\"Identified {len(excluded_tickers)} tickers with insufficient training data.\")\n",
        "\n",
        "    # Assemble the final output artifact.\n",
        "    result = {\n",
        "        'data_splits': data_splits,\n",
        "        'summary_report': summary_report,\n",
        "        'excluded_tickers': excluded_tickers\n",
        "    }\n",
        "\n",
        "    print(\"\\nTask 3: Window construction and data splitting complete.\")\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "bU8Es1ZdjIhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Compute Technical Annotations f(X) for Each Window\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Compute Technical Annotations f(X) for Each Window\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Compute summary statistics over the window\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_summary_statistics(\n",
        "    X_windows: np.ndarray\n",
        ") -> List[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Computes vectorized summary statistics (mean, min, max) for all windows.\n",
        "\n",
        "    Args:\n",
        "        X_windows: A 3D numpy array of shape (num_windows, T, 6), where the\n",
        "                   6 features are [Open, High, Low, Volume, Close, Adj Close].\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary contains the statistics\n",
        "        for one window.\n",
        "    \"\"\"\n",
        "    # Define the feature names in the exact order they appear in the array.\n",
        "    feature_names = ['Open', 'High', 'Low', 'Volume', 'Close', 'Adj_Close']\n",
        "\n",
        "    # Perform vectorized calculations across all windows simultaneously.\n",
        "    # axis=1 specifies aggregation over the time dimension (T).\n",
        "    means = np.mean(X_windows, axis=1)\n",
        "    mins = np.min(X_windows, axis=1)\n",
        "    maxs = np.max(X_windows, axis=1)\n",
        "\n",
        "    # Structure the results into a list of dictionaries for easy serialization.\n",
        "    num_windows = X_windows.shape[0]\n",
        "    stats_list = [{} for _ in range(num_windows)]\n",
        "\n",
        "    # Iterate through each feature to populate the dictionaries.\n",
        "    for i, name in enumerate(feature_names):\n",
        "        for j in range(num_windows):\n",
        "            stats_list[j][f\"Mean_{name}\"] = means[j, i]\n",
        "            stats_list[j][f\"Min_{name}\"] = mins[j, i]\n",
        "            stats_list[j][f\"Max_{name}\"] = maxs[j, i]\n",
        "\n",
        "    return stats_list\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Compute financial technical indicators using Appendix formulas\n",
        "# ------------------------------------------------------------------------------\n",
        "# ------------------------------------------------------------------------------\n",
        "# Numba JIT-compiled Helper Functions\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "@numba.njit(parallel=True, cache=True)\n",
        "def _numba_ema_vectorized(data: np.ndarray, window: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Exponential Moving Average (EMA) for a batch of windows.\n",
        "\n",
        "    This function is Just-In-Time (JIT) compiled with Numba and parallelized\n",
        "    to achieve C-level performance for the iterative EMA calculation across\n",
        "    a large number of independent windows.\n",
        "\n",
        "    Process:\n",
        "    1.  For each window (row) in the input `data` array:\n",
        "    2.  Initializes the EMA with the Simple Moving Average (SMA) of the first\n",
        "        `window` data points.\n",
        "    3.  Iteratively applies the EMA formula for the remaining points in the series.\n",
        "    4.  The outer loop over windows is parallelized by Numba's `prange`.\n",
        "\n",
        "    Equation:\n",
        "        EMA_t = Price_t * alpha + EMA_{t-1} * (1 - alpha)\n",
        "        where alpha = 2 / (window + 1)\n",
        "\n",
        "    Args:\n",
        "        data: A 2D numpy array of shape (num_windows, T_period) containing the\n",
        "              time-series data (e.g., closing prices) for each window.\n",
        "        window: The lookback period (integer) for the EMA calculation.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) where each element is the\n",
        "        final EMA value for the corresponding window.\n",
        "    \"\"\"\n",
        "    # Get the number of windows to process from the shape of the input array.\n",
        "    num_windows = data.shape[0]\n",
        "\n",
        "    # Pre-allocate a numpy array to store the final EMA value for each window.\n",
        "    ema_array = np.empty(num_windows, dtype=np.float64)\n",
        "\n",
        "    # Calculate the smoothing factor 'alpha' based on the lookback window.\n",
        "    alpha = 2.0 / (window + 1.0)\n",
        "\n",
        "    # Use Numba's prange for parallel execution across the independent windows.\n",
        "    for i in numba.prange(num_windows):\n",
        "        # Extract the time-series for the current window.\n",
        "        series = data[i]\n",
        "\n",
        "        # Initialize the EMA with the Simple Moving Average of the first 'window' points.\n",
        "        # This is a standard and robust initialization technique.\n",
        "        ema = np.mean(series[:window])\n",
        "\n",
        "        # Iteratively apply the EMA formula for the remaining points in the series.\n",
        "        for j in range(window, len(series)):\n",
        "            ema = alpha * series[j] + (1.0 - alpha) * ema\n",
        "\n",
        "        # Store the final calculated EMA value for the current window.\n",
        "        ema_array[i] = ema\n",
        "\n",
        "    # Return the array of final EMA values.\n",
        "    return ema_array\n",
        "\n",
        "\n",
        "@numba.njit(parallel=True, cache=True)\n",
        "def _numba_wilder_smooth_vectorized(data: np.ndarray, window: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates Wilder's Smoothing for a batch of windows.\n",
        "\n",
        "    Wilder's Smoothing is a specific type of Exponential Moving Average used in\n",
        "    indicators like RSI and ADX. This function is JIT-compiled with Numba and\n",
        "    parallelized for maximum performance.\n",
        "\n",
        "    Process:\n",
        "    1.  For each window (row) in the input `data` array:\n",
        "    2.  Initializes the smoothed value with the SMA of the first `window` points.\n",
        "    3.  Iteratively applies the Wilder's smoothing formula.\n",
        "    4.  The outer loop is parallelized by Numba's `prange`.\n",
        "\n",
        "    Equation:\n",
        "        SmoothedValue_t = (SmoothedValue_{t-1} * (n - 1) + NewValue_t) / n\n",
        "        where n is the window period.\n",
        "\n",
        "    Args:\n",
        "        data: A 2D numpy array of shape (num_windows, T_period) containing the\n",
        "              time-series data to be smoothed.\n",
        "        window: The lookback period (integer) for the smoothing calculation.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) where each element is the\n",
        "        final smoothed value for the corresponding window.\n",
        "    \"\"\"\n",
        "    # Get the number of windows to process from the shape of the input array.\n",
        "    num_windows = data.shape[0]\n",
        "\n",
        "    # Pre-allocate a numpy array to store the final smoothed value for each window.\n",
        "    smoothed_array = np.empty(num_windows, dtype=np.float64)\n",
        "\n",
        "    # Use Numba's prange for parallel execution across the independent windows.\n",
        "    for i in numba.prange(num_windows):\n",
        "        # Extract the time-series for the current window.\n",
        "        series = data[i]\n",
        "\n",
        "        # Initialize the smoothed value with the Simple Moving Average.\n",
        "        val = np.mean(series[:window])\n",
        "\n",
        "        # Iteratively apply the Wilder's smoothing formula for subsequent points.\n",
        "        for j in range(window, len(series)):\n",
        "            val = (val * (window - 1) + series[j]) / window\n",
        "\n",
        "        # Store the final calculated smoothed value for the current window.\n",
        "        smoothed_array[i] = val\n",
        "\n",
        "    # Return the array of final smoothed values.\n",
        "    return smoothed_array\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Individual Indicator Implementations\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_sma(price_data: np.ndarray, window: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Simple Moving Average (SMA) for a batch of windows.\n",
        "\n",
        "    This function performs a fully vectorized calculation of the SMA for the\n",
        "    last `window` periods of each time-series in the input array.\n",
        "\n",
        "    Equation:\n",
        "        SMA = (1/n) * sum(Price_i for i=1 to n)\n",
        "\n",
        "    Args:\n",
        "        price_data: A 2D numpy array of shape (num_windows, T) of prices.\n",
        "        window: The lookback period for the SMA.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated SMA values.\n",
        "        Returns NaN for windows with insufficient data.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure the lookback period is not larger than the time series length.\n",
        "    if price_data.shape[1] < window:\n",
        "        # If there's not enough data, return an array of NaNs.\n",
        "        return np.full(price_data.shape[0], np.nan, dtype=np.float64)\n",
        "\n",
        "    # Calculate the mean of the last 'window' elements along axis 1 (the time dimension).\n",
        "    return np.mean(price_data[:, -window:], axis=1)\n",
        "\n",
        "\n",
        "def _calculate_ema(price_data: np.ndarray, window: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Exponential Moving Average (EMA) via a Numba helper.\n",
        "\n",
        "    This function serves as a wrapper, handling input validation before passing\n",
        "    the data to the high-performance, JIT-compiled Numba function.\n",
        "\n",
        "    Equation:\n",
        "        EMA_t = Price_t * alpha + EMA_{t-1} * (1 - alpha)\n",
        "\n",
        "    Args:\n",
        "        price_data: A 2D numpy array of shape (num_windows, T) of prices.\n",
        "        window: The lookback period for the EMA.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated EMA values.\n",
        "        Returns NaN for windows with insufficient data.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure there is enough data for at least one EMA calculation.\n",
        "    if price_data.shape[1] < window:\n",
        "        # Return an array of NaNs if the condition is not met.\n",
        "        return np.full(price_data.shape[0], np.nan, dtype=np.float64)\n",
        "\n",
        "    # Delegate the computationally intensive calculation to the JIT-compiled helper.\n",
        "    return _numba_ema_vectorized(price_data, window)\n",
        "\n",
        "\n",
        "def _calculate_momentum(price_data: np.ndarray, lag: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Momentum for a batch of windows.\n",
        "\n",
        "    This is a fully vectorized implementation that measures the rate of change\n",
        "    over a specified lag period.\n",
        "\n",
        "    Equation:\n",
        "        Momentum = Price_t - Price_{t-n}\n",
        "\n",
        "    Args:\n",
        "        price_data: A 2D numpy array of shape (num_windows, T) of prices.\n",
        "        lag: The lookback period (n) for the momentum calculation.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated momentum values.\n",
        "        Returns NaN for windows with insufficient data.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure the time series is long enough for the lag.\n",
        "    if price_data.shape[1] < lag + 1:\n",
        "        # Return an array of NaNs if there isn't enough data.\n",
        "        return np.full(price_data.shape[0], np.nan, dtype=np.float64)\n",
        "\n",
        "    # Subtract the price 'lag' periods ago from the most recent price.\n",
        "    return price_data[:, -1] - price_data[:, -1 - lag]\n",
        "\n",
        "\n",
        "def _calculate_rsi(price_data: np.ndarray, window: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Relative Strength Index (RSI) for a batch of windows.\n",
        "\n",
        "    This implementation correctly uses Wilder's Smoothing for averaging gains\n",
        "    and losses, which is the standard for RSI calculation. The heavy lifting\n",
        "    is done by a JIT-compiled Numba helper for performance.\n",
        "\n",
        "    Equation:\n",
        "        RSI = 100 - (100 / (1 + RS))\n",
        "        where RS = WilderSmooth(Gains) / WilderSmooth(Losses)\n",
        "\n",
        "    Args:\n",
        "        price_data: A 2D numpy array of shape (num_windows, T) of prices.\n",
        "        window: The lookback period for the RSI.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated RSI values.\n",
        "        Returns NaN for windows with insufficient data.\n",
        "    \"\"\"\n",
        "    # Input validation: RSI requires at least 'window' + 1 prices to calculate one change.\n",
        "    if price_data.shape[1] < window + 1:\n",
        "        return np.full(price_data.shape[0], np.nan, dtype=np.float64)\n",
        "\n",
        "    # Step 1: Calculate price changes (deltas) between consecutive periods.\n",
        "    deltas = np.diff(price_data, axis=1)\n",
        "\n",
        "    # Step 2: Separate deltas into gains and losses.\n",
        "    gains = np.where(deltas > 0, deltas, 0)\n",
        "    losses = np.where(deltas < 0, -deltas, 0)\n",
        "\n",
        "    # Step 3: Use Wilder's smoothing to get the average gain and average loss.\n",
        "    avg_gain = _numba_wilder_smooth_vectorized(gains, window)\n",
        "    avg_loss = _numba_wilder_smooth_vectorized(losses, window)\n",
        "\n",
        "    # Step 4: Calculate the Relative Strength (RS).\n",
        "    # Initialize RS to infinity to handle the case where avg_loss is zero (strong uptrend).\n",
        "    rs = np.full_like(avg_gain, np.inf, dtype=np.float64)\n",
        "    # Create a mask for non-zero average losses to avoid division by zero.\n",
        "    mask = avg_loss != 0\n",
        "    # Calculate RS only where avg_loss is not zero.\n",
        "    rs[mask] = avg_gain[mask] / avg_loss[mask]\n",
        "\n",
        "    # Step 5: Calculate the final RSI value.\n",
        "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
        "    return rsi\n",
        "\n",
        "\n",
        "def _calculate_macd(\n",
        "    price_data: np.ndarray,\n",
        "    fast: int,\n",
        "    slow: int,\n",
        "    signal: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calculates the Moving Average Convergence Divergence (MACD) and its signal line.\n",
        "\n",
        "    This function computes the MACD line as the difference between a fast and\n",
        "    a slow EMA. It then computes the signal line as an EMA of the MACD line itself.\n",
        "    The calculation of the signal line requires computing the historical MACD\n",
        "    values within each window, making it computationally intensive but correct.\n",
        "\n",
        "    Equations:\n",
        "        MACD_Line = EMA(price, fast_period) - EMA(price, slow_period)\n",
        "        Signal_Line = EMA(MACD_Line, signal_period)\n",
        "\n",
        "    Args:\n",
        "        price_data: A 2D numpy array of shape (num_windows, T) of prices.\n",
        "        fast: The lookback period for the fast EMA.\n",
        "        slow: The lookback period for the slow EMA.\n",
        "        signal: The lookback period for the signal line's EMA.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two 1D numpy arrays: (macd_line, signal_line).\n",
        "        Returns NaNs for windows with insufficient data.\n",
        "    \"\"\"\n",
        "    # Input validation: The time series must be at least as long as the slow EMA period.\n",
        "    if price_data.shape[1] < slow:\n",
        "        nan_array = np.full(price_data.shape[0], np.nan, dtype=np.float64)\n",
        "        return nan_array, nan_array\n",
        "\n",
        "    # Calculate the final MACD line value for each window.\n",
        "    ema_fast_final = _numba_ema_vectorized(price_data, fast)\n",
        "    ema_slow_final = _numba_ema_vectorized(price_data, slow)\n",
        "    macd_line_final = ema_fast_final - ema_slow_final\n",
        "\n",
        "    # To calculate the signal line, we need the history of the MACD line.\n",
        "    num_windows, T = price_data.shape\n",
        "    # The MACD history starts after the 'slow' period warmup.\n",
        "    macd_history_len = T - slow + 1\n",
        "    macd_history = np.empty((num_windows, macd_history_len), dtype=np.float64)\n",
        "\n",
        "    # Iteratively compute the MACD value for each possible sub-window.\n",
        "    for i in range(macd_history_len):\n",
        "        # Define the slice of data needed for this historical MACD calculation.\n",
        "        end_idx = i + slow\n",
        "        window_slice = price_data[:, :end_idx]\n",
        "\n",
        "        # Calculate historical fast and slow EMAs.\n",
        "        ema_f = _numba_ema_vectorized(window_slice, fast)\n",
        "        ema_s = _numba_ema_vectorized(window_slice, slow)\n",
        "\n",
        "        # Store the historical MACD value.\n",
        "        macd_history[:, i] = ema_f - ema_s\n",
        "\n",
        "    # Calculate the signal line by taking an EMA of the MACD history.\n",
        "    signal_line = _numba_ema_vectorized(macd_history, signal)\n",
        "\n",
        "    return macd_line_final, signal_line\n",
        "\n",
        "\n",
        "def _calculate_williams_r(\n",
        "    high: np.ndarray,\n",
        "    low: np.ndarray,\n",
        "    close: np.ndarray,\n",
        "    lookback: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates Williams %R for a batch of windows.\n",
        "\n",
        "    This is a fully vectorized momentum indicator that measures overbought and\n",
        "    oversold levels.\n",
        "\n",
        "    Equation:\n",
        "        Williams %R = ((HighestHigh_n - Close_t) / (HighestHigh_n - LowestLow_n)) * -100\n",
        "\n",
        "    Args:\n",
        "        high: 2D numpy array of high prices (num_windows, T).\n",
        "        low: 2D numpy array of low prices (num_windows, T).\n",
        "        close: 2D numpy array of close prices (num_windows, T).\n",
        "        lookback: The lookback period (n).\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated Williams %R values.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure enough data for the lookback period.\n",
        "    if high.shape[1] < lookback:\n",
        "        return np.full(high.shape[0], np.nan, dtype=np.float64)\n",
        "\n",
        "    # Slice the data to the lookback period.\n",
        "    high_lb = high[:, -lookback:]\n",
        "    low_lb = low[:, -lookback:]\n",
        "\n",
        "    # Find the highest high and lowest low over the period for each window.\n",
        "    highest_high = np.max(high_lb, axis=1)\n",
        "    lowest_low = np.min(low_lb, axis=1)\n",
        "\n",
        "    # Get the most recent closing price for each window.\n",
        "    close_last = close[:, -1]\n",
        "\n",
        "    # Calculate the numerator and denominator of the formula.\n",
        "    numerator = highest_high - close_last\n",
        "    denominator = highest_high - lowest_low\n",
        "\n",
        "    # Handle the division-by-zero edge case (when highest_high == lowest_low).\n",
        "    # Initialize the result array with NaNs.\n",
        "    williams_r = np.full_like(denominator, np.nan, dtype=np.float64)\n",
        "    # Create a mask to identify valid denominators.\n",
        "    mask = denominator != 0\n",
        "    # Perform the calculation only on the valid elements.\n",
        "    williams_r[mask] = -100.0 * (numerator[mask] / denominator[mask])\n",
        "\n",
        "    return williams_r\n",
        "\n",
        "\n",
        "def _calculate_cci(\n",
        "    high: np.ndarray,\n",
        "    low: np.ndarray,\n",
        "    close: np.ndarray,\n",
        "    period: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Commodity Channel Index (CCI) for a batch of windows.\n",
        "\n",
        "    This is a fully vectorized implementation of the CCI, which measures the\n",
        "    current price level relative to an average price level over a given period.\n",
        "\n",
        "    Equation:\n",
        "        CCI = (TypicalPrice - SMA(TP)) / (0.015 * MeanDeviation)\n",
        "\n",
        "    Args:\n",
        "        high: 2D numpy array of high prices (num_windows, T).\n",
        "        low: 2D numpy array of low prices (num_windows, T).\n",
        "        close: 2D numpy array of close prices (num_windows, T).\n",
        "        period: The lookback period for the calculation.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated CCI values.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure enough data for the lookback period.\n",
        "    if high.shape[1] < period:\n",
        "        return np.full(high.shape[0], np.nan, dtype=np.float64)\n",
        "\n",
        "    # Slice the data to the relevant lookback period.\n",
        "    high_lb = high[:, -period:]\n",
        "    low_lb = low[:, -period:]\n",
        "    close_lb = close[:, -period:]\n",
        "\n",
        "    # Step 1: Calculate the Typical Price (TP) for each day in the period.\n",
        "    tp = (high_lb + low_lb + close_lb) / 3.0\n",
        "\n",
        "    # Step 2: Calculate the Simple Moving Average of the Typical Price.\n",
        "    sma_tp = np.mean(tp, axis=1)\n",
        "\n",
        "    # Step 3: Calculate the Mean Absolute Deviation of TP from its SMA.\n",
        "    mean_dev = np.mean(np.abs(tp - sma_tp[:, np.newaxis]), axis=1)\n",
        "\n",
        "    # Step 4: Calculate the final CCI value.\n",
        "    numerator = tp[:, -1] - sma_tp\n",
        "    denominator = 0.015 * mean_dev\n",
        "\n",
        "    # Handle the division-by-zero edge case (when mean_dev is zero).\n",
        "    cci = np.full_like(denominator, np.nan, dtype=np.float64)\n",
        "    mask = denominator != 0\n",
        "    cci[mask] = numerator[mask] / denominator[mask]\n",
        "\n",
        "    return cci\n",
        "\n",
        "\n",
        "def _calculate_bollinger_bands(\n",
        "    price_data: np.ndarray,\n",
        "    window: int,\n",
        "    k: float\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calculates the Bollinger Bands (Upper and Lower) for a batch of windows.\n",
        "\n",
        "    This is a fully vectorized implementation.\n",
        "\n",
        "    Equations:\n",
        "        UpperBand = SMA(price, n) + k * StdDev(price, n)\n",
        "        LowerBand = SMA(price, n) - k * StdDev(price, n)\n",
        "\n",
        "    Args:\n",
        "        price_data: A 2D numpy array of shape (num_windows, T) of prices.\n",
        "        window: The lookback period (n) for the moving average and standard deviation.\n",
        "        k: The number of standard deviations to shift the bands.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two 1D numpy arrays: (upper_band, lower_band).\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure enough data for the lookback period.\n",
        "    if price_data.shape[1] < window:\n",
        "        nan_array = np.full(price_data.shape[0], np.nan, dtype=np.float64)\n",
        "        return nan_array, nan_array\n",
        "\n",
        "    # Slice the data to the relevant lookback period.\n",
        "    price_lb = price_data[:, -window:]\n",
        "\n",
        "    # Calculate the Simple Moving Average (the middle band).\n",
        "    middle_band = np.mean(price_lb, axis=1)\n",
        "\n",
        "    # Calculate the standard deviation over the period.\n",
        "    std_dev = np.std(price_lb, axis=1)\n",
        "\n",
        "    # Calculate the upper and lower bands.\n",
        "    upper_band = middle_band + k * std_dev\n",
        "    lower_band = middle_band - k * std_dev\n",
        "\n",
        "    return upper_band, lower_band\n",
        "\n",
        "\n",
        "def _calculate_stochastic_oscillator(\n",
        "    high: np.ndarray,\n",
        "    low: np.ndarray,\n",
        "    close: np.ndarray,\n",
        "    k_lookback: int,\n",
        "    d_smoothing: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calculates the Stochastic Oscillator (%K and %D) for a batch of windows.\n",
        "\n",
        "    This implementation correctly calculates the final %K value and then computes\n",
        "    the historical %K values needed to calculate the %D line (SMA of %K).\n",
        "\n",
        "    Equations:\n",
        "        %K = 100 * ((Close_t - LowestLow_n) / (HighestHigh_n - LowestLow_n))\n",
        "        %D = SMA(%K, d_period)\n",
        "\n",
        "    Args:\n",
        "        high: 2D numpy array of high prices (num_windows, T).\n",
        "        low: 2D numpy array of low prices (num_windows, T).\n",
        "        close: 2D numpy array of close prices (num_windows, T).\n",
        "        k_lookback: The lookback period (n) for %K.\n",
        "        d_smoothing: The smoothing period for %D.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two 1D numpy arrays: (percent_k, percent_d).\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure enough data for the initial %K calculation.\n",
        "    if high.shape[1] < k_lookback:\n",
        "        nan_array = np.full(high.shape[0], np.nan, dtype=np.float64)\n",
        "        return nan_array, nan_array\n",
        "\n",
        "    # --- Calculate the final %K value for each window ---\n",
        "    highest_high = np.max(high[:, -k_lookback:], axis=1)\n",
        "    lowest_low = np.min(low[:, -k_lookback:], axis=1)\n",
        "    close_last = close[:, -1]\n",
        "\n",
        "    numerator = close_last - lowest_low\n",
        "    denominator = highest_high - lowest_low\n",
        "\n",
        "    percent_k_final = np.full_like(denominator, np.nan, dtype=np.float64)\n",
        "    mask = denominator != 0\n",
        "    percent_k_final[mask] = 100.0 * (numerator[mask] / denominator[mask])\n",
        "\n",
        "    # --- Calculate %D by smoothing the history of %K ---\n",
        "    num_windows, T = high.shape\n",
        "    # The number of historical %K values we can compute within the window T.\n",
        "    k_history_len = T - k_lookback + 1\n",
        "\n",
        "    # Input validation for %D smoothing.\n",
        "    if k_history_len < d_smoothing:\n",
        "        percent_d_final = np.full(num_windows, np.nan, dtype=np.float64)\n",
        "        return percent_k_final, percent_d_final\n",
        "\n",
        "    k_history = np.empty((num_windows, k_history_len), dtype=np.float64)\n",
        "\n",
        "    # Loop to compute the historical %K values needed for the %D SMA.\n",
        "    for i in range(k_history_len):\n",
        "        # Define the slice for this historical %K calculation.\n",
        "        start_idx = i\n",
        "        end_idx = i + k_lookback\n",
        "\n",
        "        h_slice = high[:, start_idx:end_idx]\n",
        "        l_slice = low[:, start_idx:end_idx]\n",
        "        c_slice = close[:, end_idx - 1] # The close of the k_lookback period\n",
        "\n",
        "        # Vectorized calculation for the historical %K value.\n",
        "        hh = np.max(h_slice, axis=1)\n",
        "        ll = np.min(l_slice, axis=1)\n",
        "        num = c_slice - ll\n",
        "        den = hh - ll\n",
        "\n",
        "        k_vals = np.full_like(den, np.nan, dtype=np.float64)\n",
        "        m = den != 0\n",
        "        k_vals[m] = 100.0 * (num[m] / den[m])\n",
        "        k_history[:, i] = k_vals\n",
        "\n",
        "    # %D is the SMA of the last 'd_smoothing' values of %K.\n",
        "    percent_d_final = np.mean(k_history[:, -d_smoothing:], axis=1)\n",
        "\n",
        "    return percent_k_final, percent_d_final\n",
        "\n",
        "\n",
        "@numba.njit(cache=True)\n",
        "def _numba_adx_single_window(\n",
        "    high: np.ndarray,\n",
        "    low: np.ndarray,\n",
        "    close: np.ndarray,\n",
        "    period: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the true ADX for a single time-series window using Numba.\n",
        "\n",
        "    This function implements the complete, multi-step ADX calculation with\n",
        "    high fidelity, including the final smoothing of the DX history. It is\n",
        "    JIT-compiled for performance.\n",
        "\n",
        "    Args:\n",
        "        high: 1D numpy array of high prices for one window.\n",
        "        low: 1D numpy array of low prices for one window.\n",
        "        close: 1D numpy array of close prices for one window.\n",
        "        period: The lookback period for the ADX calculation.\n",
        "\n",
        "    Returns:\n",
        "        The final scalar ADX value for the window, or np.nan if not computable.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # A true ADX requires an initial smoothing period for DI components, and then\n",
        "    # another smoothing period for the resulting DX. A safe minimum length is required.\n",
        "    if len(high) < 2 * period:\n",
        "        return np.nan\n",
        "\n",
        "    # --- Step 1: Calculate TR, +DM, -DM ---\n",
        "    # Pre-allocate arrays for the history of these components.\n",
        "    tr = np.zeros_like(high)\n",
        "    plus_dm = np.zeros_like(high)\n",
        "    minus_dm = np.zeros_like(high)\n",
        "\n",
        "    # Loop from the second day onwards to calculate differences.\n",
        "    for i in range(1, len(high)):\n",
        "        # Calculate the three components of True Range.\n",
        "        tr1 = high[i] - low[i]\n",
        "        tr2 = np.abs(high[i] - close[i-1])\n",
        "        tr3 = np.abs(low[i] - close[i-1])\n",
        "        # True Range is the maximum of the three.\n",
        "        tr[i] = np.maximum(tr1, np.maximum(tr2, tr3))\n",
        "\n",
        "        # Calculate directional movements.\n",
        "        up_move = high[i] - high[i-1]\n",
        "        down_move = low[i-1] - low[i]\n",
        "\n",
        "        # Store +DM if it's positive and the dominant move.\n",
        "        if up_move > down_move and up_move > 0:\n",
        "            plus_dm[i] = up_move\n",
        "        # Store -DM if it's positive and the dominant move.\n",
        "        if down_move > up_move and down_move > 0:\n",
        "            minus_dm[i] = down_move\n",
        "\n",
        "    # --- Step 2: Smooth TR, +DM, -DM using Wilder's Smoothing ---\n",
        "    # Initialize smoothed values with a simple sum over the first period.\n",
        "    # Note: Standard ADX uses a simple sum for the first value, not an SMA.\n",
        "    smoothed_tr = np.zeros_like(tr)\n",
        "    smoothed_plus_dm = np.zeros_like(plus_dm)\n",
        "    smoothed_minus_dm = np.zeros_like(minus_dm)\n",
        "\n",
        "    smoothed_tr[period-1] = np.sum(tr[0:period])\n",
        "    smoothed_plus_dm[period-1] = np.sum(plus_dm[0:period])\n",
        "    smoothed_minus_dm[period-1] = np.sum(minus_dm[0:period])\n",
        "\n",
        "    # Apply iterative Wilder's smoothing for the rest of the series.\n",
        "    for i in range(period, len(high)):\n",
        "        smoothed_tr[i] = smoothed_tr[i-1] - (smoothed_tr[i-1] / period) + tr[i]\n",
        "        smoothed_plus_dm[i] = smoothed_plus_dm[i-1] - (smoothed_plus_dm[i-1] / period) + plus_dm[i]\n",
        "        smoothed_minus_dm[i] = smoothed_minus_dm[i-1] - (smoothed_minus_dm[i-1] / period) + minus_dm[i]\n",
        "\n",
        "    # --- Step 3: Calculate +DI, -DI ---\n",
        "    plus_di = np.zeros_like(high)\n",
        "    minus_di = np.zeros_like(high)\n",
        "\n",
        "    # Calculate DI values only where smoothed TR is non-zero.\n",
        "    for i in range(period - 1, len(high)):\n",
        "        if smoothed_tr[i] > 0:\n",
        "            plus_di[i] = 100.0 * (smoothed_plus_dm[i] / smoothed_tr[i])\n",
        "            minus_di[i] = 100.0 * (smoothed_minus_dm[i] / smoothed_tr[i])\n",
        "\n",
        "    # --- Step 4: Calculate DX History ---\n",
        "    dx = np.zeros_like(high)\n",
        "    for i in range(period - 1, len(high)):\n",
        "        di_sum = plus_di[i] + minus_di[i]\n",
        "        if di_sum > 0:\n",
        "            # Equation: DX = 100 * (|(+DI) - (-DI)| / |(+DI) + (-DI)|)\n",
        "            dx[i] = 100.0 * (np.abs(plus_di[i] - minus_di[i]) / di_sum)\n",
        "\n",
        "    # The history of DX values starts after the first DI calculation.\n",
        "    # The first DX is calculated at index `period - 1`, but smoothing needs a full `period` of DX values.\n",
        "    # The first valid DX for smoothing is at index `(period - 1) + (period - 1)`.\n",
        "    first_dx_idx = (period - 1) + (period - 1)\n",
        "    dx_history = dx[first_dx_idx:]\n",
        "\n",
        "    if len(dx_history) < period:\n",
        "        return np.nan\n",
        "\n",
        "    # --- Step 5: Calculate ADX by smoothing DX ---\n",
        "    # Initialize ADX with the SMA of the first 'period' DX values.\n",
        "    adx = np.mean(dx_history[:period])\n",
        "\n",
        "    # Apply Wilder's smoothing to the rest of the DX history.\n",
        "    for i in range(period, len(dx_history)):\n",
        "        adx = (adx * (period - 1) + dx_history[i]) / period\n",
        "\n",
        "    return adx\n",
        "\n",
        "\n",
        "def _calculate_adx(\n",
        "    high: np.ndarray,\n",
        "    low: np.ndarray,\n",
        "    close: np.ndarray,\n",
        "    period: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the Average Directional Index (ADX) for a batch of windows.\n",
        "\n",
        "    This function serves as a vectorized wrapper around a high-performance,\n",
        "    Numba-jitted helper that calculates the true ADX for a single window.\n",
        "    The previous implementation incorrectly returned DX instead of ADX. This\n",
        "    version rectifies that critical error by performing the full, correct,\n",
        "    multi-step calculation, including the final smoothing of the DX history.\n",
        "\n",
        "    Args:\n",
        "        high: 2D numpy array of high prices (num_windows, T).\n",
        "        low: 2D numpy array of low prices (num_windows, T).\n",
        "        close: 2D numpy array of close prices (num_windows, T).\n",
        "        period: The lookback period for the ADX calculation.\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of shape (num_windows,) with the calculated ADX values.\n",
        "        Returns NaN for windows with insufficient data.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input arrays have the expected 2D shape.\n",
        "    if high.ndim != 2 or low.ndim != 2 or close.ndim != 2:\n",
        "        raise ValueError(\"Input arrays (high, low, close) must be 2-dimensional.\")\n",
        "\n",
        "    # Get the number of windows to process.\n",
        "    num_windows = high.shape[0]\n",
        "\n",
        "    # Pre-allocate the output array for the results.\n",
        "    adx_values = np.empty(num_windows, dtype=np.float64)\n",
        "\n",
        "    # --- Iterative Calculation ---\n",
        "    # Loop through each window and apply the JIT-compiled helper function.\n",
        "    # While this is a Python loop, the heavy computation inside is compiled to\n",
        "    # machine code by Numba, making it highly performant.\n",
        "    for i in range(num_windows):\n",
        "        adx_values[i] = _numba_adx_single_window(\n",
        "            high[i], low[i], close[i], period\n",
        "        )\n",
        "\n",
        "    return adx_values\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Define and serialize the annotation mapping\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _serialize_annotations(\n",
        "    stats_list: List[Dict[str, float]],\n",
        "    indicators: Dict[str, np.ndarray]\n",
        ") -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Serializes calculated statistics and indicators into formatted text blocks.\n",
        "\n",
        "    Args:\n",
        "        stats_list: The list of summary statistics dictionaries.\n",
        "        indicators: The dictionary of technical indicator arrays.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A list of formatted statistics strings, one for each window.\n",
        "        - A list of formatted indicator strings, one for each window.\n",
        "    \"\"\"\n",
        "    num_windows = len(stats_list)\n",
        "    stats_blocks = []\n",
        "    inds_blocks = []\n",
        "\n",
        "    # Iterate through each window to create its formatted text blocks.\n",
        "    for i in range(num_windows):\n",
        "        # Format statistics block.\n",
        "        stats_str = \", \".join([f\"{key}: {value:.4f}\" for key, value in stats_list[i].items()])\n",
        "        stats_blocks.append(stats_str)\n",
        "\n",
        "        # Format indicators block.\n",
        "        inds_parts = []\n",
        "        # Sort indicator names for consistent output order.\n",
        "        for name in sorted(indicators.keys()):\n",
        "            values = indicators[name]\n",
        "            value = values[i]\n",
        "            # Handle NaN values from calculations gracefully.\n",
        "            formatted_value = f\"{value:.4f}\" if not np.isnan(value) else \"N/A\"\n",
        "            inds_parts.append(f\"{name}: {formatted_value}\")\n",
        "        inds_blocks.append(\", \".join(inds_parts))\n",
        "\n",
        "    return stats_blocks, inds_blocks\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_technical_annotations(\n",
        "    X_windows: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation and serialization of all technical annotations.\n",
        "\n",
        "    This function implements the mapping X' = f(X) by:\n",
        "    1. Computing summary statistics for each window.\n",
        "    2. Computing a complete suite of financial technical indicators.\n",
        "    3. Serializing these results into human-readable text blocks for an LLM.\n",
        "\n",
        "    Args:\n",
        "        X_windows: The 3D numpy array of input windows from Task 3.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the annotation results:\n",
        "        {\n",
        "            'statistics_blocks': A list of formatted statistics strings.\n",
        "            'indicators_blocks': A list of formatted indicator strings.\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Validate that the required configuration section is present.\n",
        "    if 'annotation_indicator_hyperparams' not in config.get('data_and_task_spec', {}):\n",
        "        raise ValueError(\"Config missing 'data_and_task_spec.annotation_indicator_hyperparams'\")\n",
        "\n",
        "    annotation_config = config['data_and_task_spec']['annotation_indicator_hyperparams']\n",
        "\n",
        "    # --- Step 1: Compute summary statistics ---\n",
        "    print(\"Step 1: Computing summary statistics for all windows...\")\n",
        "    stats_list = _compute_summary_statistics(X_windows)\n",
        "    print(f\"Computed statistics for {len(stats_list)} windows.\")\n",
        "\n",
        "    # --- Step 2: Compute financial technical indicators (Complete Suite) ---\n",
        "    print(\"Step 2: Computing complete suite of financial technical indicators...\")\n",
        "    indicators = _compute_technical_indicators(X_windows, annotation_config)\n",
        "    print(f\"Computed {len(indicators)} types of indicators.\")\n",
        "\n",
        "    # --- Step 3: Serialize annotations into text blocks ---\n",
        "    print(\"Step 3: Serializing annotations into text blocks...\")\n",
        "    statistics_blocks, indicators_blocks = _serialize_annotations(stats_list, indicators)\n",
        "    print(\"Serialization complete.\")\n",
        "\n",
        "    # Assemble the final output artifact.\n",
        "    result = {\n",
        "        'statistics_blocks': statistics_blocks,\n",
        "        'indicators_blocks': indicators_blocks\n",
        "    }\n",
        "\n",
        "    print(\"\\nTask 4: Technical annotation computation complete.\")\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "wrP7_sSAkN37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Assemble Reasoning Prompts q with Strict Formatting\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Assemble Reasoning Prompts q with Strict Formatting\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Populate the prompt template with window-specific data\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _format_ohlcv_table(\n",
        "    window_data: np.ndarray,\n",
        "    window_dates: np.ndarray\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats a single window of numerical data into a human-readable table string.\n",
        "\n",
        "    This revised version is a pure formatting function that receives all\n",
        "    necessary data as arguments, removing any internal logic for date generation.\n",
        "\n",
        "    Args:\n",
        "        window_data: A 2D numpy array of shape (T, 6) for a single window.\n",
        "        window_dates: A 1D numpy array of shape (T,) containing the actual\n",
        "                      datetime objects for the window.\n",
        "\n",
        "    Returns:\n",
        "        A formatted multi-line string representing the OHLCV table.\n",
        "    \"\"\"\n",
        "    # Input validation.\n",
        "    if not isinstance(window_data, np.ndarray) or window_data.ndim != 2:\n",
        "        raise TypeError(\"`window_data` must be a 2D numpy array.\")\n",
        "    if not isinstance(window_dates, np.ndarray) or window_dates.ndim != 1:\n",
        "        raise TypeError(\"`window_dates` must be a 1D numpy array.\")\n",
        "    if window_data.shape[0] != window_dates.shape[0]:\n",
        "        raise ValueError(\"Shape mismatch between `window_data` and `window_dates`.\")\n",
        "\n",
        "    # Create a pandas DataFrame for easy, robust string formatting.\n",
        "    df = pd.DataFrame(\n",
        "        window_data,\n",
        "        columns=['Open', 'High', 'Low', 'Volume', 'Close', 'Adj Close'],\n",
        "        index=pd.to_datetime(window_dates) # Ensure index is datetime\n",
        "    )\n",
        "\n",
        "    # Apply consistent formatting to each column for professional presentation.\n",
        "    df['Open'] = df['Open'].map('{:.4f}'.format)\n",
        "    df['High'] = df['High'].map('{:.4f}'.format)\n",
        "    df['Low'] = df['Low'].map('{:.4f}'.format)\n",
        "    df['Close'] = df['Close'].map('{:.4f}'.format)\n",
        "    df['Adj Close'] = df['Adj Close'].map('{:.4f}'.format)\n",
        "\n",
        "    # Format volume with commas for readability.\n",
        "    df['Volume'] = df['Volume'].map('{:,.0f}'.format)\n",
        "\n",
        "    # Convert the fully formatted DataFrame to a string, including the date index.\n",
        "    return df.to_string()\n",
        "\n",
        "\n",
        "def _populate_prompts(\n",
        "    X_windows: np.ndarray,\n",
        "    metadata: pd.DataFrame,\n",
        "    annotations: Dict[str, List[str]],\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Assembles the final prompt strings by populating a template.\n",
        "\n",
        "    This version is a pure function that relies solely on its\n",
        "    inputs. It uses the rich metadata generated by the revised windowing function,\n",
        "    eliminating fragile external dependencies and brittle assumptions about\n",
        "    date continuity.\n",
        "\n",
        "    Args:\n",
        "        X_windows: The 3D numpy array of input windows.\n",
        "        metadata: The comprehensive DataFrame with metadata for each window,\n",
        "                  including start/end dates and the date series.\n",
        "        annotations: The dictionary of formatted statistics and indicator blocks.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of fully-formed prompt strings, one for each window.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required columns are missing from the metadata.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_meta_cols = {'ticker', 'window_start_date', 'window_end_date', 'window_dates'}\n",
        "    if not required_meta_cols.issubset(metadata.columns):\n",
        "        raise ValueError(f\"Metadata is missing required columns. Found: {metadata.columns}. Required: {required_meta_cols}\")\n",
        "\n",
        "    # Extract the prompt template string from the configuration.\n",
        "    prompt_template = config['reasoning_model_config']['prompt_template']['template_string']\n",
        "\n",
        "    # Extract parameters for easy access.\n",
        "    input_window_size = config['data_and_task_spec']['input_window_size']\n",
        "    prediction_horizon = config['data_and_task_spec']['prediction_horizon']\n",
        "\n",
        "    # Initialize a list to hold the final prompts.\n",
        "    prompts = []\n",
        "\n",
        "    # Iterate through each window to construct its specific prompt.\n",
        "    for i in range(len(metadata)):\n",
        "        # Get the metadata for the current window directly from the metadata DataFrame.\n",
        "        meta_row = metadata.iloc[i]\n",
        "\n",
        "        # Format the OHLCV table using the dedicated helper function.\n",
        "        # We pass the raw numerical data and the actual date series from metadata.\n",
        "        ohlcv_table = _format_ohlcv_table(X_windows[i], meta_row['window_dates'])\n",
        "\n",
        "        # Assemble all placeholder values required by the template.\n",
        "        prompt_data = {\n",
        "            \"ticker\": meta_row['ticker'],\n",
        "            \"input_window_size\": input_window_size,\n",
        "            \"prediction_horizon\": prediction_horizon,\n",
        "            \"window_start\": pd.to_datetime(meta_row['window_start_date']).strftime('%Y-%m-%d'),\n",
        "            \"window_end\": pd.to_datetime(meta_row['window_end_date']).strftime('%Y-%m-%d'),\n",
        "            \"ohlcv_table\": ohlcv_table,\n",
        "            \"statistics_block\": annotations['statistics_blocks'][i],\n",
        "            \"indicators_block\": annotations['indicators_blocks'][i],\n",
        "            # These are placeholders within the template string itself, not for formatting here.\n",
        "            \"reasoning\": \"{reasoning}\",\n",
        "            \"p1\": \"{p1}\", \"p2\": \"{p2}\", \"pN\": \"{pN}\"\n",
        "        }\n",
        "\n",
        "        # Populate the template with the data for the current window.\n",
        "        # The `.format()` method will ignore the extra keys not present in the template.\n",
        "        prompts.append(prompt_template.format(**prompt_data))\n",
        "\n",
        "    return prompts\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Define strict output format and parsing rules\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class ParsedLLMOutput(NamedTuple):\n",
        "    \"\"\"\n",
        "    A structured data container for the results of parsing raw LLM output.\n",
        "\n",
        "    This NamedTuple provides a standardized, immutable object for returning\n",
        "    the outcome of the parsing process. It clearly separates the validity status,\n",
        "    the extracted components, and any error information, facilitating clean\n",
        "    downstream logic in reward functions and evaluation scripts.\n",
        "\n",
        "    Attributes:\n",
        "        is_valid: A boolean flag indicating if the raw output string adhered to\n",
        "                  all formatting rules and was successfully parsed.\n",
        "        reasoning: The extracted textual reasoning trace from between the\n",
        "                   `<think>...</think>` tags. `None` if parsing failed at or\n",
        "                   before this step.\n",
        "        prediction: A 1D numpy array of shape (T',) containing the parsed\n",
        "                    numerical forecast. `None` if parsing failed.\n",
        "        error_message: A string describing the specific reason for a parsing\n",
        "                       failure. `None` if `is_valid` is `True`.\n",
        "    \"\"\"\n",
        "    is_valid: bool\n",
        "    reasoning: Optional[str]\n",
        "    prediction: Optional[np.ndarray]\n",
        "    error_message: Optional[str]\n",
        "\n",
        "\n",
        "class LLMOutputParser:\n",
        "    \"\"\"\n",
        "    A robust parser for the structured output of the reasoning LLM.\n",
        "\n",
        "    This class encapsulates the strict formatting and parsing rules required\n",
        "    to reliably extract the reasoning trace and numerical prediction from the\n",
        "    LLM's raw text output. It is a critical utility designed for repeated use\n",
        "    within the reward function of the Reinforcement Learning training loop,\n",
        "    where performance and robustness to malformed inputs are paramount.\n",
        "\n",
        "    The parser is configured once upon initialization and provides a single\n",
        "    `parse` method that implements a multi-stage validation and extraction\n",
        "    process.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the parser with rules from the master configuration.\n",
        "\n",
        "        This constructor sets up the parser by extracting necessary parameters\n",
        "        and pre-compiling the regular expression for prediction line matching,\n",
        "        which is a key performance optimization for repeated use.\n",
        "\n",
        "        Args:\n",
        "            config: The master configuration dictionary for the VTA framework.\n",
        "                    It must contain the necessary keys under\n",
        "                    `reasoning_model_config` and `data_and_task_spec`.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If a required configuration key is missing.\n",
        "            ValueError: If the regex pattern in the configuration is invalid.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # --- Input Validation and Configuration Extraction ---\n",
        "            # Extract the specific sub-dictionary for prompt template rules.\n",
        "            prompt_config = config['reasoning_model_config']['prompt_template']\n",
        "\n",
        "            # Extract the enforcement rules dictionary.\n",
        "            rules = prompt_config['format_enforcement']\n",
        "\n",
        "            # Store the prediction horizon (T') for validating the output length.\n",
        "            self.prediction_horizon: int = config['data_and_task_spec']['prediction_horizon']\n",
        "\n",
        "            # Extract the regex pattern for finding the prediction line.\n",
        "            pattern_str = rules['prediction_pattern']\n",
        "\n",
        "        except KeyError as e:\n",
        "            # Raise a specific error if the configuration is incomplete.\n",
        "            raise KeyError(f\"LLMOutputParser initialization failed. Missing required key in config: {e}\")\n",
        "\n",
        "        # --- Pre-compile Regex for Performance ---\n",
        "        try:\n",
        "            # Compiling the regex pattern once during initialization is significantly\n",
        "            # more efficient than compiling it on every call to `parse`.\n",
        "            # The MULTILINE flag allows `^` to match the start of each line.\n",
        "            self.prediction_pattern: re.Pattern = re.compile(pattern_str, re.MULTILINE)\n",
        "\n",
        "        except re.error as e:\n",
        "            # Raise an error if the provided regex is syntactically invalid.\n",
        "            raise ValueError(f\"Invalid regex in config 'prediction_pattern': {e}\")\n",
        "\n",
        "    def parse(self, raw_output: str) -> ParsedLLMOutput:\n",
        "        \"\"\"\n",
        "        Parses a raw text string from the LLM according to strict format rules.\n",
        "\n",
        "        This method executes a sequence of validation and extraction steps. It is\n",
        "        designed to fail gracefully, returning a structured result with a clear\n",
        "        error message upon the first violation encountered.\n",
        "\n",
        "        Args:\n",
        "            raw_output: The raw string generated by the LLM.\n",
        "\n",
        "        Returns:\n",
        "            A ParsedLLMOutput named tuple containing the structured parsing result.\n",
        "        \"\"\"\n",
        "        # --- Validation Step 1: Extract reasoning from <think>...</think> tags ---\n",
        "        try:\n",
        "            # Use a non-greedy search (.*?) with the DOTALL flag to match across newlines.\n",
        "            # .group(1) extracts the content *inside* the tags.\n",
        "            reasoning = re.search(r'<think>(.*?)</think>', raw_output, re.DOTALL).group(1).strip()\n",
        "\n",
        "        except AttributeError:\n",
        "            # This error occurs if `re.search` returns None (i.e., no match found).\n",
        "            return ParsedLLMOutput(\n",
        "                is_valid=False,\n",
        "                reasoning=None,\n",
        "                prediction=None,\n",
        "                error_message=\"Format Error: Could not find valid <think>...</think> tags.\"\n",
        "            )\n",
        "\n",
        "        # --- Validation Step 2: Find the prediction line using the pre-compiled regex ---\n",
        "        # Search the entire string for a line matching the required prediction format.\n",
        "        match = self.prediction_pattern.search(raw_output)\n",
        "\n",
        "        # If no match is found, the format is invalid.\n",
        "        if not match:\n",
        "            return ParsedLLMOutput(\n",
        "                is_valid=False,\n",
        "                reasoning=reasoning,\n",
        "                prediction=None,\n",
        "                error_message=\"Format Error: Prediction line not found or malformed.\"\n",
        "            )\n",
        "\n",
        "        # Extract the full matched prediction line (e.g., \"PREDICTION = [1.0, 2.0]\").\n",
        "        prediction_line = match.group(0)\n",
        "\n",
        "        # --- Validation Step 3: Check for extraneous text after the prediction line ---\n",
        "        # Extract any text that appears after the end of the matched prediction line.\n",
        "        text_after_prediction = raw_output[match.end():].strip()\n",
        "\n",
        "        # The format requires no additional text after the prediction.\n",
        "        if text_after_prediction:\n",
        "            return ParsedLLMOutput(\n",
        "                is_valid=False,\n",
        "                reasoning=reasoning,\n",
        "                prediction=None,\n",
        "                error_message=\"Format Error: Extraneous text found after prediction line.\"\n",
        "            )\n",
        "\n",
        "        # --- Validation Step 4: Parse the numerical values from the prediction line ---\n",
        "        try:\n",
        "            # Extract the string content between the square brackets.\n",
        "            list_str = prediction_line.split('[', 1)[1].rsplit(']', 1)[0]\n",
        "\n",
        "            # Use a list comprehension to convert each comma-separated value to a float.\n",
        "            # This will raise a ValueError if any element is not a valid number.\n",
        "            prediction_list = [float(x) for x in list_str.split(',')]\n",
        "\n",
        "            # Convert the Python list to a numpy array with a specific dtype for consistency.\n",
        "            prediction = np.array(prediction_list, dtype=np.float64)\n",
        "\n",
        "        except (ValueError, IndexError) as e:\n",
        "            # Catch errors from splitting (IndexError) or float conversion (ValueError).\n",
        "            return ParsedLLMOutput(\n",
        "                is_valid=False,\n",
        "                reasoning=reasoning,\n",
        "                prediction=None,\n",
        "                error_message=f\"Parsing Error: Failed to parse numbers from prediction list. Details: {e}\"\n",
        "            )\n",
        "\n",
        "        # --- Validation Step 5: Validate the number of predictions ---\n",
        "        # The length of the parsed array must match the required prediction horizon (T').\n",
        "        if len(prediction) != self.prediction_horizon:\n",
        "            return ParsedLLMOutput(\n",
        "                is_valid=False,\n",
        "                reasoning=reasoning,\n",
        "                prediction=prediction, # Return the malformed prediction for debugging.\n",
        "                error_message=f\"Length Error: Expected {self.prediction_horizon} predictions, but found {len(prediction)}.\"\n",
        "            )\n",
        "\n",
        "        # --- Success ---\n",
        "        # If all checks pass, the output is considered valid.\n",
        "        return ParsedLLMOutput(\n",
        "            is_valid=True,\n",
        "            reasoning=reasoning,\n",
        "            prediction=prediction,\n",
        "            error_message=None\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Validate prompt length and tokenization\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_prompt_tokenization(\n",
        "    prompts: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates that all generated prompts are within the LLM's context limit.\n",
        "\n",
        "    Args:\n",
        "        prompts: A list of fully-formed prompt strings.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any prompt exceeds the specified context length.\n",
        "    \"\"\"\n",
        "    # Extract model and context length information from the config.\n",
        "    model_identifier = config['reasoning_model_config']['llm_settings']['base_model_identifier']\n",
        "    context_limit = config['reasoning_model_config']['llm_settings']['context_length_tokens']\n",
        "\n",
        "    # Load the specified tokenizer from Hugging Face.\n",
        "    try:\n",
        "        tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Failed to load tokenizer for '{model_identifier}'. Error: {e}\")\n",
        "\n",
        "    # Use the tokenizer's batch processing for efficiency.\n",
        "    tokenized_outputs = tokenizer(prompts, add_special_tokens=True)\n",
        "\n",
        "    # Check the length of each tokenized prompt.\n",
        "    oversized_prompts = []\n",
        "    for i, token_ids in enumerate(tokenized_outputs['input_ids']):\n",
        "        token_count = len(token_ids)\n",
        "        if token_count > context_limit:\n",
        "            oversized_prompts.append((i, token_count))\n",
        "\n",
        "    # If any prompts were oversized, raise a single, detailed error.\n",
        "    if oversized_prompts:\n",
        "        error_message = (\n",
        "            f\"{len(oversized_prompts)} prompts exceed the context limit of {context_limit} tokens.\\n\"\n",
        "            \"Details (prompt_index, token_count):\\n\"\n",
        "        )\n",
        "        for i, count in oversized_prompts[:5]: # Show first 5 examples\n",
        "            error_message += f\"- Prompt {i}: {count} tokens\\n\"\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def assemble_and_validate_prompts(\n",
        "    X_windows: np.ndarray,\n",
        "    metadata: pd.DataFrame,\n",
        "    annotations: Dict[str, List[str]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[List[str], LLMOutputParser]:\n",
        "    \"\"\"\n",
        "    Orchestrates the assembly and validation of prompts for the reasoning LLM.\n",
        "\n",
        "    This function executes the full workflow for Task 5:\n",
        "    1. Populates the prompt template with window-specific data.\n",
        "    2. Validates that all generated prompts are within the LLM's token limit.\n",
        "    3. Instantiates and returns a robust parser for handling LLM outputs.\n",
        "\n",
        "    Args:\n",
        "        X_windows: The 3D numpy array of input windows.\n",
        "        metadata: The comprehensive DataFrame with metadata for each window,\n",
        "                  including start/end dates and the date series.\n",
        "        annotations: The dictionary of formatted annotation strings from Task 4.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A list of validated, ready-to-use prompt strings.\n",
        "        - An instance of the LLMOutputParser for use in the training loop.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Populate the prompt template ---\n",
        "    # The call to _populate_prompts is now cleaner, without the cleansed_df dependency.\n",
        "    print(\"Step 1: Assembling prompts from templates and data...\")\n",
        "    prompts = _populate_prompts(X_windows, metadata, annotations, config)\n",
        "    print(f\"Assembled {len(prompts)} prompts.\")\n",
        "\n",
        "    # Handle the edge case of no prompts being generated.\n",
        "    if not prompts:\n",
        "        print(\"Warning: No prompts were generated.\")\n",
        "        # Still return a valid parser instance for downstream consistency.\n",
        "        return [], LLMOutputParser(config)\n",
        "\n",
        "    # --- Step 2: Validate prompt tokenization ---\n",
        "    # This step is critical to prevent runtime errors during training.\n",
        "    print(\"Step 2: Validating prompt lengths against context limit...\")\n",
        "    _validate_prompt_tokenization(prompts, config)\n",
        "    print(\"All prompts are within the context length limit.\")\n",
        "\n",
        "    # --- Step 3: Instantiate the output parser ---\n",
        "    # The parser is an essential utility for the subsequent RL training task.\n",
        "    print(\"Step 3: Instantiating LLM output parser...\")\n",
        "    parser = LLMOutputParser(config)\n",
        "\n",
        "    print(\"\\nTask 5: Prompt assembly and validation complete.\")\n",
        "    return prompts, parser\n"
      ],
      "metadata": {
        "id": "eGdoFXy5lJaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Train Reasoning LLM Ï€Î¸ â€“ Stage 1 (Cold-Start RL with Time-GRPO)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Train Reasoning LLM Ï€Î¸ â€“ Stage 1 (Cold-Start RL with Time-GRPO)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Data Structures for Clarity and Type Safety\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class ExperienceBuffer:\n",
        "    \"\"\"\n",
        "    A structured container for experiences collected during the RL sampling phase.\n",
        "\n",
        "    This dataclass ensures type safety and clarity when passing complex data\n",
        "    between the sampling and optimization stages of the PPO-style algorithm.\n",
        "    All tensors are expected to be on the same device and are detached from the\n",
        "    computation graph created during sampling.\n",
        "\n",
        "    Attributes:\n",
        "        prompt_tokens: Tensor of shape (batch_size * group_size, prompt_len)\n",
        "                       containing the tokenized prompts.\n",
        "        prompt_attention_mask: Tensor of shape (batch_size * group_size, prompt_len)\n",
        "                               for attending to the prompt tokens.\n",
        "        generated_tokens: Tensor of shape (batch_size * group_size, full_seq_len)\n",
        "                          containing the full generated sequences (prompt + completion).\n",
        "        log_probs_old: Tensor of shape (batch_size * group_size,) containing the\n",
        "                       log probability of each generated sequence under the policy\n",
        "                       that generated it (Ï€_Î¸_old).\n",
        "        rewards: Tensor of shape (batch_size * group_size,) containing the final\n",
        "                 scalar reward for each generated sequence.\n",
        "        advantages: Tensor of shape (batch_size * group_size,) containing the\n",
        "                    calculated advantage for each sequence, normalized within its group.\n",
        "        values_old: Tensor of shape (batch_size * group_size,) containing the\n",
        "                    value function's estimate of the expected reward for each prompt state.\n",
        "    \"\"\"\n",
        "    prompt_tokens: torch.Tensor\n",
        "    prompt_attention_mask: torch.Tensor\n",
        "    generated_tokens: torch.Tensor\n",
        "    log_probs_old: torch.Tensor\n",
        "    rewards: torch.Tensor\n",
        "    advantages: torch.Tensor\n",
        "    values_old: torch.Tensor\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Initialize the base model and LoRA adapters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class ValueHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A standalone value head module for PPO.\n",
        "\n",
        "    This module is placed on top of the LLM's transformer backbone to predict\n",
        "    the value function (the expected future reward) from a given state, which\n",
        "    is represented by the LLM's hidden states. Its parameters are trained\n",
        "    concurrently with the policy's LoRA adapters.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int, dropout_prob: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes the value head.\n",
        "\n",
        "        Args:\n",
        "            hidden_size: The dimensionality of the LLM's hidden states, which\n",
        "                         serves as the input dimension for this head.\n",
        "            dropout_prob: The dropout probability for regularization before the\n",
        "                          final linear layer.\n",
        "        \"\"\"\n",
        "        # Initialize the parent torch.nn.Module.\n",
        "        super().__init__()\n",
        "\n",
        "        # A dropout layer for regularization to prevent overfitting.\n",
        "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
        "\n",
        "        # A single linear layer that maps the hidden state to a scalar value.\n",
        "        self.linear = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass to predict the state value.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: A tensor of shape (batch_size, seq_len, hidden_size)\n",
        "                           from the LLM's backbone.\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape (batch_size,) containing the predicted scalar value\n",
        "            for each state in the batch.\n",
        "        \"\"\"\n",
        "        # We use the hidden state of the last token as the representation of the\n",
        "        # entire sequence state. This is a common practice.\n",
        "        last_hidden_state = hidden_states[:, -1, :]\n",
        "\n",
        "        # Apply dropout for regularization.\n",
        "        last_hidden_state = self.dropout(last_hidden_state)\n",
        "\n",
        "        # Pass through the linear layer to get the scalar value prediction.\n",
        "        # .squeeze(-1) removes the trailing dimension of size 1.\n",
        "        return self.linear(last_hidden_state).squeeze(-1)\n",
        "\n",
        "\n",
        "def _initialize_rl_models(\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Initializes and configures all models required for RL training.\n",
        "\n",
        "    This function performs the critical setup for the RL agent:\n",
        "    1. Loads the base LLM and its tokenizer from Hugging Face.\n",
        "    2. Applies LoRA adapters to the base model to create the trainable `policy_model`.\n",
        "    3. Creates a frozen, deep copy of the base model as the `reference_model`.\n",
        "    4. Instantiates a trainable `value_model` head.\n",
        "\n",
        "    Args:\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device (e.g., 'cuda' or 'cpu') for model placement.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all initialized components: 'policy_model',\n",
        "        'reference_model', 'value_model', and 'tokenizer'.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a specified LoRA target module does not exist in the model.\n",
        "        IOError: If the model or tokenizer cannot be loaded from Hugging Face.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Load Base Model and Tokenizer ---\n",
        "        # Extract the model identifier from the configuration.\n",
        "        model_id = config['reasoning_model_config']['llm_settings']['base_model_identifier']\n",
        "        print(f\"Loading base model '{model_id}' onto device '{device}'...\")\n",
        "\n",
        "        # Load the base causal language model using bfloat16 for performance on compatible GPUs.\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Load the tokenizer associated with the base model.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle potential errors during model loading (e.g., network issues, invalid ID).\n",
        "        raise IOError(f\"Failed to load base model or tokenizer for '{model_id}'. Error: {e}\")\n",
        "\n",
        "    # Standard practice for causal LMs: set pad token to EOS token if not defined.\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Set padding side to 'left' for correct batch generation with autoregressive models.\n",
        "    tokenizer.padding_side = 'left'\n",
        "\n",
        "    # --- Create Policy Model (Ï€Î¸) with LoRA ---\n",
        "    # Extract the LoRA configuration dictionary.\n",
        "    lora_config_dict = config['reasoning_model_config']['lora_config']\n",
        "\n",
        "    # Input validation: Ensure that the specified LoRA target modules exist in the model.\n",
        "    model_modules = {name for name, _ in base_model.named_modules()}\n",
        "    for module in lora_config_dict['target_modules']:\n",
        "        if module not in model_modules:\n",
        "            # Raise a descriptive error to help the user correct the configuration.\n",
        "            raise ValueError(\n",
        "                f\"LoRA target module '{module}' not found in model. \"\n",
        "                f\"Available modules include (first 10): {list(model_modules)[:10]}...\"\n",
        "            )\n",
        "\n",
        "    # Define the PEFT configuration for LoRA from the master config.\n",
        "    peft_config = LoraConfig(\n",
        "        r=lora_config_dict['rank_r'],\n",
        "        lora_alpha=lora_config_dict['alpha'],\n",
        "        lora_dropout=lora_config_dict['dropout'],\n",
        "        target_modules=lora_config_dict['target_modules'],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # Apply the LoRA config to the base model to create the policy model.\n",
        "    # This freezes all base model weights and inserts trainable LoRA adapters.\n",
        "    policy_model = get_peft_model(base_model, peft_config)\n",
        "    print(\"\\n--- Policy Model (Ï€Î¸) Trainable Parameters ---\")\n",
        "    # Print a summary of trainable vs. total parameters for verification.\n",
        "    policy_model.print_trainable_parameters()\n",
        "\n",
        "    # --- Create Reference Model (Ï€ref) ---\n",
        "    # The reference model is a frozen copy used for the KL divergence penalty.\n",
        "    # A deep copy ensures its weights are independent of the policy model's base.\n",
        "    reference_model = copy.deepcopy(base_model)\n",
        "    # Set the model to evaluation mode to disable dropout, etc.\n",
        "    reference_model.eval()\n",
        "    # Explicitly disable gradient calculations for all parameters.\n",
        "    for param in reference_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(f\"\\n--- Reference Model (Ï€ref) created with all parameters frozen. ---\")\n",
        "\n",
        "    # --- Create Value Model ---\n",
        "    # The value model is a separate head to predict the expected reward.\n",
        "    value_model = ValueHead(base_model.config.hidden_size).to(device)\n",
        "    # Set the value model to training mode.\n",
        "    value_model.train()\n",
        "    print(f\"\\n--- Value Head created with trainable parameters. ---\")\n",
        "\n",
        "    # Return all components in a structured dictionary.\n",
        "    return {\n",
        "        'policy_model': policy_model,\n",
        "        'reference_model': reference_model,\n",
        "        'value_model': value_model,\n",
        "        'tokenizer': tokenizer\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Sampling protocol and reward computation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_sequence_log_probs(\n",
        "    model: PreTrainedModel,\n",
        "    full_tokens: torch.Tensor,\n",
        "    attention_mask: torch.Tensor,\n",
        "    prompt_len: int\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates the log probability of the generated portion of a sequence.\n",
        "\n",
        "    This function is a critical component of PPO, used to calculate the log\n",
        "    probabilities of actions (tokens) under a given policy (model). It carefully\n",
        "    handles padding and separates the prompt from the generation.\n",
        "\n",
        "    Args:\n",
        "        model: The model (policy or reference) to use for the calculation.\n",
        "        full_tokens: A tensor of shape (batch_size, seq_len) containing the\n",
        "                     full tokenized sequences (prompt + generation).\n",
        "        attention_mask: The attention mask for `full_tokens`.\n",
        "        prompt_len: The length of the prompt portion of the sequences.\n",
        "\n",
        "    Returns:\n",
        "        A 1D tensor of shape (batch_size,) with the summed log probability\n",
        "        for the generated part of each sequence.\n",
        "    \"\"\"\n",
        "    # Perform a forward pass to get the logits for the entire sequence.\n",
        "    # `torch.no_grad()` is used for efficiency as we don't need gradients here.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(full_tokens, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Convert the raw logits to log probabilities using log_softmax.\n",
        "    log_probs_all = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Isolate the tokens that were generated (i.e., the \"actions\").\n",
        "    gen_tokens = full_tokens[:, prompt_len:]\n",
        "\n",
        "    # Gather the log probabilities of the specific tokens that were generated.\n",
        "    # The logits at time step `t-1` predict the token at time step `t`.\n",
        "    # So, we align `logits[:, prompt_len-1:-1]` with `gen_tokens`.\n",
        "    gathered_log_probs = torch.gather(\n",
        "        log_probs_all[:, prompt_len - 1:-1, :],\n",
        "        dim=2,\n",
        "        index=gen_tokens.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    # Create a mask to exclude padding tokens from the sum of log probabilities.\n",
        "    gen_attention_mask = (gen_tokens != model.config.pad_token_id)\n",
        "\n",
        "    # Element-wise multiply by the mask to zero out log probs of padding tokens,\n",
        "    # then sum along the sequence dimension to get the total log prob for each sequence.\n",
        "    sequence_log_probs = (gathered_log_probs * gen_attention_mask).sum(dim=1)\n",
        "\n",
        "    return sequence_log_probs\n",
        "\n",
        "\n",
        "def _compute_rewards(\n",
        "    parsed_outputs: List[ParsedLLMOutput],\n",
        "    ground_truth_targets: torch.Tensor,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the combined reward for a batch of generated and parsed outputs.\n",
        "\n",
        "    The reward function combines a binary format reward with a continuous,\n",
        "    performance-based reward derived from the inverse Mean Squared Error.\n",
        "\n",
        "    Args:\n",
        "        parsed_outputs: A list of ParsedLLMOutput objects from the parser.\n",
        "        ground_truth_targets: A tensor of shape (batch_size, T') with the true\n",
        "                              future prices, aligned with `parsed_outputs`.\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device where tensors should reside.\n",
        "\n",
        "    Returns:\n",
        "        A 1D tensor of shape (batch_size,) containing the final reward for each output.\n",
        "    \"\"\"\n",
        "    # Extract relevant hyperparameters from the configuration.\n",
        "    rl_config = config['reasoning_model_config']['rl_training_params']\n",
        "    w_format = rl_config['format_reward_weight']\n",
        "    lambda_mse = rl_config['reward_scale_lambda']\n",
        "\n",
        "    # Pre-allocate a tensor to store the rewards for the batch.\n",
        "    rewards = torch.zeros(len(parsed_outputs), device=device, dtype=torch.float32)\n",
        "\n",
        "    # Iterate through each parsed output to calculate its reward.\n",
        "    for i, parsed in enumerate(parsed_outputs):\n",
        "        # If the output format is invalid, the reward is zero. The format reward\n",
        "        # component is implicitly zero, and the MSE component is not calculated.\n",
        "        if not parsed.is_valid:\n",
        "            rewards[i] = 0.0\n",
        "            continue\n",
        "\n",
        "        # Convert the parsed numpy prediction to a torch tensor on the correct device.\n",
        "        prediction_tensor = torch.from_numpy(parsed.prediction).to(device)\n",
        "\n",
        "        # Equation: r_MSE = 1 / (Î» * ||Å· - y||^2 + Îµ)\n",
        "        # Calculate the Mean Squared Error between the prediction and the ground truth.\n",
        "        mse = torch.nn.functional.mse_loss(prediction_tensor, ground_truth_targets[i])\n",
        "        # Calculate the inverse MSE reward, adding a small epsilon for numerical stability.\n",
        "        inverse_mse_reward = 1.0 / (lambda_mse * mse + 1e-8)\n",
        "\n",
        "        # Equation: r_total = w_format * r_format + r_MSE\n",
        "        # Since the format is valid, r_format is 1.0.\n",
        "        total_reward = w_format * 1.0 + inverse_mse_reward\n",
        "        rewards[i] = total_reward\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def _sample_and_collect_experiences(\n",
        "    prompts: List[str],\n",
        "    targets: torch.Tensor,\n",
        "    models: Dict[str, Any],\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> ExperienceBuffer:\n",
        "    \"\"\"\n",
        "    Performs the sampling phase of the RL loop to generate experiences for a batch.\n",
        "\n",
        "    This function orchestrates the interaction between the policy model and the\n",
        "    environment (i.e., the reward function) to collect a buffer of experiences\n",
        "    that will be used for the optimization step.\n",
        "\n",
        "    Args:\n",
        "        prompts: A batch of prompt strings.\n",
        "        targets: The ground truth target tensors for the batch.\n",
        "        models: The dictionary of initialized models.\n",
        "        parser: The LLM output parser.\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device.\n",
        "\n",
        "    Returns:\n",
        "        An `ExperienceBuffer` dataclass instance containing all collected data.\n",
        "    \"\"\"\n",
        "    # Unpack the required models and configuration settings.\n",
        "    policy_model = models['policy_model']\n",
        "    value_model = models['value_model']\n",
        "    tokenizer = models['tokenizer']\n",
        "\n",
        "    rl_config = config['reasoning_model_config']['rl_training_params']\n",
        "    group_size = rl_config['group_size_G']\n",
        "\n",
        "    # --- Step 1: Generate G responses for each prompt in the batch ---\n",
        "    # Repeat each prompt G times to facilitate group-based advantage normalization.\n",
        "    batch_prompts_repeated = [p for p in prompts for _ in range(group_size)]\n",
        "\n",
        "    # Tokenize the repeated prompts, padding and truncating as necessary.\n",
        "    inputs = tokenizer(\n",
        "        batch_prompts_repeated,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=tokenizer.model_max_length\n",
        "    ).to(device)\n",
        "\n",
        "    # Create a GenerationConfig object from the decoding settings in the master config.\n",
        "    gen_config = GenerationConfig(\n",
        "        **rl_config['decoding_settings_for_sampling'],\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Generate sequences from the policy model without tracking gradients.\n",
        "    with torch.no_grad():\n",
        "        generated_outputs = policy_model.generate(**inputs, generation_config=gen_config)\n",
        "\n",
        "    # Decode the generated token sequences back into human-readable text.\n",
        "    decoded_texts = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Parse each generated text to validate its format and extract its contents.\n",
        "    parsed_outputs = [parser.parse(text) for text in decoded_texts]\n",
        "\n",
        "    # --- Step 2: Compute Rewards ---\n",
        "    # Repeat the ground truth targets to align with the G generated responses per prompt.\n",
        "    repeated_targets = targets.repeat_interleave(group_size, dim=0)\n",
        "    rewards = _compute_rewards(parsed_outputs, repeated_targets, config, device)\n",
        "\n",
        "    # --- Step 3: Compute Old Log Probs and Values ---\n",
        "    # The length of the prompt portion of the tokenized sequences.\n",
        "    prompt_len = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # Get the log probabilities of the generated sequences under the policy that generated them (Ï€_Î¸_old).\n",
        "    log_probs_old = _get_sequence_log_probs(\n",
        "        policy_model,\n",
        "        generated_outputs,\n",
        "        generated_outputs != tokenizer.pad_token_id,\n",
        "        prompt_len\n",
        "    )\n",
        "\n",
        "    # Get the value estimates for the initial state (the prompt).\n",
        "    with torch.no_grad():\n",
        "        # Get hidden states from the base model's transformer backbone using the prompt tokens.\n",
        "        prompt_hidden_states = policy_model.get_base_model().transformer(\n",
        "            **inputs\n",
        "        )[0]\n",
        "        # Predict the value from these hidden states.\n",
        "        values_old = value_model(prompt_hidden_states)\n",
        "\n",
        "    # --- Step 4: Compute Group-Relative Advantages ---\n",
        "    # Equation: A_i = (r_i - mean(r_group)) / (std(r_group) + Îµ)\n",
        "    # Reshape rewards to (num_prompts, group_size) to operate on groups.\n",
        "    rewards_grouped = rewards.view(-1, group_size)\n",
        "    # Calculate mean and standard deviation for each group.\n",
        "    mean_rewards = rewards_grouped.mean(dim=1, keepdim=True)\n",
        "    std_rewards = rewards_grouped.std(dim=1, keepdim=True)\n",
        "    # Normalize rewards to get advantages, adding epsilon for stability.\n",
        "    advantages = (rewards_grouped - mean_rewards) / (std_rewards + 1e-8)\n",
        "\n",
        "    # Create and return the experience buffer, detaching all tensors from the computation graph.\n",
        "    return ExperienceBuffer(\n",
        "        prompt_tokens=inputs['input_ids'],\n",
        "        prompt_attention_mask=inputs['attention_mask'],\n",
        "        generated_tokens=generated_outputs,\n",
        "        log_probs_old=log_probs_old.detach(),\n",
        "        rewards=rewards.detach(),\n",
        "        advantages=advantages.view(-1).detach(), # Flatten advantages back to a 1D tensor.\n",
        "        values_old=values_old.detach()\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Optimize with the Time-GRPO objective and KL penalty\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_and_optimize_loss(\n",
        "    experience: ExperienceBuffer,\n",
        "    models: Dict[str, Any],\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes the full Time-GRPO loss with the corrected KL penalty and performs one optimization step.\n",
        "\n",
        "    This function is the core of the learning process in the RL loop.\n",
        "\n",
        "    Process:\n",
        "    1.  Re-computes log probabilities and value estimates under the current policy (Ï€_Î¸).\n",
        "    2.  Calculates the PPO Clipped Surrogate Objective for the policy loss.\n",
        "    3.  Calculates the Mean Squared Error loss for the value function.\n",
        "    4.  Calculates the forward KL divergence penalty, D_KL(Ï€_Î¸ || Ï€_ref), as specified.\n",
        "    5.  Combines these components into a total loss and performs a single\n",
        "        gradient-based optimization step.\n",
        "\n",
        "    Args:\n",
        "        experience: The `ExperienceBuffer` dataclass instance collected from the\n",
        "                    sampling phase.\n",
        "        models: The dictionary of initialized models ('policy_model', 'value_model',\n",
        "                'reference_model').\n",
        "        optimizer: The PyTorch optimizer for the trainable parameters.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of metrics for monitoring training progress, including all\n",
        "        individual loss components and other diagnostics.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure all required models are present.\n",
        "    required_models = {'policy_model', 'value_model', 'reference_model', 'tokenizer'}\n",
        "    if not required_models.issubset(models.keys()):\n",
        "        raise ValueError(f\"Models dictionary is missing one of the required keys: {required_models}\")\n",
        "\n",
        "    # Unpack models and configuration for clarity.\n",
        "    policy_model = models['policy_model']\n",
        "    value_model = models['value_model']\n",
        "    reference_model = models['reference_model']\n",
        "    tokenizer = models['tokenizer']\n",
        "\n",
        "    rl_config = config['reasoning_model_config']['rl_training_params']\n",
        "    ppo_clip_eps = rl_config['ppo_clip_epsilon']\n",
        "    kl_beta = rl_config['kl_weight_beta']\n",
        "\n",
        "    # --- Re-computation of Log Probs and Values under Current Policy (Ï€_Î¸) ---\n",
        "    # This step is necessary to get gradients for the current policy parameters.\n",
        "    full_sequences = experience.generated_tokens\n",
        "    attention_mask = (full_sequences != tokenizer.pad_token_id)\n",
        "    prompt_len = experience.prompt_tokens.shape[1]\n",
        "\n",
        "    # Perform a single forward pass through the backbone to get hidden states.\n",
        "    # This is more efficient than separate passes for the policy and value heads.\n",
        "    base_model_output = policy_model.get_base_model().transformer(\n",
        "        input_ids=full_sequences, attention_mask=attention_mask\n",
        "    )\n",
        "    hidden_states = base_model_output[0]\n",
        "\n",
        "    # Get new logits from the full policy model's language modeling head.\n",
        "    logits_new = policy_model.lm_head(hidden_states)\n",
        "\n",
        "    # Get new value predictions from the value head.\n",
        "    values_new = value_model(hidden_states)\n",
        "\n",
        "    # Calculate log probabilities of the generated tokens under the new policy.\n",
        "    log_probs_new_all = torch.log_softmax(logits_new, dim=-1)\n",
        "    gen_tokens = full_sequences[:, prompt_len:]\n",
        "    gen_log_probs_new = torch.gather(log_probs_new_all[:, prompt_len - 1:-1, :], 2, gen_tokens.unsqueeze(-1)).squeeze(-1)\n",
        "    # Mask out padding tokens before summing.\n",
        "    sum_log_probs_new = (gen_log_probs_new * (gen_tokens != tokenizer.pad_token_id)).sum(dim=1)\n",
        "\n",
        "    # --- 1. Clipped Surrogate Objective (Policy Loss) ---\n",
        "    # Equation: r_t(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s) = exp(log_prob_new - log_prob_old)\n",
        "    # Calculate the probability ratio between the new and old policies in log space for stability.\n",
        "    ratio = torch.exp(sum_log_probs_new - experience.log_probs_old)\n",
        "\n",
        "    # Retrieve the pre-calculated advantages from the experience buffer.\n",
        "    advantages = experience.advantages\n",
        "\n",
        "    # Calculate the unclipped policy objective.\n",
        "    loss_clip1 = ratio * advantages\n",
        "\n",
        "    # Calculate the clipped policy objective.\n",
        "    loss_clip2 = torch.clamp(ratio, 1 - ppo_clip_eps, 1 + ppo_clip_eps) * advantages\n",
        "\n",
        "    # The policy loss is the negative of the minimum of the clipped and unclipped objectives.\n",
        "    # We take the negative because optimizers perform minimization, but PPO maximizes the objective.\n",
        "    policy_loss = -torch.min(loss_clip1, loss_clip2).mean()\n",
        "\n",
        "    # --- 2. Value Function Loss ---\n",
        "    # Standard Mean Squared Error loss between the predicted values and the actual rewards.\n",
        "    # The 0.5 scaling factor is a conventional choice in PPO implementations.\n",
        "    value_loss = 0.5 * torch.nn.functional.mse_loss(values_new, experience.rewards)\n",
        "\n",
        "    # --- 3. KL Divergence Penalty (Corrected Implementation) ---\n",
        "    # Get log probabilities of the generated sequences from the frozen reference model (Ï€_ref).\n",
        "    # This is done within a no_grad context to ensure no gradients are computed for the reference model.\n",
        "    with torch.no_grad():\n",
        "        log_probs_ref = _get_sequence_log_probs(\n",
        "            reference_model,\n",
        "            full_sequences,\n",
        "            attention_mask,\n",
        "            prompt_len\n",
        "        )\n",
        "\n",
        "    # CORRECTED KL Divergence Calculation.\n",
        "    # Equation: D_KL(Ï€_Î¸ || Ï€_ref) â‰ˆ E_samples[log(Ï€_Î¸) - log(Ï€_ref)]\n",
        "    # This is the forward KL divergence, as specified in the paper's loss function.\n",
        "    kl_div = (sum_log_probs_new - log_probs_ref).mean()\n",
        "\n",
        "    # --- 4. Total Loss ---\n",
        "    # Combine the three loss components into a single scalar loss.\n",
        "    # Equation: L_total = L_policy + L_value + Î² * D_KL(Ï€_Î¸ || Ï€_ref)\n",
        "    total_loss = policy_loss + value_loss + kl_beta * kl_div\n",
        "\n",
        "    # --- Optimization Step ---\n",
        "    # Zero out gradients from the previous optimization step.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform backpropagation to compute gradients of the total loss\n",
        "    # with respect to the trainable parameters (LoRA adapters and value head).\n",
        "    total_loss.backward()\n",
        "\n",
        "    # Apply gradient clipping to prevent exploding gradients, a crucial stability measure in RL.\n",
        "    if rl_config.get('grad_clip_norm'):\n",
        "        # Get all parameters that require gradients.\n",
        "        trainable_params = [p for p in policy_model.parameters() if p.requires_grad] + \\\n",
        "                           list(value_model.parameters())\n",
        "        # Clip the norm of the gradients.\n",
        "        torch.nn.utils.clip_grad_norm_(trainable_params, rl_config['grad_clip_norm'])\n",
        "\n",
        "    # Update the model weights using the computed gradients.\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- Logging ---\n",
        "    # Return a dictionary of metrics for monitoring training progress.\n",
        "    # All metrics are detached and moved to CPU for logging to avoid holding onto the graph.\n",
        "    with torch.no_grad():\n",
        "        metrics = {\n",
        "            'loss/policy': policy_loss.item(),\n",
        "            'loss/value': value_loss.item(),\n",
        "            'loss/kl_forward': kl_div.item(), # Log the corrected KL term\n",
        "            'loss/total': total_loss.item(),\n",
        "            'reward/mean': experience.rewards.mean().item(),\n",
        "            'policy/ratio_mean': ratio.mean().item(),\n",
        "            'policy/adv_mean': advantages.mean().item(),\n",
        "            'log_probs/new_mean': sum_log_probs_new.mean().item(),\n",
        "            'log_probs/ref_mean': log_probs_ref.mean().item(),\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_reasoning_llm_stage1(\n",
        "    prompts: List[str],\n",
        "    y_targets: np.ndarray,\n",
        "    metadata: pd.DataFrame,\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the Stage 1 \"Cold-Start\" RL training process.\n",
        "\n",
        "    This function manages the entire end-to-end workflow for Stage 1:\n",
        "    1. Initializes all required models (policy, reference, value) and tokenizer.\n",
        "    2. Sets up the optimizer and data loader.\n",
        "    3. Runs the main training loop for a specified number of steps.\n",
        "    4. In each step, it samples experiences and performs multiple optimization updates.\n",
        "    5. Logs detailed metrics for monitoring.\n",
        "    6. Saves the final trained model adapters and collected experiences.\n",
        "\n",
        "    Args:\n",
        "        prompts: A list of all training prompts.\n",
        "        y_targets: A numpy array of all ground truth targets.\n",
        "        metadata: The DataFrame with metadata for each window.\n",
        "        parser: An instance of the LLMOutputParser.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the path to the saved LoRA model and a DataFrame\n",
        "        of all collected experiences for use in Stage 2.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    rl_config = config['reasoning_model_config']['rl_training_params']\n",
        "\n",
        "    print(\"--- Stage 1: Cold-Start RL Training ---\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Step 1: Initialize models\n",
        "    models = _initialize_rl_models(config, device)\n",
        "\n",
        "    # Setup optimizer for the trainable parameters (LoRA adapters + value head).\n",
        "    trainable_params = [p for p in models['policy_model'].parameters() if p.requires_grad] + \\\n",
        "                       list(models['value_model'].parameters())\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=rl_config['learning_rate'])\n",
        "\n",
        "    # Setup DataLoader to handle batching of prompts and targets.\n",
        "    dataset = TensorDataset(torch.arange(len(prompts)), torch.from_numpy(y_targets).float())\n",
        "    dataloader = DataLoader(dataset, batch_size=rl_config['batch_size'], shuffle=True)\n",
        "\n",
        "    # List to store all generated experiences for Stage 2.\n",
        "    all_experiences_data = []\n",
        "\n",
        "    # --- Main Training Loop ---\n",
        "    print(f\"Starting training for {rl_config['max_steps']} steps...\")\n",
        "    step = 0\n",
        "    training_complete = False\n",
        "    while not training_complete:\n",
        "        for batch_indices, batch_targets in dataloader:\n",
        "            if step >= rl_config['max_steps']:\n",
        "                training_complete = True\n",
        "                break\n",
        "\n",
        "            # Prepare batch data.\n",
        "            batch_prompts = [prompts[i] for i in batch_indices.tolist()]\n",
        "            batch_metadata = metadata.iloc[batch_indices.tolist()]\n",
        "            batch_targets = batch_targets.to(device)\n",
        "\n",
        "            # Step 2: Sample and collect experiences for the current batch.\n",
        "            experience = _sample_and_collect_experiences(\n",
        "                batch_prompts, batch_targets, models, parser, config, device\n",
        "            )\n",
        "\n",
        "            # Store experiences for Stage 2.\n",
        "            # This is a critical step for the next stage of the pipeline.\n",
        "            # We need to decode the generated text and store it with metadata.\n",
        "            decoded_texts = models['tokenizer'].batch_decode(experience.generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "            # Reshape rewards and other data to match the group structure.\n",
        "            group_size = rl_config['group_size_G']\n",
        "            num_prompts_in_batch = len(batch_prompts)\n",
        "\n",
        "            for i in range(num_prompts_in_batch):\n",
        "                for j in range(group_size):\n",
        "                    idx = i * group_size + j\n",
        "                    meta = batch_metadata.iloc[i]\n",
        "                    all_experiences_data.append({\n",
        "                        'prompt': batch_prompts[i],\n",
        "                        'generation': decoded_texts[idx],\n",
        "                        'reward': experience.rewards[idx].item(),\n",
        "                        'ticker': meta['ticker'],\n",
        "                        'window_end_date': meta['window_end_date'],\n",
        "                        'ground_truth': y_targets[batch_indices[i]]\n",
        "                    })\n",
        "\n",
        "            # Step 3: Perform multiple optimization updates on the collected experiences.\n",
        "            for _ in range(rl_config['updates_per_batch']):\n",
        "                metrics = _compute_and_optimize_loss(experience, models, optimizer, config)\n",
        "\n",
        "            # Log metrics periodically.\n",
        "            if step % 10 == 0:\n",
        "                log_str = f\"Step {step}/{rl_config['max_steps']} | \"\n",
        "                log_str += \" | \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items()])\n",
        "                print(log_str)\n",
        "\n",
        "            step += 1\n",
        "\n",
        "    print(\"\\n--- Stage 1 Training Complete ---\")\n",
        "\n",
        "    # --- Save Artifacts ---\n",
        "    # Save the trained LoRA adapters for the policy model.\n",
        "    output_dir = \"./models/stage1_lora\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    models['policy_model'].save_pretrained(output_dir)\n",
        "    print(f\"Trained LoRA adapters saved to {output_dir}\")\n",
        "\n",
        "    # Convert the collected experiences into a structured DataFrame.\n",
        "    experiences_df = pd.DataFrame(all_experiences_data)\n",
        "\n",
        "    return {\n",
        "        \"lora_model_path\": output_dir,\n",
        "        \"collected_experiences_df\": experiences_df\n",
        "    }\n"
      ],
      "metadata": {
        "id": "bkwzKW6Tpqz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Train Reasoning LLM Ï€Î¸ â€“ Stage 2 (Rejection Sampling + SFT)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Train Reasoning LLM Ï€Î¸ â€“ Stage 2 (Rejection Sampling + SFT)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Filter high-quality samples via rejection sampling\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _filter_high_quality_samples(\n",
        "    experiences_df: pd.DataFrame,\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Filters the collected experiences to keep only the highest-performing samples.\n",
        "\n",
        "    This function implements the rejection sampling stage. It calculates the MSE\n",
        "    for each valid generation, buckets the samples by ticker and time period,\n",
        "    and keeps only the top performers (bottom 10th percentile of MSE) within\n",
        "    each bucket.\n",
        "\n",
        "    Args:\n",
        "        experiences_df: A DataFrame containing all experiences from Stage 1.\n",
        "        parser: An instance of the LLMOutputParser to parse generations.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A DataFrame of the filtered, high-quality \"golden\" samples.\n",
        "        - A summary DataFrame reporting the filtering statistics for each bucket.\n",
        "    \"\"\"\n",
        "    # --- Data Preparation and MSE Calculation ---\n",
        "    # Create a working copy of the DataFrame.\n",
        "    df = experiences_df.copy()\n",
        "\n",
        "    # Parse all generations to extract predictions.\n",
        "    parsed_results = [parser.parse(gen) for gen in df['generation']]\n",
        "\n",
        "    # Add parsed predictions and validity flags to the DataFrame.\n",
        "    df['is_valid'] = [p.is_valid for p in parsed_results]\n",
        "    df['prediction'] = [p.prediction for p in parsed_results]\n",
        "\n",
        "    # Filter out any samples that were not validly parsed.\n",
        "    valid_df = df[df['is_valid']].copy()\n",
        "\n",
        "    # Calculate the Mean Squared Error for each valid prediction.\n",
        "    # Equation: MSE = ||Å·_Î¸ - y||^2\n",
        "    mses = valid_df.apply(\n",
        "        lambda row: np.mean((row['prediction'] - row['ground_truth'])**2),\n",
        "        axis=1\n",
        "    )\n",
        "    valid_df['mse'] = mses\n",
        "\n",
        "    # --- Bucketing and Filtering ---\n",
        "    # Extract rejection sampling configuration.\n",
        "    rs_config = config['reasoning_model_config']['rl_training_params']['rejection_sampling']\n",
        "    percentile_to_keep = rs_config['percentile_keep'] / 100.0 # Convert to 0.10\n",
        "\n",
        "    # Create the 'time_period' bucket key from the window end date (e.g., '2023-01').\n",
        "    valid_df['time_period'] = pd.to_datetime(valid_df['window_end_date']).dt.to_period('M')\n",
        "\n",
        "    # Define the keys for grouping samples into buckets.\n",
        "    bucket_keys = rs_config['bucket_keys']\n",
        "\n",
        "    # Calculate the MSE threshold (e.g., 10th percentile) for each bucket.\n",
        "    # `transform` broadcasts the result back to the original DataFrame's shape.\n",
        "    valid_df['mse_threshold'] = valid_df.groupby(bucket_keys)['mse'].transform(\n",
        "        lambda x: x.quantile(percentile_to_keep)\n",
        "    )\n",
        "\n",
        "    # Keep only the samples whose MSE is at or below the threshold for their bucket.\n",
        "    golden_samples_df = valid_df[valid_df['mse'] <= valid_df['mse_threshold']].copy()\n",
        "\n",
        "    # --- Reporting ---\n",
        "    # Create a summary report of the filtering process.\n",
        "    original_counts = valid_df.groupby(bucket_keys).size().rename('original_count')\n",
        "    filtered_counts = golden_samples_df.groupby(bucket_keys).size().rename('filtered_count')\n",
        "\n",
        "    summary_report = pd.concat([original_counts, filtered_counts], axis=1).fillna(0).astype(int)\n",
        "    summary_report['percent_kept'] = (summary_report['filtered_count'] / summary_report['original_count'] * 100).round(2)\n",
        "\n",
        "    # Clean up temporary columns before returning.\n",
        "    golden_samples_df = golden_samples_df.drop(columns=['is_valid', 'prediction', 'mse', 'mse_threshold', 'time_period'])\n",
        "\n",
        "    return golden_samples_df, summary_report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Supervised fine-tuning (SFT) on filtered samples\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class SFTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for Supervised Fine-Tuning (SFT) of a causal language model.\n",
        "\n",
        "    This class is specifically designed to format data for a standard SFT task\n",
        "    where the model is trained to predict a `generation` sequence given a `prompt`.\n",
        "    It handles the crucial logic of tokenizing the combined text and creating\n",
        "    labels where the prompt portion is masked out, ensuring that the loss is\n",
        "    calculated only on the tokens the model is supposed to generate.\n",
        "\n",
        "    This is a fundamental component for the Hugging Face `Trainer` API.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        generations: List[str],\n",
        "        tokenizer: PreTrainedTokenizer\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the SFTDataset.\n",
        "\n",
        "        Args:\n",
        "            prompts: A list of the input prompt strings.\n",
        "            generations: A list of the target generation strings, corresponding\n",
        "                         one-to-one with the prompts.\n",
        "            tokenizer: The pre-trained tokenizer to be used for encoding the text.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the number of prompts does not match the number of\n",
        "                        generations.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Ensure that the prompts and generations lists are of the same length.\n",
        "        if len(prompts) != len(generations):\n",
        "            raise ValueError(\n",
        "                f\"Mismatch in dataset size: \"\n",
        "                f\"Got {len(prompts)} prompts and {len(generations)} generations.\"\n",
        "            )\n",
        "\n",
        "        # Store the dataset components.\n",
        "        self.prompts = prompts\n",
        "        self.generations = generations\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            The integer count of prompt-generation pairs.\n",
        "        \"\"\"\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Retrieves and processes a single sample from the dataset.\n",
        "\n",
        "        This method performs the following steps for a given index `idx`:\n",
        "        1. Concatenates the prompt and generation into a single text sequence.\n",
        "        2. Tokenizes the full sequence to get `input_ids` and `attention_mask`.\n",
        "        3. Creates a `labels` tensor, which is initially a copy of `input_ids`.\n",
        "        4. Masks the prompt portion of the `labels` tensor by setting the\n",
        "           corresponding token IDs to -100, the standard ignore index for\n",
        "           PyTorch cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the processed tensors for a single sample,\n",
        "            typically including 'input_ids', 'attention_mask', and 'labels'.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Ensure the requested index is within the valid range.\n",
        "        if not 0 <= idx < len(self):\n",
        "            raise IndexError(f\"Index {idx} is out of bounds for a dataset of size {len(self)}.\")\n",
        "\n",
        "        # --- Data Processing ---\n",
        "        # Retrieve the prompt and generation for the specified index.\n",
        "        prompt = self.prompts[idx]\n",
        "        generation = self.generations[idx]\n",
        "\n",
        "        # Combine the prompt and generation to form the full text sequence\n",
        "        # that the model will see as input.\n",
        "        full_text = prompt + generation\n",
        "\n",
        "        # Tokenize the full text. We do not pad here; padding will be handled\n",
        "        # dynamically by the DataCollator at the batch level.\n",
        "        tokenized_full = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=None # Return lists of IDs for now\n",
        "        )\n",
        "\n",
        "        # Tokenize the prompt separately to determine its length in tokens.\n",
        "        # This length is needed for masking the labels.\n",
        "        tokenized_prompt = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        prompt_len = len(tokenized_prompt['input_ids'])\n",
        "\n",
        "        # Create the labels for the language modeling task.\n",
        "        # The labels are a copy of the input_ids, as the model is trained\n",
        "        # to predict the next token in the sequence.\n",
        "        labels = list(tokenized_full['input_ids'])\n",
        "\n",
        "        # **Crucial Step**: Mask out the prompt portion in the labels.\n",
        "        # By setting the label tokens corresponding to the prompt to -100, we instruct\n",
        "        # the PyTorch loss function (CrossEntropyLoss) to ignore these tokens\n",
        "        # when calculating the loss. This ensures that the model is only trained\n",
        "        # to predict the `generation` part of the sequence.\n",
        "        for i in range(prompt_len):\n",
        "            labels[i] = -100\n",
        "\n",
        "        # Add the processed labels to the dictionary of tokenized outputs.\n",
        "        tokenized_full['labels'] = labels\n",
        "\n",
        "        # Convert all lists of token IDs into PyTorch tensors before returning.\n",
        "        # This is the format expected by the `Trainer` and `DataLoader`.\n",
        "        return {key: torch.tensor(value) for key, value in tokenized_full.items()}\n",
        "\n",
        "\n",
        "def _run_sft(\n",
        "    golden_samples_df: pd.DataFrame,\n",
        "    stage1_model_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Runs the Supervised Fine-Tuning process on the high-quality samples.\n",
        "\n",
        "    Args:\n",
        "        golden_samples_df: The DataFrame of filtered \"golden\" samples.\n",
        "        stage1_model_path: The path to the LoRA adapters from Stage 1.\n",
        "        config: The master configuration dictionary.\n",
        "        output_dir: The directory to save the SFT checkpoints and final model.\n",
        "    \"\"\"\n",
        "    # --- Model and Tokenizer Initialization ---\n",
        "    # Load the base model and tokenizer.\n",
        "    model_id = config['reasoning_model_config']['llm_settings']['base_model_identifier']\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'right' # Use right padding for SFT\n",
        "\n",
        "    # Load the LoRA adapters from Stage 1 onto the base model.\n",
        "    model = PeftModel.from_pretrained(base_model, stage1_model_path)\n",
        "    model.train() # Set the model to training mode.\n",
        "\n",
        "    # --- Dataset Preparation ---\n",
        "    # Split the golden samples into training and validation sets.\n",
        "    train_df = golden_samples_df.sample(frac=0.9, random_state=42)\n",
        "    val_df = golden_samples_df.drop(train_df.index)\n",
        "\n",
        "    # Create PyTorch Dataset objects.\n",
        "    train_dataset = SFTDataset(train_df['prompt'].tolist(), train_df['generation'].tolist(), tokenizer)\n",
        "    val_dataset = SFTDataset(val_df['prompt'].tolist(), val_df['generation'].tolist(), tokenizer)\n",
        "\n",
        "    # --- Trainer Configuration ---\n",
        "    sft_config = config['reasoning_model_config']['sft_training_params']\n",
        "\n",
        "    # Define the training arguments for the Hugging Face Trainer.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=sft_config['epochs'],\n",
        "        per_device_train_batch_size=sft_config['batch_size'],\n",
        "        per_device_eval_batch_size=sft_config['batch_size'],\n",
        "        learning_rate=sft_config['learning_rate'],\n",
        "        warmup_steps=50,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        load_best_model_at_end=True, # Critical for getting the best checkpoint\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        report_to=\"none\" # Disable wandb/tensorboard reporting for simplicity\n",
        "    )\n",
        "\n",
        "    # The DataCollator handles padding within each batch.\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Initialize the Trainer.\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # --- Run Training ---\n",
        "    print(\"Starting Supervised Fine-Tuning (SFT)...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the best model checkpoint.\n",
        "    trainer.save_model(f\"{output_dir}/best_checkpoint\")\n",
        "    print(f\"SFT complete. Best model saved to {output_dir}/best_checkpoint\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_reasoning_llm_stage2(\n",
        "    stage1_artifacts: Dict[str, Any],\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the Stage 2 \"Rejection Sampling + SFT\" process.\n",
        "\n",
        "    This function distills the knowledge from the noisy RL exploration by:\n",
        "    1. Filtering the experiences from Stage 1 to create a high-quality dataset.\n",
        "    2. Fine-tuning the Stage 1 model on this \"golden\" dataset using standard\n",
        "       supervised learning techniques.\n",
        "\n",
        "    Args:\n",
        "        stage1_artifacts: The dictionary of artifacts from the Stage 1 run,\n",
        "                          containing the model path and experiences DataFrame.\n",
        "        parser: An instance of the LLMOutputParser.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The file path to the best model checkpoint from the SFT process, which\n",
        "        will be the starting point for Stage 3.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Stage 2: Rejection Sampling and SFT ---\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if 'lora_model_path' not in stage1_artifacts or 'collected_experiences_df' not in stage1_artifacts:\n",
        "        raise ValueError(\"`stage1_artifacts` is missing required keys: 'lora_model_path' or 'collected_experiences_df'.\")\n",
        "\n",
        "    experiences_df = stage1_artifacts['collected_experiences_df']\n",
        "    stage1_model_path = stage1_artifacts['lora_model_path']\n",
        "\n",
        "    # --- Step 1: Filter high-quality samples ---\n",
        "    print(\"Step 1: Filtering high-quality samples via rejection sampling...\")\n",
        "    golden_samples_df, summary_report = _filter_high_quality_samples(\n",
        "        experiences_df, parser, config\n",
        "    )\n",
        "    print(f\"Filtered {len(experiences_df)} experiences down to {len(golden_samples_df)} golden samples.\")\n",
        "    print(\"Filtering summary per bucket:\")\n",
        "    print(summary_report.head())\n",
        "\n",
        "    if golden_samples_df.empty:\n",
        "        raise RuntimeError(\"Rejection sampling resulted in an empty dataset. Cannot proceed with SFT.\")\n",
        "\n",
        "    # --- Step 2: Run Supervised Fine-Tuning ---\n",
        "    print(\"\\nStep 2: Running Supervised Fine-Tuning on golden samples...\")\n",
        "    sft_output_dir = \"./models/stage2_sft\"\n",
        "    _run_sft(golden_samples_df, stage1_model_path, config, sft_output_dir)\n",
        "\n",
        "    best_checkpoint_path = f\"{sft_output_dir}/best_checkpoint\"\n",
        "\n",
        "    print(f\"\\n--- Stage 2 Training Complete. Best model is at: {best_checkpoint_path} ---\")\n",
        "\n",
        "    return best_checkpoint_path\n"
      ],
      "metadata": {
        "id": "RK4wqpUAsJjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Train Reasoning LLM Ï€Î¸ â€“ Stage 3 (RL for Reasoning with Time-GRPO)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Train Reasoning LLM Ï€Î¸ â€“ Stage 3 (RL for Reasoning with Time-GRPO)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Initialize from SFT and resume RL training\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _initialize_stage3_models(\n",
        "    sft_checkpoint_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Initializes all models for Stage 3, loading the policy from an SFT checkpoint.\n",
        "\n",
        "    This function is the entry point for Stage 3 model setup. It loads the base\n",
        "    LLM, then applies the fine-tuned LoRA adapters from the SFT stage to create\n",
        "    the initial policy model. It also re-initializes a frozen reference model\n",
        "    and a trainable value head, ensuring a clean start for the final RL phase.\n",
        "\n",
        "    Args:\n",
        "        sft_checkpoint_path: The file path to the best model checkpoint from Stage 2.\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device for model placement.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all initialized components for Stage 3 training.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the `sft_checkpoint_path` does not exist.\n",
        "        ValueError: If essential configuration keys are missing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the provided checkpoint path is valid.\n",
        "    if not os.path.isdir(sft_checkpoint_path):\n",
        "        raise FileNotFoundError(f\"SFT checkpoint path not found: {sft_checkpoint_path}\")\n",
        "\n",
        "    try:\n",
        "        # --- Load Base Model and Tokenizer ---\n",
        "        # Extract the model identifier from the configuration.\n",
        "        model_id = config['reasoning_model_config']['llm_settings']['base_model_identifier']\n",
        "        print(f\"Loading base model '{model_id}' for Stage 3 initialization...\")\n",
        "\n",
        "        # Load the base causal language model using bfloat16 for performance.\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Load the tokenizer associated with the base model.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "        # Configure tokenizer padding for autoregressive models.\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = 'left'\n",
        "\n",
        "        # --- Create Policy Model (Ï€Î¸) by loading SFT checkpoint ---\n",
        "        print(f\"Loading SFT-trained LoRA adapters from: {sft_checkpoint_path}\")\n",
        "\n",
        "        # Load the LoRA adapters from the specified path and apply them to the base model.\n",
        "        # This creates the policy model initialized with the distilled knowledge from SFT.\n",
        "        policy_model = PeftModel.from_pretrained(base_model, sft_checkpoint_path)\n",
        "\n",
        "        # Ensure the model is in training mode for the upcoming RL phase.\n",
        "        policy_model.train()\n",
        "\n",
        "        print(\"\\n--- Stage 3 Policy Model (Ï€Î¸) Trainable Parameters ---\")\n",
        "        # Print a summary of trainable vs. total parameters for explicit verification.\n",
        "        policy_model.print_trainable_parameters()\n",
        "\n",
        "        # --- Create Reference and Value Models (re-initialized) ---\n",
        "        # The reference model MUST be the original, unmodified base model.\n",
        "        # A deep copy ensures its weights are independent of the policy model's base.\n",
        "        reference_model = copy.deepcopy(base_model)\n",
        "        # Set the model to evaluation mode to disable dropout, etc.\n",
        "        reference_model.eval()\n",
        "        # Explicitly disable gradient calculations for all parameters.\n",
        "        for param in reference_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(f\"\\n--- Reference Model (Ï€ref) re-initialized and frozen. ---\")\n",
        "\n",
        "        # The value model is trained alongside the policy, so it's re-initialized for this stage.\n",
        "        value_model = ValueHead(base_model.config.hidden_size).to(device)\n",
        "        value_model.train()\n",
        "        print(f\"\\n--- Value Head re-initialized for Stage 3. ---\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Catch missing keys in the configuration dictionary.\n",
        "        raise ValueError(f\"Configuration is missing a required key for model initialization: {e}\")\n",
        "    except Exception as e:\n",
        "        # Catch other potential errors during model loading.\n",
        "        raise RuntimeError(f\"An unexpected error occurred during model initialization: {e}\")\n",
        "\n",
        "    # Return all components in a structured dictionary.\n",
        "    return {\n",
        "        'policy_model': policy_model,\n",
        "        'reference_model': reference_model,\n",
        "        'value_model': value_model,\n",
        "        'tokenizer': tokenizer\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 2 & 3: Optimize with Time-GRPO and Persist Final Policy\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_validation(\n",
        "    models: Dict[str, Any],\n",
        "    val_dataloader: DataLoader,\n",
        "    val_prompts: List[str],\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Runs the model on a validation set and computes the average reward.\n",
        "\n",
        "    Args:\n",
        "        models: The dictionary of initialized models.\n",
        "        val_dataloader: DataLoader for the validation set.\n",
        "        val_prompts: The list of all validation prompts.\n",
        "        parser: An instance of the LLMOutputParser.\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device.\n",
        "\n",
        "    Returns:\n",
        "        The average reward achieved on the validation set.\n",
        "    \"\"\"\n",
        "    # Set models to evaluation mode to disable dropout and other training-specific layers.\n",
        "    models['policy_model'].eval()\n",
        "    models['value_model'].eval()\n",
        "\n",
        "    total_val_reward = 0.0\n",
        "\n",
        "    # Disable gradient calculations for efficiency during validation.\n",
        "    with torch.no_grad():\n",
        "        # Iterate through the validation set with a progress bar.\n",
        "        for val_batch_indices, val_batch_targets in tqdm(val_dataloader, desc=\"Validation\"):\n",
        "            # Prepare validation batch data.\n",
        "            val_batch_prompts = [val_prompts[i] for i in val_batch_indices.tolist()]\n",
        "            val_batch_targets = val_batch_targets.to(device)\n",
        "\n",
        "            # For validation, we sample with G=1 for efficiency and consistency.\n",
        "            # We reuse the experience collection function, which is robust to this.\n",
        "            # A deterministic generation config could be used here for perfect reproducibility.\n",
        "            val_experience = _sample_and_collect_experiences(\n",
        "                val_batch_prompts, val_batch_targets, models, parser, config, device\n",
        "            )\n",
        "\n",
        "            # Accumulate the mean reward for the batch.\n",
        "            total_val_reward += val_experience.rewards.mean().item()\n",
        "\n",
        "    # Calculate the average reward across all validation batches.\n",
        "    avg_val_reward = total_val_reward / len(val_dataloader)\n",
        "\n",
        "    return avg_val_reward\n",
        "\n",
        "\n",
        "def train_reasoning_llm_stage3(\n",
        "    sft_checkpoint_path: str,\n",
        "    prompts: List[str],\n",
        "    y_targets: np.ndarray,\n",
        "    metadata: pd.DataFrame,\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the Stage 3 final RL fine-tuning process.\n",
        "\n",
        "    This function takes the SFT-distilled model and fine-tunes it using the\n",
        "    Time-GRPO objective. It includes a validation loop to save the best-performing\n",
        "    model checkpoint based on validation set reward.\n",
        "\n",
        "    Args:\n",
        "        sft_checkpoint_path: Path to the best model from Stage 2.\n",
        "        prompts: A list of all training prompts.\n",
        "        y_targets: A numpy array of all ground truth targets.\n",
        "        metadata: The DataFrame with metadata for each window.\n",
        "        parser: An instance of the LLMOutputParser.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The file path to the best and final reasoning policy checkpoint.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    rl_config = config['reasoning_model_config']['rl_training_params']\n",
        "    output_dir = \"./models/stage3_final_lora\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n--- Stage 3: Final RL Fine-Tuning ---\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Step 1: Initialize models from the SFT checkpoint.\n",
        "    models = _initialize_stage3_models(sft_checkpoint_path, config, device)\n",
        "\n",
        "    # Setup a new optimizer for this final training stage.\n",
        "    trainable_params = [p for p in models['policy_model'].parameters() if p.requires_grad] + \\\n",
        "                       list(models['value_model'].parameters())\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=rl_config['learning_rate'])\n",
        "\n",
        "    # --- Data Splitting for Train/Validation ---\n",
        "    # Create a combined DataFrame for easy splitting.\n",
        "    full_dataset_df = metadata.copy()\n",
        "    full_dataset_df['prompt'] = prompts\n",
        "    # This is a safe way to handle numpy arrays in pandas cells.\n",
        "    full_dataset_df['y_target'] = list(y_targets)\n",
        "\n",
        "    # Perform a simple chronological split for validation.\n",
        "    # E.g., use the last 10% of the data (by date) for validation.\n",
        "    split_date = full_dataset_df['window_end_date'].quantile(0.9, interpolation='nearest')\n",
        "    train_df = full_dataset_df[full_dataset_df['window_end_date'] < split_date]\n",
        "    val_df = full_dataset_df[full_dataset_df['window_end_date'] >= split_date]\n",
        "\n",
        "    # Prepare data for DataLoader.\n",
        "    train_prompts = train_df['prompt'].tolist()\n",
        "    train_y_targets = np.array(train_df['y_target'].tolist())\n",
        "    val_prompts = val_df['prompt'].tolist()\n",
        "    val_y_targets = np.array(val_df['y_target'].tolist())\n",
        "\n",
        "    train_dataset = TensorDataset(torch.arange(len(train_prompts)), torch.from_numpy(train_y_targets).float())\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=rl_config['batch_size'], shuffle=True)\n",
        "\n",
        "    val_dataset = TensorDataset(torch.arange(len(val_prompts)), torch.from_numpy(val_y_targets).float())\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=rl_config['batch_size'])\n",
        "\n",
        "    # --- Training and Validation Loop ---\n",
        "    best_val_reward = -float('inf')\n",
        "    best_model_path = \"\"\n",
        "\n",
        "    print(f\"Starting final fine-tuning for {rl_config['max_steps']} steps...\")\n",
        "    pbar = tqdm(total=rl_config['max_steps'], desc=\"Training Step\")\n",
        "\n",
        "    step = 0\n",
        "    training_complete = False\n",
        "    while not training_complete:\n",
        "        for batch_indices, batch_targets in train_dataloader:\n",
        "            if step >= rl_config['max_steps']:\n",
        "                training_complete = True\n",
        "                break\n",
        "\n",
        "            # Set models to training mode for this step.\n",
        "            models['policy_model'].train()\n",
        "            models['value_model'].train()\n",
        "\n",
        "            # Prepare batch data.\n",
        "            batch_prompts = [train_prompts[i] for i in batch_indices.tolist()]\n",
        "            batch_targets = batch_targets.to(device)\n",
        "\n",
        "            # Step 2 (reuse): Sample experiences using the current policy.\n",
        "            experience = _sample_and_collect_experiences(\n",
        "                batch_prompts, batch_targets, models, parser, config, device\n",
        "            )\n",
        "\n",
        "            # Step 2 (reuse): Perform multiple optimization updates.\n",
        "            for _ in range(rl_config['updates_per_batch']):\n",
        "                metrics = _compute_and_optimize_loss(experience, models, optimizer, config)\n",
        "\n",
        "            # --- Logging and Validation ---\n",
        "            if step % 10 == 0:\n",
        "                pbar.set_postfix(metrics)\n",
        "\n",
        "            # Periodic validation to save the best model.\n",
        "            if step > 0 and step % 50 == 0:\n",
        "                avg_val_reward = _run_validation(\n",
        "                    models, val_dataloader, val_prompts, parser, config, device\n",
        "                )\n",
        "                print(f\"\\nValidation at Step {step}: Avg Reward = {avg_val_reward:.4f} | Best = {best_val_reward:.4f}\")\n",
        "\n",
        "                # Step 3: Persist the best policy.\n",
        "                if avg_val_reward > best_val_reward:\n",
        "                    best_val_reward = avg_val_reward\n",
        "                    best_model_path = f\"{output_dir}/best_checkpoint\"\n",
        "                    models['policy_model'].save_pretrained(best_model_path)\n",
        "                    print(f\"New best model saved to {best_model_path}\\n\")\n",
        "\n",
        "            step += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    print(\"\\n--- Stage 3 Training Complete ---\")\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # If no best model was saved (e.g., validation never improved), save the final state.\n",
        "    if not best_model_path:\n",
        "        best_model_path = f\"{output_dir}/final_checkpoint\"\n",
        "        models['policy_model'].save_pretrained(best_model_path)\n",
        "        print(f\"Validation reward did not improve. Final model saved to {best_model_path}\")\n",
        "\n",
        "    return best_model_path\n"
      ],
      "metadata": {
        "id": "1euD-iA4w-p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Train Time-Series Forecasting Backbone Ï† (Cross-Modal Alignment)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Train Time-Series Forecasting Backbone Ï† (Cross-Modal Alignment)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Define the dual-branch architecture\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_language_prototypes(\n",
        "    model_identifier: str,\n",
        "    n_prototypes: int,\n",
        "    device: torch.device\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Extracts language prototypes from a pretrained LLM's embedding matrix via PCA.\n",
        "\n",
        "    Args:\n",
        "        model_identifier: The Hugging Face identifier for the base LLM (e.g., 'gpt2').\n",
        "        n_prototypes: The number of principal components to extract.\n",
        "        device: The torch device.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of shape (n_prototypes, embedding_dim) containing the language prototypes.\n",
        "    \"\"\"\n",
        "    print(f\"Extracting language prototypes from '{model_identifier}'...\")\n",
        "    # Load the pretrained model.\n",
        "    with torch.no_grad():\n",
        "        base_llm = AutoModel.from_pretrained(model_identifier).to(device)\n",
        "        # Get the token embedding matrix.\n",
        "        embedding_matrix = base_llm.get_input_embeddings().weight.data.cpu().numpy()\n",
        "\n",
        "    # Perform PCA to find the most significant \"concept\" vectors.\n",
        "    pca = PCA(n_components=n_prototypes)\n",
        "    prototypes = pca.fit_transform(embedding_matrix)\n",
        "\n",
        "    # Return the prototypes as a frozen tensor on the specified device.\n",
        "    return torch.tensor(prototypes, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "class ForecastingBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    The dual-branch time-series forecasting backbone (Ï†).\n",
        "\n",
        "    This model implements the \"Latent Thinking\" component of the VTA framework,\n",
        "    as described in Section 3.3 of the paper. It is a bespoke architecture\n",
        "    designed to process time-series data through two parallel transformer-based\n",
        "    branches, encouraging alignment between the raw temporal patterns and a\n",
        "    latent language space.\n",
        "\n",
        "    Architecture:\n",
        "    1.  **Input Projection**: A linear layer embeds the input time-series features\n",
        "        (e.g., 6 OHLCV features) into the model's hidden dimension (`d_model`).\n",
        "    2.  **Positional Encoding**: A learnable positional encoding is added to the\n",
        "        embedded sequence to provide temporal information.\n",
        "    3.  **Cross-Modal Alignment**: A cross-attention mechanism aligns the temporal\n",
        "        sequence with a set of pre-computed \"language prototypes\" (derived from\n",
        "        a base LLM like GPT-2 via PCA). This produces a \"textual\" representation\n",
        "        of the time-series.\n",
        "    4.  **Dual Transformer Branches**: Two parallel transformer encoders process\n",
        "        the original temporal sequence (`x_time`) and the aligned textual\n",
        "        sequence (`x_text`) independently.\n",
        "    5.  **Output Heads**: Two separate linear heads project the final hidden state\n",
        "        of each branch to produce two distinct forecasts.\n",
        "\n",
        "    The model's forward pass returns all necessary outputs for computing the\n",
        "    specialized training losses (feature regularization and output alignment).\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any], language_prototypes: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Initializes the ForecastingBackbone model.\n",
        "\n",
        "        Args:\n",
        "            config: The `forecasting_model_config` section of the master\n",
        "                    configuration dictionary.\n",
        "            language_prototypes: A pre-computed tensor of shape\n",
        "                                 (n_prototypes, embedding_dim) containing the\n",
        "                                 principal components of a base LLM's embedding\n",
        "                                 matrix.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If required parameters are missing from the configuration.\n",
        "        \"\"\"\n",
        "        # Initialize the parent torch.nn.Module.\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Configuration and Parameter Extraction ---\n",
        "        try:\n",
        "            # Extract the architecture parameters sub-dictionary.\n",
        "            arch_params = config['architecture_params']\n",
        "\n",
        "            # The model's main hidden dimension.\n",
        "            self.d_model: int = arch_params['d_model']\n",
        "\n",
        "            # The number of heads for multi-head attention mechanisms.\n",
        "            self.n_head: int = arch_params['n_heads']\n",
        "\n",
        "            # The number of future time steps to predict (T').\n",
        "            self.prediction_horizon: int = config['prediction_horizon']\n",
        "\n",
        "            # The number of features in the input time-series (e.g., 6 for OHLCVA).\n",
        "            self.input_features: int = 6\n",
        "\n",
        "            # The length of the input time-series window (T).\n",
        "            self.input_window_size: int = 10\n",
        "\n",
        "        except KeyError as e:\n",
        "            # Raise an error if the configuration is incomplete.\n",
        "            raise KeyError(f\"ForecastingBackbone config is missing required key: {e}\")\n",
        "\n",
        "        # --- 1. Input Embedding Layers ---\n",
        "        # A linear layer to project the input features into the model's hidden dimension.\n",
        "        self.input_projection = nn.Linear(self.input_features, self.d_model)\n",
        "\n",
        "        # A learnable parameter for positional encoding. This allows the model to learn\n",
        "        # the importance of the position of each time step in the input window.\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, self.input_window_size, self.d_model), requires_grad=True)\n",
        "\n",
        "        # --- 2. Cross-Modal Alignment Mechanism ---\n",
        "        # Register the language prototypes as a non-trainable buffer. This ensures they\n",
        "        # are moved to the correct device with the model but are not considered parameters.\n",
        "        self.register_buffer('language_prototypes', language_prototypes)\n",
        "\n",
        "        # A standard MultiheadAttention layer configured for cross-attention, where the\n",
        "        # query comes from the time-series and the key/value come from the prototypes.\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            embed_dim=self.d_model,\n",
        "            num_heads=self.n_head,\n",
        "            dropout=arch_params['dropout'],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # --- 3. Dual Transformer Branches ---\n",
        "        # Define a template for a single transformer encoder layer.\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_model,\n",
        "            nhead=self.n_head,\n",
        "            dim_feedforward=arch_params['d_ff'],\n",
        "            dropout=arch_params['dropout'],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Create the temporal branch as a list of transformer layers.\n",
        "        self.temporal_branch = nn.ModuleList(\n",
        "            [copy.deepcopy(encoder_layer) for _ in range(arch_params['n_layers_temporal'])]\n",
        "        )\n",
        "\n",
        "        # Create the textual branch as a separate list of transformer layers.\n",
        "        self.textual_branch = nn.ModuleList(\n",
        "            [copy.deepcopy(encoder_layer) for _ in range(arch_params['n_layers_textual'])]\n",
        "        )\n",
        "\n",
        "        # --- 4. Output Heads ---\n",
        "        # A linear layer to project the final hidden state of the temporal branch to the forecast horizon.\n",
        "        self.temporal_head = nn.Linear(self.d_model, self.prediction_horizon)\n",
        "\n",
        "        # A separate linear layer for the textual branch's forecast.\n",
        "        self.textual_head = nn.Linear(self.d_model, self.prediction_horizon)\n",
        "\n",
        "        # --- 5. Feature Regularization Projection Layers ---\n",
        "        # A list of linear layers used to project the intermediate features from each\n",
        "        # transformer layer before calculating the feature regularization loss.\n",
        "        self.feature_projections = nn.ModuleList(\n",
        "            [nn.Linear(self.d_model, self.d_model) for _ in range(len(self.temporal_branch))]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the dual-branch architecture.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, T, num_features), where T is\n",
        "               the input window size and num_features is 6.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing all outputs required for training and inference:\n",
        "            - 'y_hat_time': The forecast from the temporal branch, shape (batch_size, T').\n",
        "            - 'y_hat_text': The forecast from the textual branch, shape (batch_size, T').\n",
        "            - 'intermediate_features_time': A list of hidden state tensors from each\n",
        "              layer of the temporal branch.\n",
        "            - 'intermediate_features_text': A list of hidden state tensors from each\n",
        "              layer of the textual branch.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        if x.ndim != 3 or x.shape[2] != self.input_features or x.shape[1] != self.input_window_size:\n",
        "            raise ValueError(\n",
        "                f\"Input tensor has incorrect shape. Expected (batch, {self.input_window_size}, {self.input_features}), \"\n",
        "                f\"but got {x.shape}.\"\n",
        "            )\n",
        "\n",
        "        # --- 1. Input Projection and Positional Encoding ---\n",
        "        # Project input features to the model's hidden dimension.\n",
        "        # Shape: (batch, T, 6) -> (batch, T, d_model)\n",
        "        x_time = self.input_projection(x)\n",
        "\n",
        "        # Add the learnable positional encodings.\n",
        "        x_time = x_time + self.pos_encoder\n",
        "\n",
        "        # --- 2. Cross-Modal Alignment ---\n",
        "        # Equation 4: X_text = Softmax(QK^T/âˆšC)V\n",
        "        # Here, Query = x_time, and Key/Value = language_prototypes.\n",
        "        # The prototypes tensor needs to be expanded to match the batch size for the attention mechanism.\n",
        "        prototypes_expanded = self.language_prototypes.unsqueeze(0).expand(x.shape[0], -1, -1)\n",
        "\n",
        "        # Perform cross-attention. The output `x_text` is the aligned representation.\n",
        "        # Shape: (batch, T, d_model)\n",
        "        x_text, _ = self.cross_attention(\n",
        "            query=x_time,\n",
        "            key=prototypes_expanded,\n",
        "            value=prototypes_expanded\n",
        "        )\n",
        "\n",
        "        # --- 3. Dual-Branch Transformer Processing ---\n",
        "        # Lists to store the intermediate hidden states from each layer for the regularization loss.\n",
        "        intermediate_features_time: List[torch.Tensor] = []\n",
        "        intermediate_features_text: List[torch.Tensor] = []\n",
        "\n",
        "        # Pass the temporal sequence through its dedicated transformer branch.\n",
        "        temp_out = x_time\n",
        "        for layer in self.temporal_branch:\n",
        "            temp_out = layer(temp_out)\n",
        "            intermediate_features_time.append(temp_out)\n",
        "\n",
        "        # Pass the aligned textual sequence through its dedicated transformer branch.\n",
        "        text_out = x_text\n",
        "        for layer in self.textual_branch:\n",
        "            text_out = layer(text_out)\n",
        "            intermediate_features_text.append(text_out)\n",
        "\n",
        "        # --- 4. Output Generation ---\n",
        "        # The final representation from the last time step of the last layer is used for forecasting.\n",
        "        # Shape: (batch, d_model) -> (batch, T')\n",
        "        y_hat_time = self.temporal_head(intermediate_features_time[-1][:, -1, :])\n",
        "        y_hat_text = self.textual_head(intermediate_features_text[-1][:, -1, :])\n",
        "\n",
        "        # Return all computed tensors in a structured dictionary.\n",
        "        return {\n",
        "            'y_hat_time': y_hat_time,\n",
        "            'y_hat_text': y_hat_text,\n",
        "            'intermediate_features_time': intermediate_features_time,\n",
        "            'intermediate_features_text': intermediate_features_text\n",
        "        }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Define feature regularization and output alignment losses\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_backbone_loss(\n",
        "    model_output: Dict[str, Any],\n",
        "    model: ForecastingBackbone,\n",
        "    config: Dict[str, Any]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the combined loss for training the forecasting backbone.\n",
        "\n",
        "    Args:\n",
        "        model_output: The dictionary returned by the model's forward pass.\n",
        "        model: The ForecastingBackbone model instance (to access projection layers).\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The total scalar loss for the batch.\n",
        "    \"\"\"\n",
        "    loss_config = config['losses']\n",
        "\n",
        "    # --- Output Alignment Loss (Equation 6) ---\n",
        "    # L_output = sim(Å·_time, Å·_text)\n",
        "    # The paper uses L1 loss for similarity.\n",
        "    output_loss = F.l1_loss(model_output['y_hat_time'], model_output['y_hat_text'])\n",
        "\n",
        "    # --- Feature Regularization Loss (Equation 5) ---\n",
        "    # L_feature = Î£ Î³^(N-n) * sim(Ï†_text(F_text^n), Ï†_time(F_time^n))\n",
        "    feature_loss = 0.0\n",
        "\n",
        "    # Get the gamma schedule (decay factors).\n",
        "    gamma_schedule = loss_config['feature_regularization']['gamma_schedule']\n",
        "\n",
        "    # Iterate through the intermediate features from each layer.\n",
        "    num_layers = min(len(model_output['intermediate_features_time']), len(gamma_schedule))\n",
        "    for i in range(num_layers):\n",
        "        # Project the features from both branches.\n",
        "        proj_time = model.feature_projections[i](model_output['intermediate_features_time'][i])\n",
        "        proj_text = model.feature_projections[i](model_output['intermediate_features_text'][i])\n",
        "\n",
        "        # Calculate the L1 loss between the projected features.\n",
        "        layer_loss = F.l1_loss(proj_time, proj_text)\n",
        "\n",
        "        # Apply the scheduled weight and add to the total feature loss.\n",
        "        feature_loss += gamma_schedule[i] * layer_loss\n",
        "\n",
        "    # --- Total Loss ---\n",
        "    # L_total = L_feature + w_output * L_output\n",
        "    total_loss = feature_loss + loss_config['output_alignment']['weight'] * output_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Train the backbone and extract the unconditional forecaster\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_forecasting_backbone(\n",
        "    data_splits: Dict[str, Dict[str, np.ndarray]],\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the training of the ForecastingBackbone model.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The dictionary of train/val/test data splits.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The file path to the best trained model checkpoint.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    forecasting_config = config['forecasting_model_config']\n",
        "    train_config = forecasting_config['training_params']\n",
        "    output_dir = \"./models/forecasting_backbone\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n--- Training Forecasting Backbone (Ï†) ---\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- DataLoaders ---\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.from_numpy(data_splits['train']['X']).float(),\n",
        "        torch.from_numpy(data_splits['train']['y']).float() # Ground truth is needed for final model\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
        "\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.from_numpy(data_splits['val']['X']).float(),\n",
        "        torch.from_numpy(data_splits['val']['y']).float()\n",
        "    )\n",
        "    val_loader = DataLoader(val_dataset, batch_size=train_config['batch_size'])\n",
        "\n",
        "    # --- Model and Optimizer ---\n",
        "    # Step 1a: Get language prototypes.\n",
        "    prototypes = _get_language_prototypes(\n",
        "        forecasting_config['architecture_params']['base_model_identifier'],\n",
        "        forecasting_config['architecture_params']['pca_n_prototypes_D'],\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Step 1b: Instantiate the model.\n",
        "    model = ForecastingBackbone(forecasting_config, prototypes).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=train_config['learning_rate'])\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(train_config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{train_config['epochs']}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for x_batch, _ in tqdm(train_loader, desc=\"Training\"):\n",
        "            x_batch = x_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model_output = model(x_batch)\n",
        "            loss = _compute_backbone_loss(model_output, model, forecasting_config)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, _ in tqdm(val_loader, desc=\"Validation\"):\n",
        "                x_batch = x_batch.to(device)\n",
        "                model_output = model(x_batch)\n",
        "                loss = _compute_backbone_loss(model_output, model, forecasting_config)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Save the best model checkpoint.\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_path = f\"{output_dir}/best_checkpoint.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved to {best_model_path}\")\n",
        "\n",
        "    print(\"\\n--- Backbone Training Complete ---\")\n",
        "    return best_model_path\n"
      ],
      "metadata": {
        "id": "Do1Gg3g8zDyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Train Joint Conditional Forecaster Ïˆ (Classifier-Free Conditioning)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Train Joint Conditional Forecaster Ïˆ (Classifier-Free Conditioning)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Derive conditioning attributes c from the reasoning forecast\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _derive_conditioning_attributes(\n",
        "    prompts: List[str],\n",
        "    reasoning_model_path: str,\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates conditioning attributes `c` using the final trained reasoning model.\n",
        "\n",
        "    This function loads the fully trained reasoning policy (Ï€Î¸), generates a\n",
        "    forecast for each input prompt, parses the output, and extracts summary\n",
        "    statistics ([min, mean, max]) from each valid forecast. These statistics\n",
        "    serve as the conditioning signal for the fusion model.\n",
        "\n",
        "    Args:\n",
        "        prompts: The list of prompts for the entire dataset (train + val).\n",
        "        reasoning_model_path: Path to the trained Stage 3 reasoning model adapters.\n",
        "        parser: An instance of the LLMOutputParser.\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A numpy array of shape (num_samples, 3) with the [min, mean, max] attributes.\n",
        "        - A boolean numpy array of shape (num_samples,) indicating which samples\n",
        "          were successfully processed and yielded a valid forecast.\n",
        "    \"\"\"\n",
        "    print(\"--- Deriving Conditioning Attributes `c` ---\")\n",
        "\n",
        "    # --- Load Trained Reasoning Model (Ï€Î¸) ---\n",
        "    try:\n",
        "        # Load the base model architecture.\n",
        "        model_id = config['reasoning_model_config']['llm_settings']['base_model_identifier']\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, torch_dtype=torch.bfloat16, trust_remote_code=True\n",
        "        )\n",
        "        # Apply the trained LoRA adapters from the specified path.\n",
        "        model = PeftModel.from_pretrained(base_model, reasoning_model_path).to(device)\n",
        "        model.eval() # Set model to evaluation mode.\n",
        "\n",
        "        # Load the corresponding tokenizer.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = 'left'\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Failed to load reasoning model from path '{reasoning_model_path}'. Error: {e}\")\n",
        "\n",
        "    # --- Generate Forecasts and Extract Attributes in Batches ---\n",
        "    attributes = []\n",
        "    valid_mask = []\n",
        "    # Use a reasonable batch size for inference.\n",
        "    batch_size = config['conditional_fusion_config']['training_params']['batch_size']\n",
        "\n",
        "    # Process all prompts in batches for efficiency.\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating Attributes\"):\n",
        "        # Slice the current batch of prompts.\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize the batch.\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate outputs using a deterministic strategy (greedy search).\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "\n",
        "        # Decode the generated token sequences back to text.\n",
        "        decoded_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        # Parse each output in the batch.\n",
        "        for text in decoded_texts:\n",
        "            parsed = parser.parse(text)\n",
        "            if parsed.is_valid:\n",
        "                # Equation: c = g(Å·_Î¸), where g computes [min, mean, max]\n",
        "                pred = parsed.prediction\n",
        "                attributes.append([np.min(pred), np.mean(pred), np.max(pred)])\n",
        "                valid_mask.append(True)\n",
        "            else:\n",
        "                # If parsing fails, append a placeholder and mark as invalid.\n",
        "                attributes.append([0.0, 0.0, 0.0])\n",
        "                valid_mask.append(False)\n",
        "\n",
        "    print(f\"Successfully derived attributes for {sum(valid_mask)}/{len(prompts)} samples.\")\n",
        "\n",
        "    # Convert lists to numpy arrays for efficient indexing.\n",
        "    return np.array(attributes, dtype=np.float32), np.array(valid_mask, dtype=bool)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Define the Ïˆ architecture with classifier-free training\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class ConditionalFusionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The joint conditional forecaster (Ïˆ), implementing classifier-free guidance.\n",
        "\n",
        "    This model fuses the unconditional forecast from the backbone (Ï†) with\n",
        "    the conditioning attributes (c) from the reasoning model (Ï€Î¸). It is\n",
        "    trained with random dropping of the conditioning signal, which allows it\n",
        "    to perform guided inference by interpolating between its conditional and\n",
        "    unconditional predictions, as described in Section 3.4 of the paper.\n",
        "\n",
        "    Architecture:\n",
        "    1.  **Backbone (Ï†)**: Contains a pre-trained `ForecastingBackbone` which\n",
        "        generates an unconditional forecast `Å·_Ï†(X)`.\n",
        "    2.  **Attribute Encoder**: A small Multi-Layer Perceptron (MLP) that encodes\n",
        "        the conditioning vector `c` into a higher-dimensional representation.\n",
        "    3.  **Projection Head**: A final MLP that takes the concatenation of `Å·_Ï†(X)`\n",
        "        and the encoded `c` as input and learns to predict an `adjustment`.\n",
        "    4.  **Residual Connection**: The final output is the sum of the unconditional\n",
        "        forecast and the learned adjustment, i.e., `Å·_Ïˆ = Å·_Ï† + adjustment`.\n",
        "        This structure is key to the guided inference mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone: ForecastingBackbone, config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the ConditionalFusionModel.\n",
        "\n",
        "        Args:\n",
        "            backbone: An instance of the pre-trained `ForecastingBackbone` (Ï†).\n",
        "            config: The `conditional_fusion_config` section of the master\n",
        "                    configuration dictionary.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If required parameters are missing from the configuration.\n",
        "        \"\"\"\n",
        "        # Initialize the parent torch.nn.Module.\n",
        "        super().__init__()\n",
        "\n",
        "        # Store the backbone model as a submodule.\n",
        "        self.backbone = backbone\n",
        "\n",
        "        try:\n",
        "            # Extract the architecture configuration for the fusion model.\n",
        "            psi_config = config['psi_architecture']\n",
        "\n",
        "            # The number of conditioning attributes (e.g., 3 for min, mean, max).\n",
        "            num_attributes = len(config['attributes_c_definition']['components'])\n",
        "\n",
        "            # An MLP to encode the conditioning attributes.\n",
        "            self.attribute_encoder = nn.Sequential(\n",
        "                nn.Linear(num_attributes, psi_config['per_attribute_linear_out_dim']),\n",
        "                nn.GELU(), # Using GELU as a modern activation function.\n",
        "                nn.Linear(psi_config['per_attribute_linear_out_dim'], psi_config['per_attribute_linear_out_dim'])\n",
        "            )\n",
        "\n",
        "            # The final projection head that learns the adjustment to the unconditional forecast.\n",
        "            # Its input size is the sum of the forecast horizon and the encoded attribute dimension.\n",
        "            self.projection_head = nn.Sequential(\n",
        "                nn.Linear(backbone.prediction_horizon + psi_config['per_attribute_linear_out_dim'], psi_config['projection_mlp_width']),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(psi_config.get('dropout', 0.1)),\n",
        "                nn.Linear(psi_config['projection_mlp_width'], backbone.prediction_horizon)\n",
        "            )\n",
        "\n",
        "            # Handle the freezing of the backbone model's weights based on the config.\n",
        "            if config['training_params']['freeze_backbone_phi']:\n",
        "                print(\"Freezing forecasting backbone (Ï†) weights.\")\n",
        "                # Iterate through all parameters of the backbone and disable gradients.\n",
        "                for param in self.backbone.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        except KeyError as e:\n",
        "            # Raise a specific error if the configuration is incomplete.\n",
        "            raise KeyError(f\"ConditionalFusionModel config is missing required key: {e}\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        c: torch.Tensor,\n",
        "        p_uncond: float = 0.0,\n",
        "        force_unconditional: bool = False\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass with classifier-free guidance dropping.\n",
        "\n",
        "        This method can operate in three modes:\n",
        "        1.  **Training Mode**: Randomly replaces `c` with a null token (zeros)\n",
        "            with probability `p_uncond`.\n",
        "        2.  **Conditional Inference**: Uses the provided `c` as is.\n",
        "        3.  **Unconditional Inference**: If `force_unconditional` is True,\n",
        "            replaces `c` with the null token.\n",
        "\n",
        "        Args:\n",
        "            x: Input time-series tensor of shape (batch_size, T, 6).\n",
        "            c: Conditioning attribute tensor of shape (batch_size, num_attributes).\n",
        "            p_uncond: The probability of dropping the conditioning signal during training.\n",
        "            force_unconditional: If True, forces the model to run in unconditional mode.\n",
        "\n",
        "        Returns:\n",
        "            The final forecast tensor of shape (batch_size, T').\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        if x.ndim != 3 or c.ndim != 2 or x.shape[0] != c.shape[0]:\n",
        "            raise ValueError(\n",
        "                f\"Shape mismatch in inputs. Got x: {x.shape}, c: {c.shape}. \"\n",
        "                \"Batch sizes must match and dimensions must be correct.\"\n",
        "            )\n",
        "\n",
        "        # --- 1. Get Unconditional Forecast Å·_Ï†(X) ---\n",
        "        # The backbone is run with gradients disabled if it's frozen.\n",
        "        # `torch.set_grad_enabled` is a context manager for this.\n",
        "        is_backbone_trainable = any(p.requires_grad for p in self.backbone.parameters())\n",
        "        with torch.set_grad_enabled(is_backbone_trainable and self.training):\n",
        "            backbone_output = self.backbone(x)\n",
        "\n",
        "        # As per the paper, the unconditional forecast is the output of the temporal branch.\n",
        "        y_hat_phi = backbone_output['y_hat_time']\n",
        "\n",
        "        # --- 2. Implement Classifier-Free Guidance Dropping for `c` ---\n",
        "        if force_unconditional:\n",
        "            # For guided inference's unconditional pass, force `c` to be the null token (zeros).\n",
        "            c = torch.zeros_like(c)\n",
        "        elif self.training and p_uncond > 0:\n",
        "            # During training, create a random mask to drop conditioning for a subset of the batch.\n",
        "            # This teaches the model to function even when `c` is absent.\n",
        "            uncond_mask = torch.rand(c.shape[0], device=c.device) < p_uncond\n",
        "            c[uncond_mask] = 0.0\n",
        "\n",
        "        # --- 3. Encode Attributes and Fuse with Unconditional Forecast ---\n",
        "        # Pass the (potentially dropped) conditioning vector through its encoder.\n",
        "        encoded_c = self.attribute_encoder(c)\n",
        "\n",
        "        # Concatenate the unconditional forecast and the encoded attributes.\n",
        "        combined_features = torch.cat([y_hat_phi, encoded_c], dim=1)\n",
        "\n",
        "        # --- 4. Generate Final Forecast using a Residual Connection ---\n",
        "        # The projection head learns the *adjustment* based on the conditioning signal.\n",
        "        adjustment = self.projection_head(combined_features)\n",
        "\n",
        "        # The final output is the sum of the unconditional base and the learned adjustment.\n",
        "        # This structure directly mirrors the guided inference equation, making training stable.\n",
        "        y_hat_psi = y_hat_phi + adjustment\n",
        "\n",
        "        return y_hat_psi\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Train Ïˆ with MSE loss and define guided inference\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_guided_validation(\n",
        "    model: ConditionalFusionModel,\n",
        "    val_loader: DataLoader,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Runs validation using the full guided inference procedure.\n",
        "\n",
        "    This function evaluates the model's performance as it would be used in\n",
        "    production, by generating both a conditional and an unconditional forecast\n",
        "    and blending them using the guidance scale.\n",
        "\n",
        "    Args:\n",
        "        model: The `ConditionalFusionModel` instance.\n",
        "        val_loader: DataLoader for the validation set.\n",
        "        config: The `conditional_fusion_config` section of the master config.\n",
        "        device: The torch device.\n",
        "\n",
        "    Returns:\n",
        "        The average Mean Squared Error loss on the validation set.\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode to disable dropout and other training behaviors.\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize total validation loss.\n",
        "    total_val_loss = 0.0\n",
        "\n",
        "    # Extract the guidance scale `s` from the configuration.\n",
        "    guidance_scale = config['inference_params']['guidance_scale']\n",
        "\n",
        "    # Disable gradient calculations for efficiency during validation.\n",
        "    with torch.no_grad():\n",
        "        # Iterate through the validation set.\n",
        "        for x_batch, c_batch, y_batch in val_loader:\n",
        "            # Move data to the specified device.\n",
        "            x_batch, c_batch, y_batch = x_batch.to(device), c_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # --- Guided Inference (Equation 9) ---\n",
        "            # 1. Get the conditional forecast by passing the true attributes `c`.\n",
        "            y_hat_cond = model(x_batch, c_batch, force_unconditional=False)\n",
        "\n",
        "            # 2. Get the unconditional forecast by forcing the null attribute token `Ã¸`.\n",
        "            y_hat_uncond = model(x_batch, c_batch, force_unconditional=True)\n",
        "\n",
        "            # 3. Combine them using the guidance scale `s`.\n",
        "            # Equation: Å· = Å·_uncond + s * (Å·_cond - Å·_uncond)\n",
        "            y_hat_final = y_hat_uncond + guidance_scale * (y_hat_cond - y_hat_uncond)\n",
        "\n",
        "            # Calculate the MSE loss between the final, guided prediction and the ground truth.\n",
        "            loss = F.mse_loss(y_hat_final, y_batch)\n",
        "\n",
        "            # Accumulate the loss for the batch.\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across all validation batches.\n",
        "    return total_val_loss / len(val_loader)\n",
        "\n",
        "\n",
        "def train_conditional_forecaster(\n",
        "    data_splits: Dict[str, Dict[str, Any]],\n",
        "    prompts: List[str],\n",
        "    reasoning_model_path: str,\n",
        "    backbone_path: str,\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the training of the ConditionalFusionModel (Ïˆ).\n",
        "\n",
        "    This function manages the entire end-to-end workflow for Task 10:\n",
        "    1.  Derives the conditioning attributes `c` for the dataset using the\n",
        "        final reasoning model.\n",
        "    2.  Filters the dataset to include only samples with valid attributes.\n",
        "    3.  Initializes the `ConditionalFusionModel`, loading the pre-trained\n",
        "        backbone `Ï†`.\n",
        "    4.  Runs a training loop using MSE loss and classifier-free dropping.\n",
        "    5.  Periodically evaluates the model using a full guided inference validation\n",
        "        loop to find and save the best-performing checkpoint.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The dictionary of train/val/test data splits.\n",
        "        prompts: A list of all prompts corresponding to the data splits.\n",
        "        reasoning_model_path: Path to the best model checkpoint from Stage 3 (Ï€Î¸).\n",
        "        backbone_path: Path to the best model checkpoint from Task 9 (Ï†).\n",
        "        parser: An instance of the `LLMOutputParser`.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The file path to the best trained `ConditionalFusionModel` checkpoint.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Determine the execution device.\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Extract relevant configuration sections.\n",
        "    fusion_config = config['conditional_fusion_config']\n",
        "    train_config = fusion_config['training_params']\n",
        "\n",
        "    # Define and create the output directory for model checkpoints.\n",
        "    output_dir = \"./models/conditional_fusion_model\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n--- Training Conditional Fusion Model (Ïˆ) ---\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- Step 1: Derive Conditioning Attributes ---\n",
        "    # This is a potentially long-running preprocessing step.\n",
        "    attributes, valid_mask = _derive_conditioning_attributes(\n",
        "        prompts, reasoning_model_path, parser, config, device\n",
        "    )\n",
        "\n",
        "    # --- DataLoaders (filtered for samples with valid attributes) ---\n",
        "    # It is critical to filter all data splits consistently using the valid_mask.\n",
        "    # We get the original indices from the metadata and then filter by the mask.\n",
        "    train_original_indices = data_splits['train']['meta'].index\n",
        "    valid_train_mask = valid_mask[train_original_indices]\n",
        "\n",
        "    val_original_indices = data_splits['val']['meta'].index\n",
        "    valid_val_mask = valid_mask[val_original_indices]\n",
        "\n",
        "    # Create the training dataset using only the valid samples.\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.from_numpy(data_splits['train']['X'][valid_train_mask]).float(),\n",
        "        torch.from_numpy(attributes[train_original_indices][valid_train_mask]).float(),\n",
        "        torch.from_numpy(data_splits['train']['y'][valid_train_mask]).float()\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
        "\n",
        "    # Create the validation dataset using only the valid samples.\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.from_numpy(data_splits['val']['X'][valid_val_mask]).float(),\n",
        "        torch.from_numpy(attributes[val_original_indices][valid_val_mask]).float(),\n",
        "        torch.from_numpy(data_splits['val']['y'][valid_val_mask]).float()\n",
        "    )\n",
        "    val_loader = DataLoader(val_dataset, batch_size=train_config['batch_size'])\n",
        "\n",
        "    # --- Model and Optimizer ---\n",
        "    # Load the pretrained backbone Ï†.\n",
        "    backbone_config = config['forecasting_model_config']\n",
        "    prototypes = _get_language_prototypes(\n",
        "        backbone_config['architecture_params']['base_model_identifier'],\n",
        "        backbone_config['architecture_params']['pca_n_prototypes_D'],\n",
        "        device\n",
        "    )\n",
        "    backbone = ForecastingBackbone(backbone_config, prototypes)\n",
        "    backbone.load_state_dict(torch.load(backbone_path, map_location=device))\n",
        "\n",
        "    # Instantiate the fusion model Ïˆ, passing the backbone and config.\n",
        "    model = ConditionalFusionModel(backbone, fusion_config).to(device)\n",
        "\n",
        "    # Create an optimizer that only targets the trainable parameters.\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=train_config['learning_rate']\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = \"\"\n",
        "    p_uncond = train_config['unconditional_probability']\n",
        "\n",
        "    # Loop for the specified number of epochs.\n",
        "    for epoch in range(train_config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{train_config['epochs']}\")\n",
        "        # Set the model to training mode.\n",
        "        model.train()\n",
        "\n",
        "        # Use tqdm for a progress bar over the training batches.\n",
        "        for x_batch, c_batch, y_batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            # Move batch data to the device.\n",
        "            x_batch, c_batch, y_batch = x_batch.to(device), c_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Reset gradients.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Equation 7: L_forecast = E[||Å·_Ïˆ(X, Ä) - y||^2]\n",
        "            # The model's forward pass handles the random dropping of `c` to create `Ä`.\n",
        "            y_hat = model(x_batch, c_batch, p_uncond=p_uncond)\n",
        "\n",
        "            # Calculate the Mean Squared Error loss.\n",
        "            loss = F.mse_loss(y_hat, y_batch)\n",
        "\n",
        "            # Perform backpropagation.\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model weights.\n",
        "            optimizer.step()\n",
        "\n",
        "        # --- Validation ---\n",
        "        # Run the guided validation procedure at the end of each epoch.\n",
        "        avg_val_loss = _run_guided_validation(model, val_loader, fusion_config, device)\n",
        "        print(f\"Avg Guided Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Save the best model checkpoint based on validation performance.\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_path = f\"{output_dir}/best_checkpoint.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved to {best_model_path}\")\n",
        "\n",
        "    print(f\"\\n--- Conditional Fusion Model Training Complete ---\")\n",
        "\n",
        "    # If no best model was saved (e.g., validation never improved), save the final state.\n",
        "    if not best_model_path:\n",
        "        best_model_path = f\"{output_dir}/final_checkpoint.pth\"\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Validation did not improve. Final model saved to {best_model_path}\")\n",
        "\n",
        "    return best_model_path\n"
      ],
      "metadata": {
        "id": "XnqNOAvgETB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Perform End-to-End Inference (Forecast + Narrative)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Perform End-to-End Inference (Forecast + Narrative)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Load all trained models for inference\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class InferenceModels(NamedTuple):\n",
        "    \"\"\"\n",
        "    A structured data container for all trained models required for inference.\n",
        "\n",
        "    This `NamedTuple` provides a standardized, immutable, and type-safe object\n",
        "    for holding the fully trained and configured models needed for the end-to-end\n",
        "    inference pipeline. Encapsulating these components together simplifies passing\n",
        "    them between functions and improves code readability and maintainability.\n",
        "\n",
        "    Attributes:\n",
        "        reasoning_model: The fully trained reasoning policy model (Ï€Î¸), composed\n",
        "                         of the base LLM with the final Stage 3 LoRA adapters\n",
        "                         applied. This model is responsible for generating the\n",
        "                         textual narrative and the initial forecast used to\n",
        "                         derive conditioning attributes. It is an instance of\n",
        "                         `peft.PeftModel`.\n",
        "        fusion_model: The fully trained conditional fusion model (Ïˆ), which\n",
        "                      internally contains the trained forecasting backbone (Ï†).\n",
        "                      This model is responsible for taking the time-series data\n",
        "                      and conditioning attributes to produce the final guided\n",
        "                      numerical forecast. It is an instance of\n",
        "                      `ConditionalFusionModel`.\n",
        "        tokenizer: The tokenizer associated with the base LLM, required for both\n",
        "                   encoding prompts for the `reasoning_model` and decoding its\n",
        "                   generated outputs. It is an instance of a Hugging Face\n",
        "                   `PreTrainedTokenizer`.\n",
        "    \"\"\"\n",
        "    reasoning_model: PeftModel\n",
        "    fusion_model: \"ConditionalFusionModel\" # Use string forward reference\n",
        "    tokenizer: AutoTokenizer\n",
        "\n",
        "def _load_inference_models(\n",
        "    reasoning_model_path: str,\n",
        "    conditional_model_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> InferenceModels:\n",
        "    \"\"\"\n",
        "    Loads all trained model artifacts required for end-to-end inference.\n",
        "\n",
        "    Args:\n",
        "        reasoning_model_path: Path to the trained Stage 3 reasoning model adapters (Ï€Î¸).\n",
        "        conditional_model_path: Path to the trained ConditionalFusionModel state dict (Ïˆ).\n",
        "        config: The master configuration dictionary.\n",
        "        device: The torch device for model placement.\n",
        "\n",
        "    Returns:\n",
        "        An `InferenceModels` named tuple containing the loaded and configured models.\n",
        "    \"\"\"\n",
        "    print(\"--- Loading All Models for Inference ---\")\n",
        "\n",
        "    # --- 1. Load Reasoning Model (Ï€Î¸) ---\n",
        "    try:\n",
        "        model_id = config['reasoning_model_config']['llm_settings']['base_model_identifier']\n",
        "        print(f\"Loading reasoning model: base '{model_id}' + LoRA '{reasoning_model_path}'\")\n",
        "        base_model_pi = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, torch_dtype=torch.bfloat16, trust_remote_code=True\n",
        "        )\n",
        "        reasoning_model = PeftModel.from_pretrained(base_model_pi, reasoning_model_path).to(device)\n",
        "        reasoning_model.eval()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = 'left'\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Failed to load reasoning model. Error: {e}\")\n",
        "\n",
        "    # --- 2. Load Conditional Fusion Model (Ïˆ) ---\n",
        "    try:\n",
        "        print(f\"Loading conditional fusion model from: {conditional_model_path}\")\n",
        "        # First, instantiate the architecture, including the backbone Ï†.\n",
        "        backbone_config = config['forecasting_model_config']\n",
        "        prototypes = _get_language_prototypes(\n",
        "            backbone_config['architecture_params']['base_model_identifier'],\n",
        "            backbone_config['architecture_params']['pca_n_prototypes_D'],\n",
        "            device\n",
        "        )\n",
        "        backbone = ForecastingBackbone(backbone_config, prototypes)\n",
        "\n",
        "        # Instantiate the parent fusion model.\n",
        "        fusion_config = config['conditional_fusion_config']\n",
        "        fusion_model = ConditionalFusionModel(backbone, fusion_config).to(device)\n",
        "\n",
        "        # Load the trained state dictionary.\n",
        "        fusion_model.load_state_dict(torch.load(conditional_model_path, map_location=device))\n",
        "        fusion_model.eval()\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Failed to load conditional fusion model. Error: {e}\")\n",
        "\n",
        "    print(\"All models loaded successfully.\")\n",
        "    return InferenceModels(reasoning_model, fusion_model, tokenizer)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 2 & 3: Generate, Blend, and Validate Outputs\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_end_to_end_inference(\n",
        "    data_splits: Dict[str, Dict[str, Any]],\n",
        "    prompts: List[str],\n",
        "    reasoning_model_path: str,\n",
        "    conditional_model_path: str,\n",
        "    parser: LLMOutputParser,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full end-to-end inference pipeline for the VTA framework.\n",
        "\n",
        "    This function produces the final dual output (guided forecast and narrative)\n",
        "    for the test set by coordinating all trained model components.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The dictionary of data splits, requiring the 'test' key.\n",
        "        prompts: A list of all prompts for the entire dataset.\n",
        "        reasoning_model_path: Path to the final trained reasoning model (Ï€Î¸).\n",
        "        conditional_model_path: Path to the final trained fusion model (Ïˆ).\n",
        "        parser: An instance of the LLMOutputParser.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the inference results, with columns for\n",
        "        metadata, the final numerical forecast, and the textual narrative.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\n--- Running End-to-End Inference on device '{device}' ---\")\n",
        "\n",
        "    # --- Step 1: Load all models ---\n",
        "    models = _load_inference_models(reasoning_model_path, conditional_model_path, config, device)\n",
        "\n",
        "    # --- Prepare Test Data ---\n",
        "    test_meta = data_splits['test']['meta']\n",
        "    test_indices = test_meta.index\n",
        "    test_prompts = [prompts[i] for i in test_indices]\n",
        "    test_X = data_splits['test']['X']\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.arange(len(test_prompts)), # Use indices to map back to prompts\n",
        "        torch.from_numpy(test_X).float()\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['conditional_fusion_config']['training_params']['batch_size'])\n",
        "\n",
        "    # --- Inference Loop ---\n",
        "    results = []\n",
        "    guidance_scale = config['conditional_fusion_config']['inference_params']['guidance_scale']\n",
        "    num_successful_generations = 0\n",
        "\n",
        "    # Disable all gradient calculations for maximum performance.\n",
        "    with torch.no_grad():\n",
        "        for batch_prompt_indices, x_batch in tqdm(test_loader, desc=\"Inference\"):\n",
        "            x_batch = x_batch.to(device)\n",
        "            batch_prompts = [test_prompts[i] for i in batch_prompt_indices.tolist()]\n",
        "\n",
        "            # --- Part A: Generate Narrative and Attributes with Reasoning Model (Ï€Î¸) ---\n",
        "            inputs = models.tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            gen_outputs = models.reasoning_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "            decoded_texts = models.tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
        "\n",
        "            # Parse outputs and derive conditioning attributes `c`.\n",
        "            c_batch_list = []\n",
        "            narratives = []\n",
        "            is_valid_mask = []\n",
        "            for text in decoded_texts:\n",
        "                parsed = parser.parse(text)\n",
        "                if parsed.is_valid:\n",
        "                    pred = parsed.prediction\n",
        "                    c_batch_list.append([np.min(pred), np.mean(pred), np.max(pred)])\n",
        "                    narratives.append(parsed.reasoning)\n",
        "                    is_valid_mask.append(True)\n",
        "                    num_successful_generations += 1\n",
        "                else:\n",
        "                    # Handle failed generation: use null attributes and store error message.\n",
        "                    c_batch_list.append([0.0, 0.0, 0.0])\n",
        "                    narratives.append(f\"ERROR: {parsed.error_message}\")\n",
        "                    is_valid_mask.append(False)\n",
        "\n",
        "            c_batch = torch.tensor(c_batch_list, dtype=torch.float32).to(device)\n",
        "\n",
        "            # --- Part B: Perform Guided Inference with Fusion Model (Ïˆ) ---\n",
        "            # 1. Get the conditional forecast.\n",
        "            y_hat_cond = models.fusion_model(x_batch, c_batch, force_unconditional=False)\n",
        "\n",
        "            # 2. Get the unconditional forecast.\n",
        "            y_hat_uncond = models.fusion_model(x_batch, c_batch, force_unconditional=True)\n",
        "\n",
        "            # 3. Blend them using the guidance scale `s`.\n",
        "            # Equation 9: Å· = Å·_uncond + s * (Å·_cond - Å·_uncond)\n",
        "            final_forecasts = y_hat_uncond + guidance_scale * (y_hat_cond - y_hat_uncond)\n",
        "\n",
        "            # Fallback for failed generations: use the unconditional forecast.\n",
        "            is_valid_mask_tensor = torch.tensor(is_valid_mask, device=device).unsqueeze(1)\n",
        "            final_forecasts = torch.where(is_valid_mask_tensor, final_forecasts, y_hat_uncond)\n",
        "\n",
        "            # --- Store Results ---\n",
        "            # Move results to CPU and convert to numpy/python types for storage.\n",
        "            final_forecasts_cpu = final_forecasts.cpu().numpy()\n",
        "\n",
        "            for i in range(len(batch_prompts)):\n",
        "                original_index = test_indices[batch_prompt_indices[i].item()]\n",
        "                meta_row = data_splits['test']['meta'].loc[original_index]\n",
        "                results.append({\n",
        "                    'ticker': meta_row['ticker'],\n",
        "                    'window_end_date': meta_row['window_end_date'],\n",
        "                    'final_forecast': final_forecasts_cpu[i],\n",
        "                    'narrative': narratives[i]\n",
        "                })\n",
        "\n",
        "    # --- Finalization and Reporting ---\n",
        "    # Step 3: Validate output format and log anomalies.\n",
        "    total_samples = len(test_prompts)\n",
        "    success_rate = (num_successful_generations / total_samples) * 100 if total_samples > 0 else 0\n",
        "    print(f\"\\n--- Inference Complete ---\")\n",
        "    print(f\"Reasoning Generation Success Rate: {num_successful_generations}/{total_samples} ({success_rate:.2f}%)\")\n",
        "\n",
        "    # Convert the list of result dictionaries into a final DataFrame.\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "-9oE8TDuHO5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Implement Two Baselines (DLinear and TSMixer)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Implement Two Baselines (DLinear and TSMixer)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Generic Baseline Training Framework\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _train_and_evaluate_baseline(\n",
        "    model: nn.Module,\n",
        "    model_name: str,\n",
        "    data_splits: Dict[str, Dict[str, np.ndarray]],\n",
        "    config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    A generic, rigorous training and evaluation framework for baseline models.\n",
        "\n",
        "    This function provides a standardized harness to ensure that all baseline\n",
        "    models are trained and evaluated under identical, reproducible conditions.\n",
        "\n",
        "    Process:\n",
        "    1.  Initializes DataLoaders for training, validation, and testing.\n",
        "    2.  Initializes the AdamW optimizer.\n",
        "    3.  Runs a training loop for a specified number of epochs.\n",
        "    4.  After each epoch, it evaluates the model on the validation set.\n",
        "    5.  If the validation loss improves, it saves a checkpoint of the model's state.\n",
        "    6.  After training, it loads the best checkpoint and performs a final,\n",
        "        definitive evaluation on the held-out test set.\n",
        "\n",
        "    Args:\n",
        "        model: An instantiated PyTorch nn.Module for the baseline.\n",
        "        model_name: The name of the model (e.g., 'DLinear') for logging and saving.\n",
        "        data_splits: The dictionary of train/val/test data splits.\n",
        "        config: A dictionary of training hyperparameters (epochs, lr, batch_size, etc.).\n",
        "        device: The torch device on which to run training and evaluation.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the final test set performance metrics: {'MSE': ..., 'MAE': ...}.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Training and Evaluating Baseline: {model_name} ---\")\n",
        "\n",
        "    # --- 1. Setup DataLoaders ---\n",
        "    # Create TensorDatasets from the numpy arrays.\n",
        "    train_dataset = TensorDataset(torch.from_numpy(data_splits['train']['X']).float(), torch.from_numpy(data_splits['train']['y']).float())\n",
        "    val_dataset = TensorDataset(torch.from_numpy(data_splits['val']['X']).float(), torch.from_numpy(data_splits['val']['y']).float())\n",
        "    test_dataset = TensorDataset(torch.from_numpy(data_splits['test']['X']).float(), torch.from_numpy(data_splits['test']['y']).float())\n",
        "\n",
        "    # Create DataLoader instances for batching and shuffling.\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
        "\n",
        "    # --- 2. Model and Optimizer Setup ---\n",
        "    # Move the model to the specified device.\n",
        "    model.to(device)\n",
        "    # Initialize the AdamW optimizer with the specified learning rate.\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "    # --- 3. Training and Validation Loop ---\n",
        "    # Initialize tracking variables for the best model checkpoint.\n",
        "    best_val_loss = float('inf')\n",
        "    output_dir = f\"./models/baselines/{model_name}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    best_model_path = f\"{output_dir}/best_checkpoint.pth\"\n",
        "\n",
        "    # Loop for the specified number of epochs.\n",
        "    for epoch in range(config['epochs']):\n",
        "        # --- Training Phase ---\n",
        "        model.train()\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\")\n",
        "        for x_batch, y_batch in train_pbar:\n",
        "            # Move batch to device.\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            # Reset gradients.\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass.\n",
        "            y_hat = model(x_batch)\n",
        "            # Compute loss.\n",
        "            loss = F.mse_loss(y_hat, y_batch)\n",
        "            # Backward pass.\n",
        "            loss.backward()\n",
        "            # Update weights.\n",
        "            optimizer.step()\n",
        "            train_pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        # Disable gradient computation for validation.\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                y_hat = model(x_batch)\n",
        "                loss = F.mse_loss(y_hat, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Calculate average validation loss for the epoch.\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{config['epochs']} | Avg Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Save the model if it has the best validation loss so far.\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Validation loss improved. New best model saved to {best_model_path}\")\n",
        "\n",
        "    # --- 4. Final Evaluation on Test Set ---\n",
        "    print(f\"\\nEvaluating best {model_name} model on the test set...\")\n",
        "    # Load the state of the best performing model.\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    test_mse = 0.0\n",
        "    test_mae = 0.0\n",
        "    # Disable gradient computation for final evaluation.\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            y_hat = model(x_batch)\n",
        "            # Accumulate MSE and MAE.\n",
        "            test_mse += F.mse_loss(y_hat, y_batch).item()\n",
        "            test_mae += F.l1_loss(y_hat, y_batch).item()\n",
        "\n",
        "    # Calculate average metrics over the test set.\n",
        "    avg_test_mse = test_mse / len(test_loader)\n",
        "    avg_test_mae = test_mae / len(test_loader)\n",
        "\n",
        "    print(f\"Final Test Results for {model_name} - MSE: {avg_test_mse:.6f}, MAE: {avg_test_mae:.6f}\")\n",
        "\n",
        "    # Return the final performance metrics.\n",
        "    return {'MSE': avg_test_mse, 'MAE': avg_test_mae}\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Implement the DLinear baseline\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class DLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    A professional, configurable PyTorch implementation of the DLinear model.\n",
        "\n",
        "    DLinear is a simple yet surprisingly effective model for time-series\n",
        "    forecasting. Its core principle is to decompose the input time series into\n",
        "    two components: a long-term trend and a shorter-term seasonal (or residual)\n",
        "    pattern. Each component is then modeled by a separate linear layer, and their\n",
        "    outputs are summed to produce the final forecast.\n",
        "\n",
        "    Architecture:\n",
        "    1.  **Decomposition**: A moving average filter (`nn.AvgPool1d`) is applied to\n",
        "        the input sequence to extract the trend component. The seasonal component\n",
        "        is simply the residual (input - trend).\n",
        "    2.  **Forecasting**: The flattened trend and seasonal sequences are each passed\n",
        "        through their own dedicated `nn.Linear` layer, which directly maps the\n",
        "        input sequence to the output forecast horizon.\n",
        "    3.  **Combination**: The final forecast is the sum of the outputs from the\n",
        "        trend and seasonal linear layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_len: int, output_len: int, num_features: int, kernel_size: int = 25):\n",
        "        \"\"\"\n",
        "        Initializes the DLinear model.\n",
        "\n",
        "        Args:\n",
        "            input_len: The length of the input time-series sequence (T).\n",
        "            output_len: The length of the output forecast horizon (T').\n",
        "            num_features: The number of features in the input time series (e.g., 6).\n",
        "            kernel_size: The window size for the moving average filter used for\n",
        "                         trend decomposition.\n",
        "        \"\"\"\n",
        "        # Initialize the parent torch.nn.Module.\n",
        "        super().__init__()\n",
        "\n",
        "        # Store sequence lengths for internal calculations.\n",
        "        self.input_len = input_len\n",
        "        self.output_len = output_len\n",
        "\n",
        "        # A 1D average pooling layer serves as an efficient moving average filter.\n",
        "        # Padding is set to `(kernel_size - 1) // 2` to ensure the output sequence\n",
        "        # has the same length as the input sequence (\"same\" padding).\n",
        "        self.decomposer = nn.AvgPool1d(\n",
        "            kernel_size=kernel_size,\n",
        "            stride=1,\n",
        "            padding=(kernel_size - 1) // 2\n",
        "        )\n",
        "\n",
        "        # A linear layer dedicated to learning patterns from the seasonal component.\n",
        "        # It maps the flattened input sequence to the flattened output sequence.\n",
        "        self.linear_seasonal = nn.Linear(input_len * num_features, output_len)\n",
        "\n",
        "        # A separate linear layer dedicated to learning patterns from the trend component.\n",
        "        self.linear_trend = nn.Linear(input_len * num_features, output_len)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the DLinear model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, input_len, num_features).\n",
        "\n",
        "        Returns:\n",
        "            The output forecast tensor of shape (batch_size, output_len).\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        if x.ndim != 3 or x.shape[1] != self.input_len:\n",
        "            raise ValueError(f\"Input tensor has incorrect shape. Expected (B, {self.input_len}, F), got {x.shape}\")\n",
        "\n",
        "        # --- Decomposition ---\n",
        "        # Permute the tensor for nn.AvgPool1d, which expects (batch, features, length).\n",
        "        # Current shape: (batch_size, input_len, num_features)\n",
        "        # Target shape: (batch_size, num_features, input_len)\n",
        "        permuted_x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply the moving average filter to the permuted tensor to extract the trend.\n",
        "        trend_init = self.decomposer(permuted_x)\n",
        "\n",
        "        # Permute the result back to the original dimension order.\n",
        "        # Shape: (batch_size, num_features, input_len) -> (batch_size, input_len, num_features)\n",
        "        trend_init = trend_init.permute(0, 2, 1)\n",
        "\n",
        "        # The seasonal component is the residual left after subtracting the trend.\n",
        "        seasonal_init = x - trend_init\n",
        "\n",
        "        # Flatten the 2D time-series components into 1D vectors for the linear layers.\n",
        "        # Shape: (batch_size, input_len, num_features) -> (batch_size, input_len * num_features)\n",
        "        seasonal_flat = seasonal_init.reshape(seasonal_init.shape[0], -1)\n",
        "        trend_flat = trend_init.reshape(trend_init.shape[0], -1)\n",
        "\n",
        "        # --- Forecasting ---\n",
        "        # Pass the flattened seasonal component through its dedicated linear layer.\n",
        "        seasonal_output = self.linear_seasonal(seasonal_flat)\n",
        "\n",
        "        # Pass the flattened trend component through its dedicated linear layer.\n",
        "        trend_output = self.linear_trend(trend_flat)\n",
        "\n",
        "        # The final forecast is the sum of the two component forecasts.\n",
        "        return seasonal_output + trend_output\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Implement the TSMixer baseline\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class MixerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single, complete block of the TSMixer architecture.\n",
        "\n",
        "    Each MixerBlock performs two distinct mixing operations with residual\n",
        "    connections and layer normalization, following the standard \"pre-norm\"\n",
        "    transformer block structure: `x + SubLayer(LayerNorm(x))`.\n",
        "\n",
        "    1.  **Time-Mixing**: An MLP is applied across the time dimension for each\n",
        "        feature independently, allowing information to flow between time steps.\n",
        "    2.  **Feature-Mixing**: An MLP is applied across the feature dimension for\n",
        "        each time step independently, allowing information to flow between features.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_len: int, d_model: int, dropout: float, ff_dim: int):\n",
        "        \"\"\"\n",
        "        Initializes the MixerBlock.\n",
        "\n",
        "        Args:\n",
        "            input_len: The length of the input time-series sequence (T).\n",
        "            d_model: The hidden dimension of the features.\n",
        "            dropout: The dropout probability.\n",
        "            ff_dim: The hidden dimension of the feed-forward networks within the MLPs.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer normalization applied before the time-mixing MLP.\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Layer normalization applied before the feature-mixing MLP.\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # The Time-Mixing MLP, which operates on the time dimension.\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(input_len, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, input_len),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # The Feature-Mixing MLP, which operates on the feature dimension.\n",
        "        self.feature_mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the MixerBlock.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, input_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of the same shape.\n",
        "        \"\"\"\n",
        "        # --- Time-Mixing with Residual Connection ---\n",
        "        # Store the input for the first residual connection.\n",
        "        residual = x\n",
        "\n",
        "        # Apply pre-normalization.\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Permute for time-mixing: (B, T, F) -> (B, F, T).\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply the time-domain MLP.\n",
        "        x = self.time_mlp(x)\n",
        "\n",
        "        # Permute back to the original dimension order: (B, F, T) -> (B, T, F).\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Add the residual connection.\n",
        "        x = x + residual\n",
        "\n",
        "        # --- Feature-Mixing with Residual Connection ---\n",
        "        # Store the input for the second residual connection.\n",
        "        residual = x\n",
        "\n",
        "        # Apply pre-normalization.\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Apply the feature-domain MLP.\n",
        "        x = self.feature_mlp(x)\n",
        "\n",
        "        # Add the residual connection.\n",
        "        x = x + residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TSMixer(nn.Module):\n",
        "    \"\"\"\n",
        "    A professional, configurable PyTorch implementation of the TSMixer model.\n",
        "\n",
        "    TSMixer is an \"All-MLP\" architecture for time-series forecasting. It avoids\n",
        "    complex attention mechanisms and instead relies on a stack of `MixerBlock`s\n",
        "    that repeatedly mix information across the time and feature dimensions. This\n",
        "    design makes it computationally efficient and effective for many time-series tasks.\n",
        "\n",
        "    Architecture:\n",
        "    1.  **Input Projection**: A linear layer projects the raw input features into\n",
        "        the model's hidden dimension (`d_model`).\n",
        "    2.  **Mixer Blocks**: The core of the model is a sequence of `MixerBlock`s.\n",
        "        Each block performs both time-mixing and feature-mixing with residual\n",
        "        connections and layer normalization.\n",
        "    3.  **Final Head**: After the final mixer block, the output sequence is\n",
        "        flattened and passed through a final linear layer to produce the forecast\n",
        "        of the desired horizon length.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_len: int,\n",
        "        output_len: int,\n",
        "        num_features: int,\n",
        "        num_blocks: int,\n",
        "        d_model: int,\n",
        "        ff_dim: int,\n",
        "        dropout: float\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the TSMixer model.\n",
        "\n",
        "        Args:\n",
        "            input_len: The length of the input time-series sequence (T).\n",
        "            output_len: The length of the output forecast horizon (T').\n",
        "            num_features: The number of features in the raw input time series.\n",
        "            num_blocks: The number of `MixerBlock`s to stack.\n",
        "            d_model: The hidden dimension used throughout the mixer blocks.\n",
        "            ff_dim: The hidden dimension of the feed-forward networks within the MLPs.\n",
        "            dropout: The dropout probability used in the mixer blocks.\n",
        "        \"\"\"\n",
        "        # Initialize the parent torch.nn.Module.\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Input Validation ---\n",
        "        if not all(isinstance(arg, int) and arg > 0 for arg in [input_len, output_len, num_features, num_blocks, d_model, ff_dim]):\n",
        "            raise ValueError(\"All integer arguments must be positive.\")\n",
        "        if not 0.0 <= dropout < 1.0:\n",
        "            raise ValueError(\"Dropout must be a float between 0.0 and 1.0.\")\n",
        "\n",
        "        # An initial linear layer to project the input features to the model's\n",
        "        # hidden dimension (`d_model`). This allows the model to work with a\n",
        "        # different internal dimension than the number of input features.\n",
        "        self.input_projection = nn.Linear(num_features, d_model)\n",
        "\n",
        "        # A sequential container stacking the specified number of MixerBlocks.\n",
        "        # The `*` operator unpacks the list of MixerBlock instances.\n",
        "        self.mixer_blocks = nn.Sequential(\n",
        "            *[MixerBlock(input_len, d_model, dropout, ff_dim) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        # A final linear layer to project the flattened output of the mixer blocks\n",
        "        # to the desired forecast horizon (`output_len`).\n",
        "        self.final_head = nn.Linear(input_len * d_model, output_len)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the TSMixer model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, input_len, num_features).\n",
        "\n",
        "        Returns:\n",
        "            The output forecast tensor of shape (batch_size, output_len).\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Ensure the input tensor has the correct number of dimensions.\n",
        "        if x.ndim != 3:\n",
        "            raise ValueError(f\"Input tensor must be 3-dimensional (B, T, F), but got {x.ndim} dimensions.\")\n",
        "        # Ensure the input tensor dimensions match the model's configuration.\n",
        "        if x.shape[1] != self.input_projection.in_features and x.shape[2] != self.mixer_blocks[0].time_mlp[0].in_features:\n",
        "             pass # A more robust check would be needed here if dimensions were not fixed.\n",
        "\n",
        "        # 1. Project input features to the model's hidden dimension.\n",
        "        # Shape: (batch_size, input_len, num_features) -> (batch_size, input_len, d_model)\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # 2. Pass the sequence through the stack of mixer blocks.\n",
        "        # The shape remains (batch_size, input_len, d_model).\n",
        "        x = self.mixer_blocks(x)\n",
        "\n",
        "        # 3. Flatten the output and pass through the final head to get the forecast.\n",
        "        # Reshape from (batch_size, input_len, d_model) to (batch_size, input_len * d_model).\n",
        "        x_flat = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        # Project to the final output dimension.\n",
        "        # Shape: (batch_size, input_len * d_model) -> (batch_size, output_len)\n",
        "        return self.final_head(x_flat)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_and_evaluate_baselines(\n",
        "    data_splits: Dict[str, Dict[str, np.ndarray]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the training and evaluation of all specified baseline models.\n",
        "\n",
        "    Args:\n",
        "        data_splits: The dictionary of train/val/test data splits.\n",
        "        config: The master configuration dictionary (for extracting parameters).\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the final performance metrics for each baseline.\n",
        "    \"\"\"\n",
        "    # Determine the execution device.\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Extract common data shape parameters.\n",
        "    input_len = data_splits['train']['X'].shape[1]\n",
        "    output_len = data_splits['train']['y'].shape[1]\n",
        "    num_features = data_splits['train']['X'].shape[2]\n",
        "\n",
        "    # Define baseline-specific configurations. In a real system, this would\n",
        "    # be part of the master config file.\n",
        "    baseline_configs = {\n",
        "        'DLinear': {\n",
        "            'model_params': {'kernel_size': 25},\n",
        "            'train_params': {'epochs': 50, 'learning_rate': 0.001, 'batch_size': 32}\n",
        "        },\n",
        "        'TSMixer': {\n",
        "            'model_params': {'num_blocks': 2, 'd_model': 32, 'ff_dim': 64, 'dropout': 0.1},\n",
        "            'train_params': {'epochs': 50, 'learning_rate': 0.001, 'batch_size': 32}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Dictionary to store the final results.\n",
        "    results = {}\n",
        "\n",
        "    # --- DLinear ---\n",
        "    # Instantiate the DLinear model with its specific parameters.\n",
        "    dlinear_model = DLinear(\n",
        "        input_len, output_len, num_features,\n",
        "        **baseline_configs['DLinear']['model_params']\n",
        "    )\n",
        "    # Run the generic training and evaluation harness.\n",
        "    results['DLinear'] = _train_and_evaluate_baseline(\n",
        "        dlinear_model, 'DLinear', data_splits,\n",
        "        baseline_configs['DLinear']['train_params'], device\n",
        "    )\n",
        "\n",
        "    # --- TSMixer ---\n",
        "    # Instantiate the TSMixer model with its specific parameters.\n",
        "    tsmixer_model = TSMixer(\n",
        "        input_len, output_len, num_features,\n",
        "        **baseline_configs['TSMixer']['model_params']\n",
        "    )\n",
        "    # Run the generic training and evaluation harness.\n",
        "    results['TSMixer'] = _train_and_evaluate_baseline(\n",
        "        tsmixer_model, 'TSMixer', data_splits,\n",
        "        baseline_configs['TSMixer']['train_params'], device\n",
        "    )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ola-lL67MkYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Evaluate Forecasting Performance and Portfolio Utility\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Evaluate Forecasting Performance and Portfolio Utility\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Compute MSE and MAE with consistent aggregation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_error_metrics(\n",
        "    model_name: str,\n",
        "    predictions: np.ndarray,\n",
        "    ground_truth: np.ndarray,\n",
        "    metadata: pd.DataFrame,\n",
        "    ticker_to_dataset_map: Dict[str, str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates and aggregates MSE and MAE for a given set of predictions.\n",
        "\n",
        "    This function provides a single, canonical method for calculating error\n",
        "    metrics, ensuring all models are evaluated under identical conditions.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model being evaluated.\n",
        "        predictions: A 2D numpy array of shape (num_samples, T') of model forecasts.\n",
        "        ground_truth: A 2D numpy array of shape (num_samples, T') of true values.\n",
        "        metadata: The metadata DataFrame corresponding to the samples.\n",
        "        ticker_to_dataset_map: A map from ticker to dataset name (e.g., 'StockNet').\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the metrics for the model.\n",
        "    \"\"\"\n",
        "    # --- Per-Window Error Calculation ---\n",
        "    # Equation: MSE_i = (1/T') * Î£(Å·_i,k - y_i,k)^2\n",
        "    per_window_mse = np.mean((predictions - ground_truth)**2, axis=1)\n",
        "    # Equation: MAE_i = (1/T') * Î£|Å·_i,k - y_i,k|\n",
        "    per_window_mae = np.mean(np.abs(predictions - ground_truth), axis=1)\n",
        "\n",
        "    # --- Aggregation ---\n",
        "    results_df = metadata.copy()\n",
        "    results_df['mse'] = per_window_mse\n",
        "    results_df['mae'] = per_window_mae\n",
        "    results_df['dataset'] = results_df['ticker'].map(ticker_to_dataset_map)\n",
        "\n",
        "    # Group by dataset and calculate the mean of the per-window errors.\n",
        "    dataset_metrics = results_df.groupby('dataset')[['mse', 'mae']].mean()\n",
        "\n",
        "    # --- Formatting Output ---\n",
        "    final_metrics = {}\n",
        "    for dataset, row in dataset_metrics.iterrows():\n",
        "        final_metrics[f\"{dataset}_MSE\"] = row['mse']\n",
        "        final_metrics[f\"{dataset}_MAE\"] = row['mae']\n",
        "\n",
        "    # Calculate \"All Data\" metrics by averaging across all windows.\n",
        "    final_metrics['All_Data_MSE'] = results_df['mse'].mean()\n",
        "    final_metrics['All_Data_MAE'] = results_df['mae'].mean()\n",
        "\n",
        "    # Return as a DataFrame with the model name as the index.\n",
        "    return pd.DataFrame(final_metrics, index=[model_name])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Construct daily Markowitz portfolios\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _solve_markowitz_optimization(\n",
        "    exp_returns: np.ndarray,\n",
        "    cov_matrix: np.ndarray,\n",
        "    risk_aversion: float\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Solves the classic Markowitz mean-variance optimization problem.\n",
        "\n",
        "    This function finds the optimal portfolio weights `w` that maximize the\n",
        "    risk-adjusted expected return, subject to standard constraints (long-only,\n",
        "    fully invested). It uses `cvxpy` for robust and efficient convex optimization.\n",
        "\n",
        "    Equation:\n",
        "        maximize(w):  Î¼'w - (Î³/2) * w'Î£w\n",
        "        subject to:   w >= 0, sum(w) = 1\n",
        "\n",
        "    Args:\n",
        "        exp_returns: A 1D numpy array of expected returns for each asset (Î¼).\n",
        "        cov_matrix: A 2D numpy array representing the covariance matrix of\n",
        "                    asset returns (Î£).\n",
        "        risk_aversion: The risk aversion parameter (Î³).\n",
        "\n",
        "    Returns:\n",
        "        A 1D numpy array of optimal asset weights `w`. If the solver fails,\n",
        "        it returns an equal-weight portfolio as a robust fallback.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    num_assets = len(exp_returns)\n",
        "    if cov_matrix.shape != (num_assets, num_assets):\n",
        "        raise ValueError(\"Shape mismatch between expected returns and covariance matrix.\")\n",
        "\n",
        "    # --- Optimization Problem Definition ---\n",
        "    # Define the portfolio weights as a cvxpy Variable.\n",
        "    w = cp.Variable(num_assets)\n",
        "\n",
        "    # Define the objective function to be maximized.\n",
        "    objective = cp.Maximize(exp_returns.T @ w - (risk_aversion / 2) * cp.quad_form(w, cov_matrix))\n",
        "\n",
        "    # Define the constraints: weights must sum to 1 and be non-negative.\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "\n",
        "    # Formulate the optimization problem.\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "\n",
        "    # --- Solve and Handle Failures ---\n",
        "    try:\n",
        "        # Solve the problem using a suitable solver (e.g., OSQP for quadratic programs).\n",
        "        problem.solve(solver=cp.OSQP, verbose=False)\n",
        "\n",
        "        # If the solver fails to find a solution or the problem is infeasible,\n",
        "        # fall back to a simple equal-weight portfolio.\n",
        "        if w.value is None or problem.status in [cp.INFEASIBLE, cp.UNBOUNDED]:\n",
        "            return np.full(num_assets, 1.0 / num_assets)\n",
        "\n",
        "        # Return the optimal weights.\n",
        "        return w.value\n",
        "\n",
        "    except (cp.error.SolverError, Exception):\n",
        "        # Broad exception to catch any other potential solver-related errors.\n",
        "        return np.full(num_assets, 1.0 / num_assets)\n",
        "\n",
        "\n",
        "def _calculate_portfolio_metrics(daily_returns: pd.Series) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculates standard portfolio performance metrics from a daily return series.\n",
        "\n",
        "    Args:\n",
        "        daily_returns: A pandas Series of daily portfolio returns, indexed by date.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the key performance metrics: 'Return' (annualized),\n",
        "        'Volatility' (annualized), 'Sharpe' ratio, and 'MaxDrawdown'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # If the return series is empty, return zero for all metrics.\n",
        "    if daily_returns.empty:\n",
        "        return {'Return': 0.0, 'Volatility': 0.0, 'Sharpe': 0.0, 'MaxDrawdown': 0.0}\n",
        "\n",
        "    # --- Metric Calculation ---\n",
        "    # Assume 252 trading days in a year for annualization.\n",
        "    trading_days = 252\n",
        "\n",
        "    # Calculate the total compounded return over the period.\n",
        "    total_return = (1 + daily_returns).prod()\n",
        "\n",
        "    # Annualize the total return.\n",
        "    num_years = len(daily_returns) / trading_days\n",
        "    annualized_return = total_return**(1 / num_years) - 1 if num_years > 0 else 0.0\n",
        "\n",
        "    # Calculate annualized volatility (standard deviation of daily returns).\n",
        "    annualized_volatility = daily_returns.std() * np.sqrt(trading_days)\n",
        "\n",
        "    # Calculate the Sharpe Ratio, assuming a risk-free rate of 0.\n",
        "    # Handle the case of zero volatility to avoid division by zero.\n",
        "    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility > 1e-6 else 0.0\n",
        "\n",
        "    # Calculate the Maximum Drawdown.\n",
        "    # 1. Calculate the cumulative return series (equity curve).\n",
        "    cumulative_returns = (1 + daily_returns).cumprod()\n",
        "    # 2. Calculate the running maximum (high-water mark).\n",
        "    peak = cumulative_returns.expanding(min_periods=1).max()\n",
        "    # 3. Calculate the drawdown from the peak.\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    # 4. The maximum drawdown is the minimum value in the drawdown series.\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    # Return all metrics in a structured dictionary.\n",
        "    return {\n",
        "        'Return': annualized_return,\n",
        "        'Volatility': annualized_volatility,\n",
        "        'Sharpe': sharpe_ratio,\n",
        "        'MaxDrawdown': max_drawdown\n",
        "    }\n",
        "\n",
        "\n",
        "def _run_portfolio_backtest(\n",
        "    model_name: str,\n",
        "    predictions_pivot: pd.DataFrame,\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs a daily rebalancing backtest using Markowitz portfolio optimization.\n",
        "\n",
        "    This function simulates a trading strategy where a portfolio is rebalanced\n",
        "    daily based on the model's forecasts for the next `T'` days.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model being backtested.\n",
        "        predictions_pivot: A DataFrame of forecasts, indexed by date, with\n",
        "                           tickers as columns.\n",
        "        cleansed_df: The full cleansed market data DataFrame containing actual prices.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the final portfolio performance metrics,\n",
        "        indexed by the model name.\n",
        "    \"\"\"\n",
        "    print(f\"Running portfolio backtest for {model_name}...\")\n",
        "\n",
        "    # Extract portfolio-specific configuration.\n",
        "    portfolio_config = config['evaluation_and_portfolio_config']['portfolio']\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "    # Get actual historical prices and calculate daily returns.\n",
        "    actual_prices = cleansed_df['Adj Close'].unstack()\n",
        "    actual_returns = actual_prices.pct_change().fillna(0)\n",
        "\n",
        "    # Find the common dates between available forecasts and actual returns.\n",
        "    common_dates = predictions_pivot.index.intersection(actual_returns.index)\n",
        "\n",
        "    # List to store the calculated daily returns of the strategy.\n",
        "    portfolio_returns = []\n",
        "\n",
        "    # --- Daily Rebalancing Loop ---\n",
        "    # Iterate through each day for which we have a forecast to make a decision.\n",
        "    # We stop one day early because we need the next day's return to calculate performance.\n",
        "    for t in tqdm(common_dates[:-1], desc=f\"Backtesting {model_name}\"):\n",
        "        # On day `t`, we use the forecast made at the end of day `t` to decide\n",
        "        # our portfolio for day `t+1`.\n",
        "\n",
        "        # Get the forecasts for all available assets on day `t`.\n",
        "        daily_forecasts = predictions_pivot.loc[t].dropna()\n",
        "\n",
        "        # If no assets have forecasts for this day, hold cash (0% return).\n",
        "        if daily_forecasts.empty:\n",
        "            portfolio_returns.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Get the list of assets to include in the portfolio.\n",
        "        assets = daily_forecasts.index\n",
        "        price_forecasts = np.array(daily_forecasts.tolist())\n",
        "\n",
        "        # Ensure forecasts are 2D for consistent processing.\n",
        "        if price_forecasts.ndim == 1:\n",
        "            price_forecasts = price_forecasts.reshape(1, -1)\n",
        "\n",
        "        # --- Prepare Inputs for Optimizer ---\n",
        "        # 1. Expected Returns (Î¼): Convert T'-step price forecasts to (T'-1) return forecasts\n",
        "        #    and take the mean as the expected return for each asset.\n",
        "        return_forecasts = (price_forecasts[:, 1:] - price_forecasts[:, :-1]) / price_forecasts[:, :-1]\n",
        "        exp_returns = np.mean(return_forecasts, axis=1)\n",
        "\n",
        "        # 2. Covariance Matrix (Î£): Use the model's forecasted return paths to estimate covariance.\n",
        "        if len(assets) > 1 and return_forecasts.shape[1] > 1:\n",
        "            cov_matrix = np.cov(return_forecasts)\n",
        "            # Add a small identity matrix (regularization) for numerical stability.\n",
        "            cov_matrix += np.eye(len(assets)) * 1e-6\n",
        "        else:\n",
        "            # Handle the case of a single asset or single-step return forecast.\n",
        "            cov_matrix = np.array([[np.var(return_forecasts) if return_forecasts.size > 0 else 1e-6]])\n",
        "\n",
        "        # --- Solve for Optimal Weights ---\n",
        "        weights = _solve_markowitz_optimization(\n",
        "            exp_returns, cov_matrix, portfolio_config['risk_aversion_gamma']\n",
        "        )\n",
        "\n",
        "        # --- Calculate Realized Return for day t+1 ---\n",
        "        # Get the actual returns for the chosen assets on the next trading day.\n",
        "        next_day = t + pd.Timedelta(days=1)\n",
        "        # Check if the next day is a valid trading day in our data.\n",
        "        if next_day not in actual_returns.index or not all(asset in actual_returns.columns for asset in assets):\n",
        "            portfolio_returns.append(0.0) # Assume 0% return if data is missing.\n",
        "            continue\n",
        "\n",
        "        realized_returns = actual_returns.loc[next_day, assets].values\n",
        "\n",
        "        # The portfolio's return is the dot product of the weights and the realized returns.\n",
        "        daily_return = np.dot(weights, realized_returns)\n",
        "        portfolio_returns.append(daily_return)\n",
        "\n",
        "    # --- Calculate Final Performance Metrics ---\n",
        "    # Create a pandas Series from the list of daily returns.\n",
        "    portfolio_returns_series = pd.Series(portfolio_returns, index=common_dates[1:])\n",
        "\n",
        "    # Calculate the final summary metrics from the daily return series.\n",
        "    metrics = _calculate_portfolio_metrics(portfolio_returns_series)\n",
        "\n",
        "    # Return the metrics as a DataFrame, indexed by the model name.\n",
        "    return pd.DataFrame(metrics, index=[model_name])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_performance_and_utility(\n",
        "    vta_inference_results: pd.DataFrame,\n",
        "    baseline_model_paths: Dict[str, str],\n",
        "    data_splits: Dict[str, Dict[str, Any]],\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    ticker_to_dataset_map: Dict[str, str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full evaluation of forecasting and portfolio performance for all models.\n",
        "\n",
        "    This function provides a complete, end-to-end evaluation pipeline that\n",
        "    compares the main VTA model against all specified baselines on both\n",
        "    statistical error metrics (MSE, MAE) and practical financial utility\n",
        "    (portfolio performance).\n",
        "\n",
        "    Args:\n",
        "        vta_inference_results: The DataFrame of results from the main VTA model.\n",
        "        baseline_model_paths: A dictionary mapping baseline model names to the\n",
        "                              paths of their trained checkpoints.\n",
        "        data_splits: The dictionary of data splits (train/val/test).\n",
        "        cleansed_df: The full cleansed market data DataFrame.\n",
        "        ticker_to_dataset_map: A map from ticker to dataset name.\n",
        "        config: The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing two DataFrames:\n",
        "        - 'error_metrics': A summary of MSE and MAE for all models.\n",
        "        - 'portfolio_metrics': A summary of portfolio performance for all models.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Task 13: Evaluating Performance and Utility ---\")\n",
        "\n",
        "    # --- Setup ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    test_meta = data_splits['test']['meta']\n",
        "    test_X = data_splits['test']['X']\n",
        "    test_y = data_splits['test']['y']\n",
        "\n",
        "    # Lists to store the results from each model.\n",
        "    all_error_metrics = []\n",
        "    all_portfolio_metrics = []\n",
        "\n",
        "    # --- 1. Evaluate VTA Model ---\n",
        "    print(\"Evaluating VTA model...\")\n",
        "    # Align VTA predictions with the test set's ground truth and metadata.\n",
        "    vta_preds_df = vta_inference_results.set_index(['ticker', 'window_end_date'])\n",
        "    aligned_meta = test_meta.set_index(['ticker', 'window_end_date'])\n",
        "    common_index = vta_preds_df.index.intersection(aligned_meta.index)\n",
        "\n",
        "    # Extract the aligned predictions.\n",
        "    vta_preds = np.array(vta_preds_df.loc[common_index]['final_forecast'].tolist())\n",
        "\n",
        "    # Find the original integer indices to slice the ground truth array correctly.\n",
        "    gt_indices = [test_meta.index.get_loc(idx) for idx in aligned_meta.loc[common_index].index]\n",
        "    vta_gt = test_y[gt_indices]\n",
        "    vta_meta = test_meta.iloc[gt_indices]\n",
        "\n",
        "    # Calculate error metrics for the VTA model.\n",
        "    all_error_metrics.append(_calculate_error_metrics(\n",
        "        'VTA', vta_preds, vta_gt, vta_meta, ticker_to_dataset_map\n",
        "    ))\n",
        "\n",
        "    # Run portfolio backtest for the VTA model.\n",
        "    vta_pivot = vta_inference_results.pivot(index='window_end_date', columns='ticker', values='final_forecast')\n",
        "    all_portfolio_metrics.append(_run_portfolio_backtest(\n",
        "        'VTA', vta_pivot, cleansed_df, config\n",
        "    ))\n",
        "\n",
        "    # --- 2. Evaluate Baseline Models ---\n",
        "    for model_name, model_path in baseline_model_paths.items():\n",
        "        print(f\"\\nEvaluating baseline model: {model_name}...\")\n",
        "\n",
        "        # Instantiate the correct model architecture from the config.\n",
        "        model_params = config['baseline_configs'][model_name]['model_params']\n",
        "        if model_name == 'DLinear':\n",
        "            model = DLinear(test_X.shape[1], test_y.shape[1], test_X.shape[2], **model_params)\n",
        "        elif model_name == 'TSMixer':\n",
        "            model = TSMixer(test_X.shape[1], test_y.shape[1], test_X.shape[2], **model_params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown baseline model name: {model_name}\")\n",
        "\n",
        "        # Load the best checkpoint from the baseline training phase.\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Generate predictions for the entire test set.\n",
        "        with torch.no_grad():\n",
        "            baseline_preds = model(torch.from_numpy(test_X).float().to(device)).cpu().numpy()\n",
        "\n",
        "        # Calculate error metrics for the baseline model.\n",
        "        all_error_metrics.append(_calculate_error_metrics(\n",
        "            model_name, baseline_preds, test_y, test_meta, ticker_to_dataset_map\n",
        "        ))\n",
        "\n",
        "        # Prepare the predictions pivot table for the backtester.\n",
        "        baseline_preds_df = test_meta.copy()\n",
        "        baseline_preds_df['prediction'] = list(baseline_preds)\n",
        "        baseline_pivot = baseline_preds_df.pivot(index='window_end_date', columns='ticker', values='prediction')\n",
        "\n",
        "        # Run the portfolio backtest for the baseline model.\n",
        "        all_portfolio_metrics.append(_run_portfolio_backtest(\n",
        "            model_name, baseline_pivot, cleansed_df, config\n",
        "        ))\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # Combine all results into final summary DataFrames.\n",
        "    error_summary_df = pd.concat(all_error_metrics)\n",
        "    portfolio_summary_df = pd.concat(all_portfolio_metrics)\n",
        "\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "    print(\"\\nError Metrics Summary (MSE/MAE):\")\n",
        "    print(error_summary_df)\n",
        "    print(\"\\nPortfolio Performance Summary:\")\n",
        "    print(portfolio_summary_df)\n",
        "\n",
        "    # Return the final, formatted summary tables.\n",
        "    return {\n",
        "        'error_metrics': error_summary_df,\n",
        "        'portfolio_metrics': portfolio_summary_df\n",
        "    }\n"
      ],
      "metadata": {
        "id": "_GWzoP7PPK_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Create an Orchestrator Function for End-to-End Pipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Create an Orchestrator Function for End-to-End Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Data Preparation Sub-Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_data_preparation_pipeline(\n",
        "    raw_market_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    ticker_to_market_map: Dict[str, str],\n",
        "    dataset_name: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete data preparation pipeline (Tasks 1-5).\n",
        "\n",
        "    This function executes all data validation, cleansing, windowing, annotation,\n",
        "    and prompt assembly steps in a sequential and rigorous manner. It corrects\n",
        "    a critical flaw in a previous version by ensuring that data splits ('train',\n",
        "    'val', 'test') are processed independently after the windowing stage,\n",
        "    preventing any cross-contamination of artifacts.\n",
        "\n",
        "    Args:\n",
        "        raw_market_data_df: The raw input DataFrame of market data.\n",
        "        config: The master configuration dictionary.\n",
        "        ticker_to_market_map: A map from ticker to market (e.g., 'US').\n",
        "        dataset_name: The name of the dataset to guide the splitting logic\n",
        "                      (e.g., \"StockNet\").\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all data artifacts required for the modeling stages,\n",
        "        cleanly separated by data split where appropriate.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Announce the start of the pipeline for clear logging.\n",
        "    print(\"--- Starting Data Preparation Pipeline (Tasks 1-5) ---\")\n",
        "\n",
        "    # Initialize the dictionary that will hold all generated artifacts.\n",
        "    artifacts: Dict[str, Any] = {}\n",
        "\n",
        "    # --- Task 1: Validate Inputs and Configuration ---\n",
        "    # This is the first gatekeeper step to ensure all inputs are sound.\n",
        "    print(\"\\n[Task 1] Validating inputs and master configuration...\")\n",
        "    validate_inputs_and_config(raw_market_data_df, config)\n",
        "    print(\"Validation successful.\")\n",
        "\n",
        "    # --- Task 2: Cleanse and Prepare Raw Market Data ---\n",
        "    # This step removes invalid data and aligns the timeline to trading calendars.\n",
        "    print(\"\\n[Task 2] Cleansing and preparing raw market data...\")\n",
        "    cleansed_df, cleansing_reports = cleanse_and_prepare_data(raw_market_data_df, ticker_to_market_map)\n",
        "    artifacts['cleansed_df'] = cleansed_df\n",
        "    artifacts['cleansing_reports'] = cleansing_reports\n",
        "    print(\"Data cleansing complete.\")\n",
        "\n",
        "    # --- Task 3: Construct Sliding Windows and Split Data ---\n",
        "    # This step transforms the flat time-series into windowed samples for the models.\n",
        "    print(\"\\n[Task 3] Constructing sliding windows and splitting data...\")\n",
        "    window_artifacts = construct_windows_and_split_data(cleansed_df, config, dataset_name)\n",
        "    # Update the main artifacts dictionary with the results (data_splits, summary_report, etc.).\n",
        "    artifacts.update(window_artifacts)\n",
        "    print(\"Windowing and splitting complete.\")\n",
        "\n",
        "    # --- Task 4 & 5: Split-wise Annotation and Prompt Assembly ---\n",
        "    # This is the corrected logic. We process each data split independently.\n",
        "    print(\"\\n[Tasks 4 & 5] Computing annotations and assembling prompts for each data split...\")\n",
        "\n",
        "    # Initialize nested dictionaries to store the split-specific artifacts.\n",
        "    annotation_artifacts: Dict[str, Dict[str, Any]] = {'train': {}, 'val': {}, 'test': {}}\n",
        "    prompt_artifacts: Dict[str, List[str]] = {'train': [], 'val': [], 'test': []}\n",
        "    parser_instance = None\n",
        "\n",
        "    # Loop through each data split ('train', 'val', 'test').\n",
        "    for split_name in artifacts['data_splits'].keys():\n",
        "        print(f\"\\nProcessing split: '{split_name}'...\")\n",
        "\n",
        "        # Extract the data for the current split.\n",
        "        split_data = artifacts['data_splits'][split_name]\n",
        "\n",
        "        # Check if the split is empty. If so, skip processing.\n",
        "        if split_data['X'].shape[0] == 0:\n",
        "            print(f\"Split '{split_name}' is empty. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Task 4: Compute Technical Annotations for the current split ---\n",
        "        # Call the Task 4 orchestrator on the split's `X` data.\n",
        "        annotations = compute_technical_annotations(split_data['X'], config)\n",
        "        annotation_artifacts[split_name] = annotations\n",
        "        print(f\"Computed annotations for {split_data['X'].shape[0]} samples in '{split_name}' split.\")\n",
        "\n",
        "        # --- Task 5: Assemble and Validate Prompts for the current split ---\n",
        "        # Call the Task 5 orchestrator on the split's data and its newly created annotations.\n",
        "        # The `assemble_and_validate_prompts` function was fixed to no longer require `cleansed_df`.\n",
        "        prompts, parser = assemble_and_validate_prompts(\n",
        "            split_data['X'],\n",
        "            split_data['meta'],\n",
        "            annotations,\n",
        "            config\n",
        "        )\n",
        "        prompt_artifacts[split_name] = prompts\n",
        "\n",
        "        # The parser is the same for all splits, so we only need to store it once.\n",
        "        if parser_instance is None:\n",
        "            parser_instance = parser\n",
        "\n",
        "        print(f\"Assembled and validated {len(prompts)} prompts for '{split_name}' split.\")\n",
        "\n",
        "    # Store the final, split-wise artifacts.\n",
        "    artifacts['annotations'] = annotation_artifacts\n",
        "    artifacts['prompts'] = prompt_artifacts\n",
        "    artifacts['parser'] = parser_instance\n",
        "\n",
        "    # --- Finalization ---\n",
        "    print(\"\\n--- Data Preparation Pipeline Complete ---\")\n",
        "\n",
        "    # Return the comprehensive dictionary of all generated data artifacts.\n",
        "    return artifacts\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Modeling, Evaluation, and Ablation Sub-Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _save_artifact(data: Any, path: str) -> None:\n",
        "    \"\"\"\n",
        "    Saves a Python object to a specified file path using pickle serialization.\n",
        "\n",
        "    This utility function is a core component of a resumable pipeline. It ensures\n",
        "    that intermediate and final results (artifacts) are reliably persisted to\n",
        "    disk. It automatically creates the necessary directory structure if it does\n",
        "    not exist.\n",
        "\n",
        "    Args:\n",
        "        data: The Python object to be saved (e.g., DataFrame, dict, model state).\n",
        "        path: The full file path where the artifact will be saved.\n",
        "\n",
        "    Raises:\n",
        "        IOError: If the file cannot be written due to permissions or other\n",
        "                 file system issues.\n",
        "        pickle.PicklingError: If the provided object cannot be serialized by pickle.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get the directory part of the specified path.\n",
        "        dir_path = os.path.dirname(path)\n",
        "\n",
        "        # Create the directory structure if it doesn't already exist.\n",
        "        # The `exist_ok=True` argument prevents an error if the directory is already there.\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        # Open the file in binary write mode ('wb') and serialize the object.\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "        # Log a confirmation message indicating successful save.\n",
        "        logging.info(f\"Artifact saved successfully to: {path}\")\n",
        "\n",
        "    except (IOError, pickle.PicklingError) as e:\n",
        "        # Log a critical error if saving fails and re-raise to halt the pipeline.\n",
        "        logging.error(f\"Failed to save artifact to {path}. Error: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def _load_artifact(path: str) -> Any:\n",
        "    \"\"\"\n",
        "    Loads a Python object from a specified file path using pickle deserialization.\n",
        "\n",
        "    This utility is the counterpart to `_save_artifact` and is essential for\n",
        "    reloading previously computed results, enabling the pipeline to be resumed.\n",
        "\n",
        "    Args:\n",
        "        path: The full file path of the artifact to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        The deserialized Python object.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If no file exists at the specified path.\n",
        "        IOError: If the file cannot be read.\n",
        "        pickle.UnpicklingError: If the file content is not a valid pickle stream.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # First, check if the file exists to provide a clear error message.\n",
        "    if not os.path.exists(path):\n",
        "        # Raise FileNotFoundError, which is the most specific and appropriate error.\n",
        "        raise FileNotFoundError(f\"Artifact not found at path: {path}\")\n",
        "\n",
        "    try:\n",
        "        # Open the file in binary read mode ('rb') and deserialize the object.\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        # Log a confirmation message indicating successful load.\n",
        "        logging.info(f\"Artifact loaded successfully from: {path}\")\n",
        "\n",
        "        # Return the loaded object.\n",
        "        return data\n",
        "\n",
        "    except (IOError, pickle.UnpicklingError) as e:\n",
        "        # Log a critical error if loading fails and re-raise to halt the pipeline.\n",
        "        logging.error(f\"Failed to load artifact from {path}. The file may be corrupted. Error: {e}\")\n",
        "        raise\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Modeling Sub-Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_modeling_pipeline(\n",
        "    data_artifacts: Dict[str, Any],\n",
        "    config: Dict[str, Any],\n",
        "    run_dir: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete modeling and evaluation pipeline (Tasks 6-13).\n",
        "\n",
        "    This function is designed to be robust and resumable. For each major,\n",
        "    time-consuming task, it checks if a corresponding output artifact already\n",
        "    exists in the specified `run_dir`. If so, it skips the task and loads the\n",
        "    artifact. Otherwise, it runs the task, saves the output artifact, and proceeds.\n",
        "    This ensures that the pipeline can be restarted from the point of failure\n",
        "    without re-computing completed stages.\n",
        "\n",
        "    Args:\n",
        "        data_artifacts: The dictionary of artifacts from the data preparation pipeline.\n",
        "        config: The master configuration dictionary.\n",
        "        run_dir: The root directory for saving all artifacts for this specific run.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all artifacts and results from the modeling and\n",
        "        evaluation stages.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Log the start of this major pipeline stage.\n",
        "    logging.info(\"--- Starting Modeling & Evaluation Pipeline (Tasks 6-13) ---\")\n",
        "\n",
        "    # Initialize a dictionary to hold all generated modeling artifacts.\n",
        "    modeling_artifacts: Dict[str, Any] = {}\n",
        "\n",
        "    # --- Task 6: Train Reasoning LLM - Stage 1 (Cold-Start RL) ---\n",
        "    # Define the single, atomic artifact path for this task's complete output.\n",
        "    stage1_artifact_path = os.path.join(run_dir, \"artifacts\", \"task_6_stage1_outputs.pkl\")\n",
        "\n",
        "    # Resumability Check: If the artifact exists, load it; otherwise, run the task.\n",
        "    if os.path.exists(stage1_artifact_path):\n",
        "        logging.info(\"[Task 6] Artifact found. Skipping Stage 1 RL training and loading from disk.\")\n",
        "        stage1_artifacts = _load_artifact(stage1_artifact_path)\n",
        "    else:\n",
        "        logging.info(\"[Task 6] Artifact not found. Running Stage 1 RL training...\")\n",
        "        # Execute the Task 6 orchestrator.\n",
        "        stage1_artifacts = train_reasoning_llm_stage1(\n",
        "            prompts=data_artifacts['prompts']['train'],\n",
        "            y_targets=data_artifacts['data_splits']['train']['y'],\n",
        "            metadata=data_artifacts['data_splits']['train']['meta'],\n",
        "            parser=data_artifacts['parser'],\n",
        "            config=config,\n",
        "            # Pass the run-specific directory for saving model checkpoints.\n",
        "            output_dir=os.path.join(run_dir, \"models\", \"stage1_lora\")\n",
        "        )\n",
        "        # Save the complete output of the task as a single atomic artifact.\n",
        "        _save_artifact(stage1_artifacts, stage1_artifact_path)\n",
        "    # Store the loaded or computed artifact.\n",
        "    modeling_artifacts['stage1_artifacts'] = stage1_artifacts\n",
        "\n",
        "    # --- Task 7: Train Reasoning LLM - Stage 2 (SFT) ---\n",
        "    # The primary artifact of this stage is the path to the best model checkpoint.\n",
        "    stage2_model_path = os.path.join(run_dir, \"models\", \"stage2_sft\", \"best_checkpoint\")\n",
        "    if os.path.exists(stage2_model_path):\n",
        "        logging.info(\"[Task 7] SFT model checkpoint found. Skipping Stage 2 SFT.\")\n",
        "    else:\n",
        "        logging.info(\"[Task 7] SFT model not found. Running Stage 2 SFT...\")\n",
        "        # The SFT trainer saves its own model, so we don't need to save its return value.\n",
        "        # We just need to ensure it runs and creates the expected directory.\n",
        "        _ = train_reasoning_llm_stage2(\n",
        "            stage1_artifacts=stage1_artifacts,\n",
        "            parser=data_artifacts['parser'],\n",
        "            config=config,\n",
        "            output_dir=os.path.join(run_dir, \"models\", \"stage2_sft\")\n",
        "        )\n",
        "    # Store the path to the artifact for the next stage.\n",
        "    modeling_artifacts['stage2_reasoning_model_path'] = stage2_model_path\n",
        "\n",
        "    # --- Task 8: Train Reasoning LLM - Stage 3 (Final RL) ---\n",
        "    stage3_model_path = os.path.join(run_dir, \"models\", \"stage3_final_lora\", \"best_checkpoint\")\n",
        "    if os.path.exists(stage3_model_path):\n",
        "        logging.info(\"[Task 8] Final reasoning model found. Skipping Stage 3 RL training.\")\n",
        "    else:\n",
        "        logging.info(\"[Task 8] Final reasoning model not found. Running Stage 3 RL training...\")\n",
        "        _ = train_reasoning_llm_stage3(\n",
        "            sft_checkpoint_path=stage2_model_path,\n",
        "            prompts=data_artifacts['prompts']['train'],\n",
        "            y_targets=data_artifacts['data_splits']['train']['y'],\n",
        "            metadata=data_artifacts['data_splits']['train']['meta'],\n",
        "            parser=data_artifacts['parser'],\n",
        "            config=config,\n",
        "            output_dir=os.path.join(run_dir, \"models\", \"stage3_final_lora\")\n",
        "        )\n",
        "    modeling_artifacts['stage3_reasoning_model_path'] = stage3_model_path\n",
        "\n",
        "    # --- Task 9: Train Forecasting Backbone (Ï†) ---\n",
        "    backbone_path = os.path.join(run_dir, \"models\", \"forecasting_backbone\", \"best_checkpoint.pth\")\n",
        "    if os.path.exists(backbone_path):\n",
        "        logging.info(\"[Task 9] Forecasting backbone model found. Skipping training.\")\n",
        "    else:\n",
        "        logging.info(\"[Task 9] Forecasting backbone not found. Running training...\")\n",
        "        _ = train_forecasting_backbone(\n",
        "            data_splits=data_artifacts['data_splits'],\n",
        "            config=config,\n",
        "            output_dir=os.path.join(run_dir, \"models\", \"forecasting_backbone\")\n",
        "        )\n",
        "    modeling_artifacts['backbone_model_path'] = backbone_path\n",
        "\n",
        "    # --- Task 10: Train Conditional Forecaster (Ïˆ) ---\n",
        "    fusion_model_path = os.path.join(run_dir, \"models\", \"conditional_fusion_model\", \"best_checkpoint.pth\")\n",
        "    if os.path.exists(fusion_model_path):\n",
        "        logging.info(\"[Task 10] Conditional fusion model found. Skipping training.\")\n",
        "    else:\n",
        "        logging.info(\"[Task 10] Conditional fusion model not found. Running training...\")\n",
        "        # Combine all prompts for the attribute derivation step.\n",
        "        all_prompts = data_artifacts['prompts']['train'] + data_artifacts['prompts']['val']\n",
        "        _ = train_conditional_forecaster(\n",
        "            data_splits=data_artifacts['data_splits'],\n",
        "            prompts=all_prompts,\n",
        "            reasoning_model_path=stage3_model_path,\n",
        "            backbone_path=backbone_path,\n",
        "            parser=data_artifacts['parser'],\n",
        "            config=config,\n",
        "            output_dir=os.path.join(run_dir, \"models\", \"conditional_fusion_model\")\n",
        "        )\n",
        "    modeling_artifacts['conditional_model_path'] = fusion_model_path\n",
        "\n",
        "    # --- Task 11: Perform End-to-End Inference ---\n",
        "    inference_artifact_path = os.path.join(run_dir, \"artifacts\", \"task_11_inference_results.pkl\")\n",
        "    if os.path.exists(inference_artifact_path):\n",
        "        logging.info(\"[Task 11] Inference results found. Skipping inference.\")\n",
        "        vta_inference_results = _load_artifact(inference_artifact_path)\n",
        "    else:\n",
        "        logging.info(\"[Task 11] Inference results not found. Running end-to-end inference...\")\n",
        "        # Combine all prompts from all splits for the inference step.\n",
        "        all_prompts = data_artifacts['prompts']['train'] + data_artifacts['prompts']['val'] + data_artifacts['prompts']['test']\n",
        "        vta_inference_results = run_end_to_end_inference(\n",
        "            data_splits=data_artifacts['data_splits'],\n",
        "            prompts=all_prompts,\n",
        "            reasoning_model_path=stage3_model_path,\n",
        "            conditional_model_path=fusion_model_path,\n",
        "            parser=data_artifacts['parser'],\n",
        "            config=config\n",
        "        )\n",
        "        _save_artifact(vta_inference_results, inference_artifact_path)\n",
        "    modeling_artifacts['vta_inference_results'] = vta_inference_results\n",
        "\n",
        "    # --- Task 12: Train and Evaluate Baselines ---\n",
        "    baselines_artifact_path = os.path.join(run_dir, \"artifacts\", \"task_12_baseline_artifacts.pkl\")\n",
        "    if os.path.exists(baselines_artifact_path):\n",
        "        logging.info(\"[Task 12] Baseline artifacts found. Skipping training.\")\n",
        "        baseline_artifacts = _load_artifact(baselines_artifact_path)\n",
        "    else:\n",
        "        logging.info(\"[Task 12] Baseline artifacts not found. Running baseline training...\")\n",
        "        baseline_artifacts = train_and_evaluate_baselines(\n",
        "            data_splits=data_artifacts['data_splits'],\n",
        "            config=config,\n",
        "            run_dir=run_dir\n",
        "        )\n",
        "        _save_artifact(baseline_artifacts, baselines_artifact_path)\n",
        "    modeling_artifacts.update(baseline_artifacts)\n",
        "\n",
        "    # --- Task 13: Evaluate Final Performance ---\n",
        "    # This is a fast, analytical step, so we run it every time without caching.\n",
        "    logging.info(\"[Task 13] Evaluating final performance and utility of all models...\")\n",
        "    final_evaluation_results = evaluate_performance_and_utility(\n",
        "        vta_inference_results=vta_inference_results,\n",
        "        baseline_artifacts=baseline_artifacts,\n",
        "        data_splits=data_artifacts['data_splits'],\n",
        "        cleansed_df=data_artifacts['cleansed_df'],\n",
        "        config=config\n",
        "    )\n",
        "    modeling_artifacts['final_evaluation_results'] = final_evaluation_results\n",
        "\n",
        "    # --- Finalization ---\n",
        "    logging.info(\"--- Modeling & Evaluation Pipeline Complete ---\")\n",
        "\n",
        "    # Return the comprehensive dictionary of all modeling and evaluation artifacts.\n",
        "    return modeling_artifacts\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Master Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _configure_for_dry_run(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Modifies a configuration dictionary for a fast, low-resource dry run.\n",
        "\n",
        "    This helper function takes a deep copy of the main configuration and\n",
        "    systematically reduces computational parameters (e.g., epochs, steps)\n",
        "    across all training stages to enable a quick, end-to-end test of the\n",
        "    pipeline's integrity.\n",
        "\n",
        "    Args:\n",
        "        config: The original master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A new configuration dictionary modified for the dry run.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to avoid modifying the original configuration object.\n",
        "    dry_run_config = copy.deepcopy(config)\n",
        "\n",
        "    logging.warning(\"!!! CONFIGURING FOR DRY-RUN MODE !!!\")\n",
        "    logging.warning(\"Training steps and epochs will be reduced to minimal values.\")\n",
        "\n",
        "    # Modify RL training parameters for all stages.\n",
        "    dry_run_config['reasoning_model_config']['rl_training_params']['max_steps'] = 2\n",
        "    dry_run_config['reasoning_model_config']['rl_training_params']['updates_per_batch'] = 1\n",
        "    dry_run_config['reasoning_model_config']['rl_training_params']['batch_size'] = 2 # Smaller batch size\n",
        "\n",
        "    # Modify SFT training parameters.\n",
        "    dry_run_config['reasoning_model_config']['sft_training_params']['epochs'] = 1\n",
        "    dry_run_config['reasoning_model_config']['sft_training_params']['batch_size'] = 2\n",
        "\n",
        "    # Modify forecasting backbone training parameters.\n",
        "    dry_run_config['forecasting_model_config']['training_params']['epochs'] = 1\n",
        "    dry_run_config['forecasting_model_config']['training_params']['batch_size'] = 4\n",
        "\n",
        "    # Modify conditional forecaster training parameters.\n",
        "    dry_run_config['conditional_fusion_config']['training_params']['epochs'] = 1\n",
        "    dry_run_config['conditional_fusion_config']['training_params']['batch_size'] = 4\n",
        "\n",
        "    # Modify baseline training parameters.\n",
        "    if 'baseline_configs' in dry_run_config:\n",
        "        for model_name in dry_run_config['baseline_configs']:\n",
        "            dry_run_config['baseline_configs'][model_name]['train_params']['epochs'] = 1\n",
        "            dry_run_config['baseline_configs'][model_name]['train_params']['batch_size'] = 4\n",
        "\n",
        "    return dry_run_config\n",
        "\n",
        "\n",
        "def run_vta_pipeline(\n",
        "    raw_market_data_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    ticker_to_market_map: Dict[str, str],\n",
        "    ticker_to_dataset_map: Dict[str, str],\n",
        "    dataset_name: str,\n",
        "    run_id: str = None,\n",
        "    dry_run: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end VTA research pipeline.\n",
        "\n",
        "    This master orchestrator manages the entire workflow from raw data to final\n",
        "    evaluation. It establishes a reproducible environment by setting random seeds,\n",
        "    configuring file-based logging, and creating a unique directory for all\n",
        "    experimental artifacts. It also includes a `dry_run` mode for rapid,\n",
        "    end-to-end testing of the entire codebase.\n",
        "\n",
        "    Args:\n",
        "        raw_market_data_df: The raw input DataFrame of market data.\n",
        "        config: The master configuration dictionary.\n",
        "        ticker_to_market_map: A map from ticker to market (e.g., 'US').\n",
        "        ticker_to_dataset_map: A map from ticker to dataset name (e.g., 'StockNet').\n",
        "        dataset_name: The name of the dataset to guide splitting logic.\n",
        "        run_id: An optional identifier for the run. If None, a timestamp-based\n",
        "                ID is generated.\n",
        "        dry_run: If True, runs the pipeline on a small subset of data with minimal\n",
        "                 training settings for quick validation of the code.\n",
        "\n",
        "    Returns:\n",
        "        A comprehensive dictionary containing all artifacts generated throughout\n",
        "        the entire pipeline, organized by stage.\n",
        "    \"\"\"\n",
        "    # --- 1. Reproducibility and Environment Setup ---\n",
        "    # Generate a unique run ID using the current timestamp if one is not provided.\n",
        "    if run_id is None:\n",
        "        run_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    if dry_run:\n",
        "        run_id += \"_dry_run\"\n",
        "\n",
        "    # Define the root directory for all artifacts of this run.\n",
        "    run_dir = os.path.join(\"./results\", run_id)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Configure centralized logging to both a file and the console for this run.\n",
        "    log_file = os.path.join(run_dir, \"pipeline.log\")\n",
        "    # Remove any existing handlers to ensure a clean logger setup.\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s [%(levelname)s] - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logging.info(\"--- Initializing VTA End-to-End Pipeline ---\")\n",
        "    logging.info(f\"Run ID: {run_id}\")\n",
        "    logging.info(f\"Artifacts will be saved in: {run_dir}\")\n",
        "\n",
        "    # Make a deep copy of the config to avoid modifying the original object.\n",
        "    run_config = copy.deepcopy(config)\n",
        "\n",
        "    # Set global random seeds for full reproducibility.\n",
        "    try:\n",
        "        seeds = run_config['reproducibility']['seeds']\n",
        "        random.seed(seeds['python'])\n",
        "        np.random.seed(seeds['numpy'])\n",
        "        torch.manual_seed(seeds['framework'])\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seeds['framework'])\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "        logging.info(f\"Random seeds set: {seeds}\")\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Configuration missing 'reproducibility.seeds' key: {e}\")\n",
        "\n",
        "    # --- 2. Handle Dry-Run Mode ---\n",
        "    if dry_run:\n",
        "        # Modify the configuration for a fast, low-resource run.\n",
        "        run_config = _configure_for_dry_run(run_config)\n",
        "\n",
        "        # Slice the input data to a small subset.\n",
        "        all_tickers = raw_market_data_df.index.get_level_values('ticker').unique()\n",
        "        if len(all_tickers) > 2:\n",
        "            dry_run_tickers = all_tickers[:2]\n",
        "            raw_market_data_df = raw_market_data_df[\n",
        "                raw_market_data_df.index.get_level_values('ticker').isin(dry_run_tickers)\n",
        "            ]\n",
        "            logging.info(f\"Data sliced to {len(dry_run_tickers)} tickers for dry run: {dry_run_tickers.tolist()}\")\n",
        "\n",
        "    # Save the exact configuration used for this run for a complete audit trail.\n",
        "    _save_artifact(run_config, os.path.join(run_dir, \"run_config.pkl\"))\n",
        "\n",
        "    # --- 3. Execute Sub-Pipelines ---\n",
        "    try:\n",
        "        # Execute the data preparation pipeline (Tasks 1-5).\n",
        "        data_artifacts = _run_data_preparation_pipeline(\n",
        "            raw_market_data_df=raw_market_data_df,\n",
        "            config=run_config,\n",
        "            ticker_to_market_map=ticker_to_market_map,\n",
        "            dataset_name=dataset_name\n",
        "        )\n",
        "\n",
        "        # Execute the modeling and evaluation pipeline (Tasks 6-13).\n",
        "        modeling_artifacts = _run_modeling_pipeline(\n",
        "            data_artifacts=data_artifacts,\n",
        "            config=run_config,\n",
        "            run_dir=run_dir\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Catch any exception during the pipeline execution, log it, and re-raise.\n",
        "        logging.error(\"!!! PIPELINE EXECUTION FAILED !!!\", exc_info=True)\n",
        "        raise e\n",
        "\n",
        "    # --- 4. Finalization ---\n",
        "    # Combine all artifacts into a single, comprehensive output dictionary.\n",
        "    final_artifacts = {\n",
        "        \"run_id\": run_id,\n",
        "        \"run_dir\": run_dir,\n",
        "        \"data_preparation_artifacts\": data_artifacts,\n",
        "        \"modeling_and_evaluation_artifacts\": modeling_artifacts\n",
        "    }\n",
        "\n",
        "    logging.info(f\"--- VTA End-to-End Pipeline Finished Successfully for run {run_id} ---\")\n",
        "\n",
        "    # Close the logger handlers.\n",
        "    logging.shutdown()\n",
        "\n",
        "    return final_artifacts\n"
      ],
      "metadata": {
        "id": "3wcLBjj3Z0Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Conduct Robustness Analyses (Sensitivity Analyses)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Conduct Robustness Analyses (Sensitivity Analyses)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Sensitivity to key hyperparameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _deep_set_value(d: Dict[str, Any], key_path: str, value: Any) -> None:\n",
        "    \"\"\"\n",
        "    Safely sets a value in a deeply nested dictionary using a dot-separated key path.\n",
        "\n",
        "    Args:\n",
        "        d: The dictionary to modify.\n",
        "        key_path: A string representing the path (e.g., 'a.b.c').\n",
        "        value: The value to set at the specified path.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If an intermediate key in the path does not exist.\n",
        "    \"\"\"\n",
        "    # Split the key path into individual keys.\n",
        "    keys = key_path.split('.')\n",
        "    # Traverse the dictionary to the second-to-last key.\n",
        "    for key in keys[:-1]:\n",
        "        d = d[key]\n",
        "    # Set the value on the final key.\n",
        "    d[keys[-1]] = value\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_sensitivity_analysis(\n",
        "    base_config: Dict[str, Any],\n",
        "    # Pass through all required arguments for the main pipeline\n",
        "    raw_market_data_df: pd.DataFrame,\n",
        "    ticker_to_market_map: Dict[str, str],\n",
        "    ticker_to_dataset_map: Dict[str, str],\n",
        "    dataset_name: str,\n",
        "    base_run_id: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates a sensitivity analysis by systematically varying key hyperparameters.\n",
        "\n",
        "    This function runs the full end-to-end VTA pipeline multiple times, each\n",
        "    time modifying a single hyperparameter to observe its effect on the final\n",
        "    performance metrics. It leverages the resumability of the main pipeline\n",
        "    to avoid re-computing stages that are unaffected by a given change.\n",
        "\n",
        "    Args:\n",
        "        base_config: The baseline master configuration dictionary.\n",
        "        raw_market_data_df: The raw input DataFrame of market data.\n",
        "        ticker_to_market_map: A map from ticker to market (e.g., 'US').\n",
        "        ticker_to_dataset_map: A map from ticker to dataset name (e.g., 'StockNet').\n",
        "        dataset_name: The name of the dataset to guide splitting logic.\n",
        "        base_run_id: A base identifier for this suite of experiments.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the performance metrics (MSE, MAE, Sharpe)\n",
        "        for each tested hyperparameter value.\n",
        "    \"\"\"\n",
        "    # --- Define the Hyperparameter Space for Sensitivity Analysis ---\n",
        "    # This dictionary maps a human-readable name to the config path and values to test.\n",
        "    sensitivity_params = {\n",
        "        'p_uncond': {\n",
        "            'path': 'conditional_fusion_config.training_params.unconditional_probability',\n",
        "            'values': [0.1, 0.3, 0.5]\n",
        "        },\n",
        "        'guidance_scale_s': {\n",
        "            'path': 'conditional_fusion_config.inference_params.guidance_scale',\n",
        "            'values': [0.0, 0.1, 0.3]\n",
        "        },\n",
        "        'reward_scale_lambda': {\n",
        "            'path': 'reasoning_model_config.rl_training_params.reward_scale_lambda',\n",
        "            'values': [0.5, 1.0, 2.0]\n",
        "        },\n",
        "        'group_size_G': {\n",
        "            'path': 'reasoning_model_config.rl_training_params.group_size_G',\n",
        "            'values': [4, 8, 16]\n",
        "        },\n",
        "        'pca_prototypes_D': {\n",
        "            'path': 'forecasting_model_config.architecture_params.pca_n_prototypes_D',\n",
        "            'values': [50, 100, 200]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # List to store the results of each experimental run.\n",
        "    all_results = []\n",
        "\n",
        "    # --- Main Experiment Loop ---\n",
        "    # Iterate through each hyperparameter to be tested.\n",
        "    for param_name, details in sensitivity_params.items():\n",
        "        print(f\"\\n{'='*80}\\nSENSITIVITY ANALYSIS FOR: {param_name}\\n{'='*80}\")\n",
        "\n",
        "        # Iterate through each value for the current hyperparameter.\n",
        "        for value in details['values']:\n",
        "            # --- 1. Create Modified Configuration ---\n",
        "            # Create a deep copy of the base config to avoid side effects.\n",
        "            experiment_config = copy.deepcopy(base_config)\n",
        "\n",
        "            # Use the helper function to safely set the nested value.\n",
        "            _deep_set_value(experiment_config, details['path'], value)\n",
        "\n",
        "            # --- 2. Define Unique Run ID ---\n",
        "            # Create a unique ID for this specific experiment run.\n",
        "            # This ensures that artifacts are stored separately for each configuration.\n",
        "            experiment_run_id = f\"{base_run_id}_{param_name}_{value}\"\n",
        "            print(f\"\\n--- Running experiment with {param_name} = {value} (Run ID: {experiment_run_id}) ---\")\n",
        "\n",
        "            # --- 3. Run the Full Pipeline ---\n",
        "            # Call the master orchestrator from Task 14. Its internal resumability\n",
        "            # will handle skipping of already-computed steps if possible.\n",
        "            final_artifacts = run_vta_pipeline(\n",
        "                raw_market_data_df=raw_market_data_df,\n",
        "                config=experiment_config,\n",
        "                ticker_to_market_map=ticker_to_market_map,\n",
        "                ticker_to_dataset_map=ticker_to_dataset_map,\n",
        "                dataset_name=dataset_name,\n",
        "                run_id=experiment_run_id,\n",
        "                dry_run=False # Always run full experiments for analysis\n",
        "            )\n",
        "\n",
        "            # --- 4. Record Results ---\n",
        "            # Extract the key performance metrics from the final evaluation results.\n",
        "            eval_results = final_artifacts['modeling_and_evaluation']['final_evaluation_results']\n",
        "            error_metrics = eval_results['error_metrics']\n",
        "            portfolio_metrics = eval_results['portfolio_metrics']\n",
        "\n",
        "            # Store the results in a structured dictionary.\n",
        "            result_record = {\n",
        "                'parameter': param_name,\n",
        "                'value': value,\n",
        "                'MSE': error_metrics.loc['VTA', 'All_Data_MSE'],\n",
        "                'MAE': error_metrics.loc['VTA', 'All_Data_MAE'],\n",
        "                'Sharpe': portfolio_metrics.loc['VTA', 'Sharpe']\n",
        "            }\n",
        "            all_results.append(result_record)\n",
        "\n",
        "            print(f\"--- Completed experiment with {param_name} = {value}. Results: {result_record} ---\")\n",
        "\n",
        "    # --- 5. Finalization ---\n",
        "    # Convert the list of result dictionaries into a final summary DataFrame.\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Set a multi-index for easy analysis and plotting.\n",
        "    results_df = results_df.set_index(['parameter', 'value'])\n",
        "\n",
        "    print(\"\\n\\n--- Sensitivity Analysis Complete ---\")\n",
        "    print(\"Summary of Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Save the final results table to the base run directory.\n",
        "    results_df.to_csv(f\"./results/{base_run_id}_sensitivity_analysis_summary.csv\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "_gHHicbvmQja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Task: Create a Top-Level Orchestrator for the Entire Project\n",
        "# ==============================================================================\n",
        "\n",
        "def main(\n",
        "    market_data_path: str,\n",
        "    config: Dict[str, Any],\n",
        "    ticker_to_market_map: Dict[str, str],\n",
        "    ticker_to_dataset_map: Dict[str, str],\n",
        "    dataset_name: str,\n",
        "    base_run_id: str,\n",
        "    run_sensitivity: bool = True,\n",
        "    dry_run: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    The master orchestrator for the entire VTA research project.\n",
        "\n",
        "    This function serves as the main entry point for running the complete\n",
        "    experimental pipeline. It executes two major phases:\n",
        "    1.  A baseline run of the full VTA pipeline (Tasks 1-14) to establish the\n",
        "        primary results.\n",
        "    2.  An optional, comprehensive sensitivity analysis (Task 15) to evaluate\n",
        "        the model's robustness to key hyperparameter changes.\n",
        "\n",
        "    It leverages the robust, resumable sub-orchestrators for each phase and\n",
        "    manages the overall experimental setup, including data loading, configuration,\n",
        "    and artifact aggregation.\n",
        "\n",
        "    Args:\n",
        "        market_data_path: The file path to the raw market data (e.g., CSV or Parquet).\n",
        "        config: The master configuration dictionary for the entire project.\n",
        "        ticker_to_market_map: A map from ticker to market (e.g., 'US').\n",
        "        ticker_to_dataset_map: A map from ticker to dataset name (e.g., 'StockNet').\n",
        "        dataset_name: The name of the primary dataset to guide splitting logic.\n",
        "        base_run_id: A base identifier for this suite of experiments, used to\n",
        "                     group related runs.\n",
        "        run_sensitivity: If True, the sensitivity analysis will be run after the\n",
        "                         baseline pipeline completes.\n",
        "        dry_run: If True, both pipelines will be run in dry-run mode on a small\n",
        "                 subset of data for rapid testing.\n",
        "\n",
        "    Returns:\n",
        "        A comprehensive dictionary containing the artifacts from the baseline run\n",
        "        and, if executed, the results of the sensitivity analysis.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the `market_data_path` is invalid.\n",
        "    \"\"\"\n",
        "    # --- 1. Initial Setup and Data Loading ---\n",
        "    print(f\"{'='*80}\\nSTARTING VTA PROJECT EXECUTION\\n{'='*80}\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input data file exists before proceeding.\n",
        "    if not os.path.exists(market_data_path):\n",
        "        raise FileNotFoundError(f\"Market data file not found at: {market_data_path}\")\n",
        "\n",
        "    # Load the raw market data from the specified path.\n",
        "    # This assumes a CSV with the correct MultiIndex structure.\n",
        "    # A production system might support multiple formats like Parquet.\n",
        "    print(f\"Loading raw market data from: {market_data_path}\")\n",
        "    raw_market_data_df = pd.read_csv(market_data_path, index_col=[0, 1], parse_dates=[0])\n",
        "\n",
        "    # --- 2. Execute the Baseline VTA Pipeline ---\n",
        "    # This run establishes the main results and creates all necessary trained models.\n",
        "    print(f\"\\n--- Phase 1: Executing Baseline VTA Pipeline ---\")\n",
        "\n",
        "    # Define a specific run ID for the baseline experiment.\n",
        "    baseline_run_id = f\"{base_run_id}_baseline\"\n",
        "\n",
        "    # Call the master orchestrator from Task 14.\n",
        "    baseline_artifacts = run_vta_pipeline(\n",
        "        raw_market_data_df=raw_market_data_df,\n",
        "        config=config,\n",
        "        ticker_to_market_map=ticker_to_market_map,\n",
        "        ticker_to_dataset_map=ticker_to_dataset_map,\n",
        "        dataset_name=dataset_name,\n",
        "        run_id=baseline_run_id,\n",
        "        dry_run=dry_run\n",
        "    )\n",
        "\n",
        "    # Initialize the final results dictionary with the baseline artifacts.\n",
        "    final_project_artifacts = {\n",
        "        \"baseline_run_artifacts\": baseline_artifacts\n",
        "    }\n",
        "\n",
        "    # --- 3. Optionally Execute the Sensitivity Analysis ---\n",
        "    if run_sensitivity:\n",
        "        print(f\"\\n--- Phase 2: Executing Sensitivity Analysis ---\")\n",
        "\n",
        "        # Call the sensitivity analysis orchestrator from Task 15.\n",
        "        # It uses the same base data and config as the baseline run.\n",
        "        sensitivity_results_df = run_sensitivity_analysis(\n",
        "            base_config=config,\n",
        "            raw_market_data_df=raw_market_data_df,\n",
        "            ticker_to_market_map=ticker_to_market_map,\n",
        "            ticker_to_dataset_map=ticker_to_dataset_map,\n",
        "            dataset_name=dataset_name,\n",
        "            base_run_id=base_run_id # The function will append param-specific suffixes\n",
        "        )\n",
        "\n",
        "        # Add the sensitivity results to the final artifacts.\n",
        "        final_project_artifacts[\"sensitivity_analysis_results\"] = sensitivity_results_df\n",
        "\n",
        "    else:\n",
        "        # Log that the sensitivity analysis was skipped.\n",
        "        print(\"\\n--- Phase 2: Skipping Sensitivity Analysis as per configuration. ---\")\n",
        "        final_project_artifacts[\"sensitivity_analysis_results\"] = None\n",
        "\n",
        "    # --- 4. Final Summary ---\n",
        "    print(f\"\\n{'='*80}\\nVTA PROJECT EXECUTION COMPLETE\\n{'='*80}\")\n",
        "\n",
        "    # Print a summary of the key results from the baseline run.\n",
        "    try:\n",
        "        final_evals = baseline_artifacts['modeling_and_evaluation']['final_evaluation_results']\n",
        "        print(\"\\nBaseline VTA Model Performance Summary:\")\n",
        "        print(final_evals['error_metrics'].loc[['VTA']])\n",
        "        print(\"\\nBaseline VTA Portfolio Utility Summary:\")\n",
        "        print(final_evals['portfolio_metrics'].loc[['VTA']])\n",
        "    except KeyError:\n",
        "        print(\"\\nCould not display final summary as evaluation artifacts were not found.\")\n",
        "\n",
        "    # Return the complete set of artifacts from the entire project execution.\n",
        "    return final_project_artifacts\n"
      ],
      "metadata": {
        "id": "JOSOCVPIqhi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}